{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Server Information:\n",
    "You are using Jupyter notebook.\n",
    "\n",
    "The version of the notebook server is: 5.7.8\n",
    "The server is running on this version of Python:\n",
    "Python 3.6.6 |Anaconda, Inc.| (default, Oct  9 2018, 12:34:16) \n",
    "[GCC 7.3.0]\n",
    "\n",
    "Current Kernel Information:\n",
    "Python 3.6.6 |Anaconda, Inc.| (default, Oct  9 2018, 12:34:16) \n",
    "Type 'copyright', 'credits' or 'license' for more information\n",
    "IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Implement Embeddings and evaluate them using text classification\n",
    "## Key Source - https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa\n",
    "## Algos implemented - Word2Vec, GloVe and FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### STEPS:\n",
    "\n",
    "#### Embeddings Generation - Word2Vec\n",
    "#### Embeddings Generation - Gensim-Word2Vec\n",
    "#### Text Classification Accuracy - Gensim-Word2Vec\n",
    "\n",
    "#### Embeddings Generation - GloVe\n",
    "#### Text Classification Accuracy - GloVe\n",
    "\n",
    "#### Embeddings Generation - FastText\n",
    "#### Text Classification Accuracy - FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "################ Embeddings Generation - Word2Vec\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "################ Embeddings Generation - Gensim-Word2Vec\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "################ Text Classification Accuracy - Gensim-Word2Vec\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "################ Embeddings Generation - GloVe\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "################ Text Classification Accuracy - GloVe\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "################ Embeddings Generation - FastText\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "################ Text Classification Accuracy - FastText\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import numpy as np  \n",
    "import re  # regular expressions like '+', '*'\n",
    "import nltk  # The Natural Language Toolkit\n",
    "from sklearn.datasets import load_files  \n",
    "# nltk.download('popular') \n",
    "# will download stopwords, punkt etc # download in default dir else error later on\n",
    "import pickle  \n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Industry</th>\n",
       "      <th>Wiki_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hollywood</td>\n",
       "      <td>is a neighborhood in the central region of Los...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hollywood</td>\n",
       "      <td>including several of its historic studios. Its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hollywood</td>\n",
       "      <td>ethnically diverse, densely populated, economi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hollywood</td>\n",
       "      <td>Hollywood was a small community in 1870 and wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hollywood</td>\n",
       "      <td>1910, and soon thereafter a prominentÂ film in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hollywood</td>\n",
       "      <td>In 1853, oneÂ adobeÂ hut stood in Nopalera (No...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hollywood</td>\n",
       "      <td>community flourished. The area was known as th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hollywood</td>\n",
       "      <td>According to the diary ofÂ H.Â J. Whitley, kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hollywood</td>\n",
       "      <td>carrying wood. The man got out of the wagon an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hollywood</td>\n",
       "      <td>represent England and wood would represent his...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Industry                                          Wiki_Text\n",
       "0  Hollywood  is a neighborhood in the central region of Los...\n",
       "1  Hollywood  including several of its historic studios. Its...\n",
       "2  Hollywood  ethnically diverse, densely populated, economi...\n",
       "3  Hollywood  Hollywood was a small community in 1870 and wa...\n",
       "4  Hollywood  1910, and soon thereafter a prominentÂ film in...\n",
       "5  Hollywood  In 1853, oneÂ adobeÂ hut stood in Nopalera (No...\n",
       "6  Hollywood  community flourished. The area was known as th...\n",
       "7  Hollywood  According to the diary ofÂ H.Â J. Whitley, kno...\n",
       "8  Hollywood  carrying wood. The man got out of the wagon an...\n",
       "9  Hollywood  represent England and wood would represent his..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## importing data\n",
    "# Windows \n",
    "# hollybolly_inc=pd.read_excel(\"C:/Users/anurag.trivedi/Dropbox/Job 06 Sep 2018/prep/13. Data/1. Real Data/11. Text Classification Data/1. HollyBolly/HollyBolly Increased.xlsx\",sheet_name='HollyBolly Increased')\n",
    "# Linux\n",
    "hollybolly_inc=pd.read_csv(\"/home/antrived/Dropbox/Job-25-Feb-2019/prep/13. Data/1. Real Data/11. Text Classification Data/1. HollyBolly/HollyBolly_Increased_V2.csv\", encoding='ISO-8859-1')\n",
    "hollybolly = hollybolly_inc\n",
    "hollybolly.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Labelling\n",
    "hollybolly.Industry[hollybolly.Industry == \"Hollywood\"] = 1\n",
    "hollybolly.Industry[hollybolly.Industry == \"Bollywood\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = hollybolly.iloc[:,0].values\n",
    "type(Y) # numpy array\n",
    "Y\n",
    "# sum(hollybolly.Industry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['is a neighborhood in the central region of Los Angeles, California. The neighborhood is notable for its place as the home of the U.S. film industry, ', 'including several of its historic studios. Its name has come to be aÂ\\xa0metonymfor theÂ\\xa0motion picture industry of the United States. Hollywood is also a highlyÂ\\xa0', 'ethnically diverse, densely populated, economically diverse neighborhood and retail business district.', 'Hollywood was a small community in 1870 and was incorporated as a municipality in 1903.[3][4]Â\\xa0It officially merged with the city of Los Angeles inÂ\\xa0', '1910, and soon thereafter a prominentÂ\\xa0film industryÂ\\xa0began to emerge, eventually becoming the most recognizable film industry in the world.[5][6]', 'In 1853, oneÂ\\xa0adobeÂ\\xa0hut stood in Nopalera (NopalÂ\\xa0field), named for the MexicanÂ\\xa0Nopal cactusÂ\\xa0indigenous to the area. By 1870, an agriculturalÂ\\xa0', 'community flourished. The area was known as theÂ\\xa0Cahuenga Valley, after the pass in theÂ\\xa0Santa Monica MountainsÂ\\xa0immediately to the north.', 'According to the diary ofÂ\\xa0H.Â\\xa0J. Whitley, known as the \"Father of Hollywood\", on his honeymoon in 1886 he stood at the top of the hill looking out over the valley. Along came a Chinese man in a wagonÂ\\xa0', 'carrying wood. The man got out of the wagon and bowed. The Chinese man was asked what he was doing and replied, \"I holly-wood\", meaning \\'hauling wood.\\' H.Â\\xa0J. Whitley had an epiphany and decided to name his new town Hollywood. Holly wouldÂ\\xa0', 'represent England and wood would represent his Scottish heritage. Whitley had already started over 100 towns across the western United States', \"spread to GeneralÂ\\xa0Harrison Gray Otis, Hurd's wife, eastern adjacent ranch co-ow\", 'Daeida WilcoxÂ\\xa0may have learned of the nameÂ\\xa0HollywoodÂ\\xa0from Ivar Weid, her neighbor in Holly Canyon (now Lake Hollywood) and a prominent', \"Whitley's.[9][10]Â\\xa0She recommended the same name to her husband,Â\\xa0Harvey. H. Wilcox. In August 1887, Wilcox filed with the Los Angeles Cou\", 'parcel map of property he had sold named \"Hollywood, California.\" Wilcox wanted to be the first to record it on a deed. The earlyÂ\\xa0', 'By 1900, the region had a post office, newspaper, hotel, and two markets. LosÂ\\xa0', 'Angeles, with a population of 102,479 lay 10 miles (16Â\\xa0km) east through theÂ\\xa0vineyards, barley fields, andÂ\\xa0citrusÂ\\xa0groves. A single', 'Prospect Avenue from it, but service was infrequent and the trip took two hours. The old citrus fruit-packing house was convertedÂ\\xa0', 'TheÂ\\xa0Hollywood HotelÂ\\xa0was opened in 1902 by H.Â\\xa0J. Whitley, president of the Los Pacific Boulevard and Development Company. Having finally', 'subdivided it, Whitley built the hotel to attract land buyers. Flanking the west side ofÂ\\xa0Highland Avenue, the structure fro', 'unpaved road, was regularly graded and graveled. The hotel was to become internationally known and was the center of the civic and social lif', \"Whitley's company developed and sold one of the early residential areas, the Ocean View Tract.[12]Â\\xa0Whitley did much to promote th\", 'electric lighting, including bringing electricity and building a bank, as well as a road into theÂ\\xa0Cahuenga Pass. The lighting ran for several blocksÂ\\xa0', \"Whitley's land was centered onÂ\\xa0Highland Avenue.[13][14]Â\\xa0His 1918 development,Â\\xa0Whitley Heights, was named for him.\", 'Â\\xa0incorporated as aÂ\\xa0municipalityÂ\\xa0on November 14, 1903, by a vote of 88 for and 77 against. On January 30, 1904, the', '113 toÂ\\xa096, for the banishment of liquor in the city, except when it was being sold for medicinal purposes. Neithe', 'In 1910, the city voted for merger with Los Angeles in order to secure an adequate water supply and to gain access to theÂ\\xa0', 'By 1912, major motion-picture companies had set up production near or in Los Angeles.[17]Â\\xa0In the early 1900s, mostÂ\\xa0', \"Edison'sÂ\\xa0Motion Picture Patents CompanyÂ\\xa0in New Jersey, and filmmakers were often sued to stop their productions. To escape this, film\", \"where Edison's patents could not be enforced.[18]Â\\xa0Also, the weather was ideal and there was quick access to various settings. Los Angeles bec\", 'DirectorÂ\\xa0D. W. GriffithÂ\\xa0was the first to make a motion picture in Hollywood. His 17-minute short filmÂ\\xa0InÂ\\xa0Old CaliforniaÂ\\xa0(1910) was filmed for theÂ\\xa0Biograph Company.[20][21][22]Â\\xa0Although Hollywood banned movie thea', 'will recognize names like Abbey Road, Electric Lady, and Motown as places where The Beatles, Jimi Hendrix, Diana Ross, and the Supremes left their musical marks.', 'Since then, an impressive list of rock, pop, country, and reggae artists â\\x80\\x94 Aerosmith, Britney Spears, Keith Urban, and Damian Marley to name a few â\\x80\\x94 have followed suit.', 'Around 1905, â\\x80\\x9cNickelodeonsâ\\x80\\x9d, or 5-cent movie theaters, began to offer an easy and inexpensive way for the public to watch movies. Nickelodeons helped the movie industry move into the 1920â\\x80\\x99s by increasing the public appeal of film and generate more money for filmmakers, alongside the widespread use of theaters to screen World War I propaganda.Â\\xa0', \"Other moviemakers arrived from Europe after World War I: directors likeÂ\\xa0Ernst Lubitsch,Â\\xa0Alfred Hitchcock,Â\\xa0Fritz LangÂ\\xa0andÂ\\xa0Jean Renoir; and actors likeÂ\\xa0Rudolph Valentino,Â\\xa0Marlene Dietrich,Â\\xa0Ronald Colman, andÂ\\xa0Charles Boyer. They joined a homegrown supply of actorsÂ\\xa0â\\x80\\x94 lured west from the New York City stage after the introduction of sound filmsÂ\\xa0â\\x80\\x94 to form one of the 20th century's most remarkable growth industries. At motion pictures' height of popularity in the mid-1940s, the studios were cranking out a total of about 400 movies a year, seen by an audience of 90 million Americans per week.\", 'At the same time, one could usually guess which studio made which film, largely because of the actors who appeared in it;Â\\xa0MGM, for example, claimed it had contracted \"more stars than there are in heaven.\" Each studio had its own style and characteristic touches which made it possible to know thisÂ\\xa0â\\x80\\x93 a trait that does not exist today.', 'Movie-making was still a business, however, and motion picture companies made money by operating under theÂ\\xa0studio system. The major studios kept thousands of people on salaryÂ\\xa0â\\x80\\x94 actors, producers, directors, writers, stunt men, craftspersons, and technicians.', 'The Supreme Court eventually ruled that the major studios ownership of theaters and film distribution was a violation of theÂ\\xa0Sherman Antitrust Act. As a result, the studios began to release actors and technical staff from their contracts with the studios.', 'The drive to produce a spectacle on the movie screen has largely shaped American cinema ever since.', 'The first impression from those quotes is to assume that Russo is talking about future MCU films such as the second part ofÂ\\xa0Infinity WarÂ\\xa0not being constrained by the idea that films more than two hours are too long for audiences.', 'Could it be only a few years until the Russo Brothers names are appearing as â\\x80\\x9cexecutive producerâ\\x80\\x9d above a slew of Oscar nominees, even as they continue to direct some of the most popular blockbusters in recent memory? Weâ\\x80\\x99ll see if the AGBO experiment pans out.', 'To his credit, Weinstein eventually made good on his promise. Following a widely reported sexual harassment scandal, in 2018 The Weinstein Company declared bankruptcy.', 'Six people were arrested Thursday and one was arrested Friday after a grand jury indicted them on a variety of election fraud charges. They face accusations of bribing voters, improperly helping people fill out absentee ballots, voting despite being convicted of disqualifying felonies, and voting even though they lived outside the city or voting district.', 'In view of all this, it should come as no surprise that thereâ\\x80\\x99s already a film in development about what was perhaps theÂ\\xa0biggest storyÂ\\xa0in journalism this past year: the New York Timesâ\\x80\\x99 investigation into allegations of decades-long sexual abuse against Harvey Weinstein.', 'According to aÂ\\xa0reportÂ\\xa0in Mumbai Mirror, Lawrence Kasanoff, who is the producer of the Mortal Kombat series, made a quiet visit to Mumbai to meet Tiger and discuss details of the project. It is expected to be an action film. Lawrence, along with a big studio head and Emmy Award-winning writer Sean Catherine Derek of the Batman series came to Mumbai. Sanjay Grover, son of veteran actor Gulshan Grover, who has been part of the project for the last one year, was also part of the meetings, the report added.', 'The foremost example of this M.O.: 2012â\\x80\\x99sÂ\\xa0Argo.Â\\xa0In the late â\\x80\\x9990s, the producer had read the then-recently declassified testimony of CIA exfiltration expert Tony Mendez in a CIA journal, detailing how he helped American diplomats escape the 1979 Iran hostage crisis by posing them as a group of Canadian filmmakers.', 'Seven of the 11 top-grossing films of 2017 were superhero movies, based on characters first introduced in comic books.', 'Mr. Sonnierâ\\x80\\x99s revenues from a film are a tiny fraction of those from a major studio release, but he is making money off his strategy by keeping production costs low and relying on word-of-mouth to turn his movies into sleeper hits.', 'But according to aÂ\\xa0new reportÂ\\xa0from the University of Southern Californiaâ\\x80\\x99s Annenberg School for Communication and Journalism, youâ\\x80\\x99d be hard-pressed to find anyone who is like me or looks like me', 'â\\x80\\x9cThe Chinese film market is going to be the largest film market in short order,â\\x80\\x9d said Charles Rivkin, a former US assistant secretary of state who in January took over from Christopher Dodd as MPAA chairman. â\\x80\\x9cTheyâ\\x80\\x99re building about 25 screens a day.â\\x80\\x9d', 'This movie was not supposed to be good. Hereâ\\x80\\x99s the plot: A middle-aged cardiovascular surgeonâ\\x80\\x99s wife is killed by a one-armed man, and said surgeon is sent to death row. But his bus crashes on the way to prison, then a train crashes into the bus crash, then Dr. Richard Kimble escapes to go on the run with five U.S. marshals on his heels. This is literally the opening 20 minutes ofÂ\\xa0The Fugitive.', 'One, prominent among Asian immigrant communities, argues that the movie marks a breakthrough in Asian representation by virtue of its all-Asian cast, and holds the promise of further representation.', 'The ceremony will feature presentations, performances, and a special achievement award. Past honorees include Diane Warren, Smokey Robinson, and Glen Campbell.', 'No matter how high it is, a quote is a number that an actorâ\\x80\\x99s representation is constantly trying to jack upâ\\x80\\x94a never-ending cache of money and extras to be forever bested. â\\x80\\x9cAgents build quotes and fight for things in each successful deal that then become bedrock,â\\x80\\x9d says one veteran producer. This is also how agents, who skim 10 percent of their clientsâ\\x80\\x99 takes, earn more themselves.', 'â\\x80\\x9cOne of the most exciting aspects of our Fox acquisition is that it will allow us to greatly accelerate our direct-to-consumer strategy,â\\x80\\x9d Mr. Iger said when he announced the deal in December. â\\x80\\x9cWe believe creating a direct-to-consumer relationship is vital to the future of our media businesses, and itâ\\x80\\x99s our highest priority.â\\x80\\x9d', 'Donald Jr. opens the closet door to reveal De Niro as Mueller hiding behind the door, causing Eric to exclaim:Â\\xa0\"Robert Mueller is in there!\"Â\\xa0', 'â\\x80\\x9cSpaceyâ\\x80\\x99s conduct was extreme and outrageous,â\\x80\\x9d says the filing of yesterday, which is well within the statute of limitations that have stymied many of these types of cases in the now #MeToo era.', 'â\\x80\\x9cI was completely shocked by how little dialogue there was in the film,â\\x80\\x9d Mr. Haynes said in a phone interview.', 'Male actors command about twice the screen time of female ones. Men are the heroes and villains and do most of the talking, with (at the last count) two-thirds of speaking parts in successful films, a ratio only slightly better than in the late 1940s. Women, still, are often ornaments or victims, love interests or damsels in distress: useful for being disrobed, attacked or both.', 'â\\x80\\x9cFor a guy who always had control over actors, filmmakers, his company, his familyâ\\x80\\x94everybodyâ\\x80\\x94this was the first time I saw that he couldnâ\\x80\\x99t control a situation.â\\x80\\x9d Even so, Weinstein seemed hell-bent on trying.', 'Sorry to Bother You: is this the most shocking anti-capitalistÂ\\xa0filmÂ\\xa0ever?', 'No, youâ\\x80\\x99re not seeing double. These beautiful stars all have daughters who look just like them. From Kelly Ripa and Lola Consuelos, to Catherine Zeta-Jones and Carys Douglas,Â\\xa0the resemblance between these duos is uncanny!', 'Lady Gaga surprised the small crowd at the Black Rabbit Rose venue in Hollywood, California Thursday with a performance of Frank Sinatra songs. â\\x80\\x9cIâ\\x80\\x99m here to ruin the party. Iâ\\x80\\x99m so sorry,â\\x80\\x9d Gaga quipped.', 'Rising from the ashes of the 1929 stock-market crash, Twentieth Century-Fox was forged in a master stroke by ruthless producer Darryl F. Zanuck, who merged Twentieth Century Pictures with William Foxâ\\x80\\x99s ailing studioâ\\x80\\x94and booted out Fox in the processâ\\x80\\x94to create a formidable Hollywood player. It was the studio that turned Marilyn Monroe into a star, awarded Elizabeth Taylor her first $1 million payday (for the costly Cleopatra, which at $44 million nearly capsized the company), and helped solidify the modern-day blockbuster with George Lucasâ\\x80\\x99s space fantasy,Â\\xa0Star Wars.', 'Triple FrontierÂ\\xa0is a good movie. The J.C. Chandor-directed action melodrama, penned by Mark Boal and starring Ben Affleck, Oscar Isaac, Garrett Hedland, Charlie Hunnam and Pedro Pascal, isnâ\\x80\\x99t a new genre classic or a towering artistic achievement. It starts better than it ends, and it seems somewhat unwilling to truly confront the morality of its story (especially after its heroes cross a line of no return), but itâ\\x80\\x99s exactly what it promises.', 'When Aunt Becky and that Desperate Housewife became ensnared in the wild Hollywood cheating scam this week, countless questions arose. What does Teri Hatcher think? If Aunt Becky ends up in the slammer, will it be enough to make the Olsen twins resurface? And most importantly: where did Aunt Becky get $500,000 from?', 'She has starred inÂ\\xa0Coronation Street and is currently in Emmerdale.Â\\xa0And Gaynor Faye has now shared that she had a shot to break Hollywood prior to taking up roles in the British soap world. The English actress, 47, opened up toÂ\\xa0The Mirror on how her career could have taken a different path if she had headed across the pond.', 'Former \"Desperate Housewives\" star Felicity Huffman and actress Lori Loughlin, best known for playing Aunt Becky on \"Full House,\" are facing court appearances later this month related to allegations they participated in a college admissions cheating scheme. In the court of public opinion, however, it\\'s hard to say whether fans and Hollywood will, as Uncle Jesse says, \"Have mercy.\"', 'Airplanes are a versatile tool in the screenwriterâ\\x80\\x99s arsenal: They get characters from place to place, they can crash dramatically, they can get filled with inexplicable, profanity-inducing snakes. But theyâ\\x80\\x99re also kind of a nightmare to film movies in, on account of really only being designed for the one thing: Cramming as many people as possible into a little tube and hurling them into the sky, without much considerations for stuff like camera equipment and crews.', 'Entrepreneur Mariam Shaarâ\\x80\\x99s idea of using Palestiniansâ\\x80\\x99 national cuisine to provide hope and opportunity for refugee women has succeeded beyond her wildest dreams. Shaarâ\\x80\\x99s project â\\x80\\x9cSoufraâ\\x80\\x9d â\\x80\\x94 which means a table laden with food â\\x80\\x94 drew the attention of Hollywood actress and social activist Susan Sarandon, whose documentary â\\x80\\x94 also called â\\x80\\x9cSoufraâ\\x80\\x9d â\\x80\\x94 was screened for the first time this week in Beirut.', 'Several weeks ago, during all the pomp and circumstance of Oscar Sunday, stood actress Selma Blair in her sheer dress carrying a custom-made cane that was covered in patent leather. She posed for photographs before entering the Vanity Fair party. This was her first public appearance since revealing that she had been diagnosed with multiple sclerosis.', 'It is more formally referred to asÂ\\xa0Hindi cinema.[4]Â\\xa0The term \"Bollywood\" is often used by non-Indians as aÂ\\xa0synecdocheÂ\\xa0to refer t', 'however, Bollywood proper is only a part of the larger Indian film industry, which includes other production', 'ollywood is one of the largest film producers in India, representing 43% of the net box office revenue, whileÂ\\xa0Ta', 'rest of the regional cinema constitutes 21% as of 2014.[7]Â\\xa0Bollywood is also one of the largest centers of film production in the wo', 'of the biggest film industries in the world in terms of the number of people employed and the number of films produced.[1', 'were sold across the globe which in comparison is 900,000 tickets more thanÂ\\xa0Hollywood.[12]Â\\xa0Bollywood produced 252 films in 2014Â\\xa0', 'The name \"Bollywood\" is aÂ\\xa0portmanteauÂ\\xa0derived fromÂ\\xa0BombayÂ\\xa0(the former name for Mumbai) andÂ\\xa0Hollywood, the center of theÂ\\xa0American film industry.[13]Â\\xa0However, u', 'The naming scheme for \"Bollywood\" was inspired by \"Tollywood\", the name that was used to refer to theÂ\\xa0cinema of West Bengal. Dating back to 1932, \"Tollywood\" was the earliestÂ\\xa0', 'was this \"chance juxtaposition of two pairs of rhyming syllables,\" Holly and Tolly, that led to the portmanteau name \"Tollywood\" being coined. The name \"Tollywood\" went on to be', 'basedÂ\\xa0Junior StatesmanÂ\\xa0youth magazine, establishing a precedent for other film industries to use similar-sounding names, eventually leading to the coining of \"Bollywood\"', \"the world's largest film producer. Credit for the term has been claimed by several different people, including the lyricist, filmmaker and scholar Amit Khanna\", 'film made inÂ\\xa0India. By the 1930s, the industry was producing over 200 films per annum.[22]Â\\xa0The first Indian s', 'commercial success.[23]Â\\xa0There was clearly a huge market for talkies and musicals; Bollywood and all the regional film industries qu', 'The 1930s and 1940s were tumultuous times:Â\\xa0IndiaÂ\\xa0was buffeted by theÂ\\xa0Great Depression,Â\\xa0World War II, theÂ\\xa0Indian independence movement, and the violence of theÂ\\xa0Partition. Most Bollywood films were unab', 'Kanya. The next year, he made another colour film, a version ofÂ\\xa0Mother India. However, colour did not become aÂ\\xa0', 'lavish romantic musicals and melodramas were the staple fare at the cinema.', 'FollowingÂ\\xa0India\\'s independence, the period from the late 1940s to the 1960s is regarded by film historians as the \"Golden Age\" of Hindi cine', 'Examples include theÂ\\xa0Guru DuttÂ\\xa0filmsÂ\\xa0PyaasaÂ\\xa0(1957) andÂ\\xa0Kaagaz Ke PhoolÂ\\xa0(1959) and theÂ\\xa0Raj KapoorÂ\\xa0filmsÂ\\xa0AwaaraÂ\\xa0(1951),Â\\xa0Shree 420Â\\xa0(1955)Â\\xa0', 'India;Â\\xa0AwaaraÂ\\xa0presented the city as both a nightmare and a dream, whileÂ\\xa0Pyaasacritiqued the unreality of city life.[27]Â\\xa0Some of the most famousÂ\\xa0', 'popularised the theme ofÂ\\xa0reincarnationÂ\\xa0inÂ\\xa0Western popular culture.[30]Â\\xa0Other acclaimed mainstream Hindi filmmakers at the time includedÂ\\xa0Kamal AmrohiÂ\\xa0andÂ\\xa0Vijay Bhatt. Successful actors at the time include', 'While commercial Hindi cinema was thriving, the 1950s also saw the emergence of a newÂ\\xa0Parallel CinemaÂ\\xa0movement.[27]Though the movement was mainly led byÂ\\xa0Bengali cinema, it also began gain', \"Ever since theÂ\\xa0social realistÂ\\xa0filmÂ\\xa0Neecha NagarÂ\\xa0won theÂ\\xa0Grand PrizeÂ\\xa0at theÂ\\xa0first Cannes Film Festival,[32]Â\\xa0Hindi films were frequently in competition for theÂ\\xa0Palme d'OrÂ\\xa0at theÂ\\xa0Cannes Film FestivalÂ\\xa0throughout the 1950s and early 1960s, with some of them winning major prizes at the festival.[35]Â\\xa0Guru Dutt, while overlooke\", 'international recognition much later in the 1980s.[35][36]Â\\xa0Dutt is now regarded as one of the greatestÂ\\xa0Asian filmmakersÂ\\xa0of all time, alongside the more famous Indian Bengali filmmakerÂ\\xa0Satyaji', 'In the late 1960s and early 1970s, romance movies and action films starred actors likeÂ\\xa0Shammi Kapoor,Â\\xa0Jeetendra,Â\\xa0Rajesh Khanna,Â\\xa0Dharmendra,Â\\xa0Sanjeev KumarÂ\\xa0andÂ\\xa0Shashi KapoorÂ\\xa0and actresses likeÂ\\xa0Sharmila Tagore,Â\\xa0Mumtaz,Â\\xa0Saira B', 'Some Hindi filmmakers such asÂ\\xa0Shyam BenegalÂ\\xa0continued to produce realisticÂ\\xa0Parallel CinemaÂ\\xa0throughout the 1970s,[40]alongsideÂ\\xa0Mani Kaul,Â\\xa0Kumar Shahani,Â\\xa0Ketan Mehta,Â\\xa0Govind NihalaniÂ\\xa0andÂ\\xa0Vijaya Mehta.', \"The 1970s thus saw the rise of commercial cinema in the form of enduring films such asÂ\\xa0SholayÂ\\xa0(1975), which consolidatedÂ\\xa0Amitabh Bachchan's position as a lead actor. The devotional classicÂ\\xa0Jai Santoshi MaÂ\\xa0was also released in 1975.[41]Â\\xa0Another important film from 1975Â\\xa0\", 'During the late 1980s and 1990s, the pendulum swung back toward family-centric romantic musicals with the success of such films asÂ\\xa0Qayamat Se Qayamat TakÂ\\xa0(1988),Â\\xa0Maine Pyar Kiya', 'films were also successful, with actors likeÂ\\xa0Govinda,Â\\xa0Akshay KumarÂ\\xa0andÂ\\xa0Ajay Devgan, and Kumar gaining popularity forÂ\\xa0performingÂ\\xa0dangerousÂ\\xa0stuntsÂ\\xa0inÂ\\xa0action filmsÂ\\xa0in his well knownÂ\\xa0Khiladi (film series)Â\\xa0and oth', 'These films often featured actors likeÂ\\xa0Nana PatekarÂ\\xa0andÂ\\xa0Manoj Bajpai, and actresses likeÂ\\xa0Manisha Koirala,Â\\xa0Tabu,Â\\xa0Pooja BhattandÂ\\xa0Urmila Matondkar, whose performances were usually critically acclaimed.', \"She had signed a contract with Pooja Films on January 17, 2018, with respect to the film Padman, asking for financial help. Before that, KriArj Entertainment Pvt Ltd had also entered a contract with Gothik Entertainment Pvt Ltd in November 2017. Arora and her company kept both production houses in the dark about each other's contracts.\", 'Shraddha was spotted with Rohan last night (December 07, 2018) as the alleged couple stepped out and possibly went on a dinner date. She sported a chic look withÂ\\xa0a white and green striped dress with a denim jacket and her alleged beau donned a cool look in a casual black tee and blue denim.', \"'Gold' is the fictional re-telling of the India's iconic win at the 1948 London OlympicsÂ\\xa0\", 'In this situation, could an intimacy director, someone who keeps people comfortable on the sets, be essential? Filmmaker Madhur Bhandarkar thinks so.', 'And this trend is here to say. In a conversation with Entrepreneur India, Omung Kumar, the man behind two of Bollywood iconic biopic movie', 'A recent report by media consulting firmÂ\\xa0Ormax Media says Facebook and YouTube are the most effective media in driving buzz power, at 67 percent and 51 percent respectively. Television ranks at number three at 43 percent. The Facebook-Instagram combine thus has total buzz power of 69 percent.', 'Dolly Ki Doli director Abhishek Dogra feels that marriage movies are very relatable for audiences.', 'â\\x80\\x9cItâ\\x80\\x99s always humbling when fans come and speak to you, when they take the time out to say â\\x80\\x98helloâ\\x80\\x99, sometimes missing work and changing schedules. When I met Kauthar Abdulalim on the day of a press conference, I could barely see her with all the press people lined up in the middle.', 'India are some of the films that has been shot in the continent and with the wealth of beauty Australia has, it definitely is one of the most perfect locations for all the producers to explore.â\\x80\\x9d', 'December brings the second season ofÂ\\xa0The Marvelous Mrs. Maisel, the award-winning series that follows a 1958 housewife who discovers she has a talent for stand-up comedy', \"While 'Hindi Medium' bagged honours in the categories of BesÂ\\xa0..Â\\xa0\", 'With a funny cold war between him and the dean of the college, Dr. Asthana (Boman Irani), this film is a laughter riot. The film taught us that while conventional ways may seem like the right way to go about it, but sometimes a miracle is long due and it is only possible when a student-teacher relationship becomes a mutually understanding one. Well, like they say, a teacher never stops learning either.', 'Losely based on the life of Helen Keller and her teacher, Black is a film that made its mark on Indian Cinema with not just the story but with outstanding performances by Amitabh Bachchan andÂ\\xa0RaniÂ\\xa0Mukherjee.', 'A couple of weeks beforeÂ\\xa0Befikre, in Gauri Shindeâ\\x80\\x99sÂ\\xa0Dear Zindagi, Alia Bhatt was Kaira.', 'The paper entitledÂ\\xa0Transnational Cinema: A CrossÂ\\xa0Culture Communication Medium, attempts to look into transnational cinemaâ\\x80\\x99s innate nature to network the national to the international, not only in the panoptical sense but also with questions related to aesthetics, ideas, and mostly factors pertaining to cross-culture communication, especially in Indian context', 'The film is a fictional action fantasy featuring a hunky warrior prince who can throw trees and wrestle bulls but still loves his mother. It had the third-largest opening for a foreign-language film in U.S. history', 'The debutante cannot behold her excitement. As quoted byÂ\\xa0Hindustan Times', 'The film has finally got a release date and it is slated for a release on Eid-Ul-Fitr 2019.', '\"To character assassinate a hardworking, driven, achiever like Priyanka Chopra in this warped, petty manner and for TheCut to publish such a low-brow piece is the lowest of the lows.', 'Netflix didnâ\\x80\\x99t regard India as much of an opportunity when it first expanded there in January 2016. The company has built the worldâ\\x80\\x99sÂ\\xa0largest paid online TV networkÂ\\xa0by taking advantage of the proliferation of high-speed internet to deliver a video service thatâ\\x80\\x99s cheaper and easier to use than cable or satellite TV.Â\\xa0', 'The singer was released Thursday night following the intervention of the Indian Embassy in Abu Dhabi, Navdeep Singh Suri, the Indian Ambassador to the UAE, told Gulf News.', 'A Mumbai teen reflects on his upbringing in the slums when he is accused of cheating on the Indian Version of \"Who Wants to be a Millionaire?\"', 'Two friends are searching for their long lost companion. They revisit their college days and recall the memories of their friend who inspired them to think differently, even as the rest of the world called them \"idiots\".', 'An eight-year-old boy is thought to be a lazy trouble-maker, until the new art teacher has the patience and compassion to discover the real problem behind his struggles in school.', 'Over a hundred guests attended the sparkling event at Bowden Rugby Club.Â\\xa0As well as a festive and colourful atmosphere, guests were treated to a glass of bubbly on arrival, a delicious curry buffet, mince pies,Â\\xa0 a raffle with great prizes, andÂ\\xa0 a fabulous set from DJ Irf.', 'The Matrix Reloaded is a 2003 science fiction action film, the first sequel to The Matrix, and the second installment in The Matrix trilogy.', 'Among the guests who have reached the city, areÂ\\xa0actors RekhaÂ\\xa0andÂ\\xa0Disha Patani.', 'Taimur was seen in bright yellow shirt, blue pants and sneakers. He was clicked sprinting across the field with a baton in his hand, playing with his friends, being cheered by mom Kareena and even breaking into a few tears, perhaps unhappy with the cheerleading duties bestowed upon him.', 'Trade analyst Taran Adarsh also shared the box office figures of the film.', 'â\\x80\\x9cFilms like Toilet... are about social and public awareness. As there is similarity between the two countries, such movies are getting accepted in China.', 'On veteran actorÂ\\xa0Dharmendraâ\\x80\\x99s 83rd birthday, his wife and evergreen actress Hema Malini wished him saying he is her â\\x80\\x9ceverlasting loveâ\\x80\\x9d.', \"Famous celebrity Mehendi artist, Veena Nagda, who was in charge of Deepika's Mehendi ceremony, recentlyÂ\\xa0made this really cute revelation in interaction with a radio channel.\", 'Box office success is one thing but to outdo what has been a singular cultural phenomenon spanning the breadth of the country takes some derring-do.', \"After theÂ\\xa0women dazzled,Â\\xa0and theÂ\\xa0couples rockedÂ\\xa0the red carpet, Bollywood's hunks arrived at Priyanka Chopra and Nick Jonas's wedding reception in Mumbai on Thursday, December 20, night.\", 'PoliticianÂ\\xa0Asif BhamlaÂ\\xa0withÂ\\xa0Alka Yagnik, Sudesh Bhosale, Tabassum andÂ\\xa0Arun Govil.', 'The B-town beauty has been updating her Instagram account with beautiful pictures from her exotic holiday.', 'After aÂ\\xa0receptionÂ\\xa0for the media on Wednesday December 19, Priyanka and Nick hosted the Big One at the Taj Lands End hotel', \"Bauua's coarse charm isn't lost on Afiya Yusufzai Bhinder (Anushka Sharma), the wheelchair-bound NASA brains affected by cerebral palsy.\", \"He got his big break in Hrishikesh Mukherjee'sÂ\\xa0Namak HaraamÂ\\xa0in 1973. Raj Kapoor'sÂ\\xa0Prem Rog, in which he played the main villain, made sure there was no looking back.\", 'Vikram has cases of money laundering and drug dealing against him, and the couple were arrested last year from Kenya.', 'The smooth talking gangster had an instant connection with aspiring actress Monica Bedi.', 'The first celebrity gangster of Bombay got married to Sona, mostly based on the fact that she looked so much like Madhubala. Haji had a distinct fondness for the deceased Madhubala, and was married to Sona in a heartbeat.', 'Anita Ayub, a beauty pageant contestant, was very close to underworld terror Dawood Ibrahim.', 'The actress, famous for dancing under a waterfall, is said to have had an affair with the don.', 'Music label T-Series owner Gulshan Kumar was shot to death in broad daylight in 1997.', \"After Hrithik's father refused to give dreaded don Ali Baba Budesh a share of the profits from Kaho Na Pyaar Hai in 2000, he was yet to learn the true meaning of extortion.\", \"Mukesh Bhatt, Boney Kapoor, Rakesh Roshan have all admitted to receiving calls for sharing of profits, with dire consequences threatened if they didn't comply.\", 'While Sanjay was recently charged and placed in jail, he was earlier picked up in 1994 by the police as well.', 'SRK has openly admitted to speaking to dreaded gangster Chhota Shakeel under duress, claiming he had to listen to a story and decide whether to make a film about it. Apart from this, he has also gotten threat calls from Abu Salem and received death threats after the movie Happy New Year.', 'And it starts way back in 1931. Okay, Itâ\\x80\\x99s actually much older. The Underworld is basically working out of and headquartered in Pakistan.', 'Heâ\\x80\\x99s been threatened multiple times, and since his popularity is linked to Salman, Itâ\\x80\\x99s inevitable that he can be used by underworld.', 'Lets not forget all the other actors/actresses/producers who had connections, and used those connections to push their careers up in the beginning, only to switch sides once they became successful.', 'Fawad Khan is the son of Sales Head/Chair of Pakistanâ\\x80\\x99s biggest Pharmaceutical company. And he was roped in first by KJo, in 2008, but he couldnâ\\x80\\x99t get through, owing to the Mumbai blasts. He instead hit his first release with Sonam', 'The attack on RoshanÂ\\xa0 was not undertaken to prove that the Shiv Sena could no longer protectÂ\\xa0 its clients.', 'Rajasthan gangster Lawrence Bishnoi threatened to kill Race 3 actor Salman Khan in Jodhpur where the actor had gone for the hearing of his blackbuck poaching case.Â\\xa0', 'The underworld don has also asked the singer to perform at two stage shows for him free of cost.', 'The recommendation has come based on a recent audit it conducted on the security cover provided to various important personalities in the city, the report stated. In 2014, the agency had provided security cover to SRK', 'Satya, Company, Vaastav, Once Upon a Time in Mumbaai, Agneepathâ\\x80¦ and now Daddy.', 'While the law was amended in 2000 to allow the film industry to qualify for bank credit, the underworldâ\\x80\\x99s influence within Indian cinema had already become deeply entrenched and pervasive', \"Another incident that became the talk of the town and still remains finds its ground in 2000 released film â\\x80\\x98Chori Chori Chupke Chupke.'\", 'Mamta Kulkarni starred in a couple of hits before running off with Vikram Goswami (the co-accused who also has cases of money laundering and drug dealing) to live anonymously in Dubai.', \"What I've noticed mostly is Tamil and Telugu movies are over hyped all the time because of their over enthusiastic fans.\", 'They know what problems a majority of Indians are facing, their living conditions, their heavily burdened cultural values and they make 90% of movies based on poverty, corruption, police crime drama or rich girl poor boy stories. However, they make shitty, illogical and time wasting movies.', 'South movies are more content driven.', 'Before Baahubali, it was Rajinikanthâ\\x80\\x99s Endhiran, which stunned trade pundits that they started seeing the actor as a box office demigod.', 'Point to be noted here is we havenâ\\x80\\x99t added Malayalam and Kannada movies, especially the former producing some of the finest films in modern Indian cinema and they are slowly attracting non-Malayalam speaking multiplex audiences.', 'Allu Arjun has also signed a Tamil biggie with director Lingusamy and Jr NTR is also said to be planning to soon make his debut in Tamil.', 'A good example of this is how Pillai points out the absence of mythological themes during the genesis of Malayalam cinema.', 'Raman makes a very astute observation as to why there was the rise of misogynistic cinema after the â\\x80\\x9990s.', 'For all its â\\x80\\x98socialâ\\x80\\x99 and contemporary themes, the public space for women in Malayalam cinema was cruelly restricted', 'Zeenat Aman:Â\\xa0She was known for her bold on-screen appearances. She starred in films like Satyam Shivam Sundaram (1978), Ajanabee (1974), Hare Rama Hare Krishna (1971), The Great Gambler (1979), Yaadon Ki Baaraat (1973) and Heera Panna (1973).']\n"
     ]
    }
   ],
   "source": [
    "X = hollybolly.iloc[:,1].values.tolist() \n",
    "print(type(X)) # X is list\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning - run either Clean1 or Clean2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neighborhood central region los angeles california neighborhood notable place home film industry', 'include several historic studio name come metonymfor motion picture industry united state hollywood also highly', 'ethnically diverse densely populate economically diverse neighborhood retail business district', 'hollywood small community incorporate municipality officially merge city los angeles', 'soon thereafter prominent film industry begin emerge eventually become recognizable film industry world', 'one adobe hut stood nopalera nopal field name mexican nopal cactus indigenous area agricultural', 'community flourish area know cahuenga valley pas santa monica mountain immediately north', 'accord diary whitley know father hollywood honeymoon stood top hill look valley along come chinese man wagon', 'carry wood man get wagon bow chinese man ask reply meaning wood whitley epiphany decide name new town hollywood holly would', 'represent england wood would represent scottish heritage whitley already start town across western united state', 'spread general harrison gray otis hurd wife eastern adjacent ranch', 'daeida wilcox may learn name hollywood ivar weid neighbor holly canyon lake hollywood prominent', 'whitley recommend name husband harvey wilcox august wilcox file los angeles cou', 'parcel map property sell name hollywood california wilcox want first record deed early', 'region post office newspaper hotel two market los', 'angeles population lay mile km east vineyard barley field citrus grove single', 'prospect avenue service infrequent trip take two hour old citrus house convert', 'hollywood hotel open whitley president los pacific boulevard development company finally', 'subdivide whitley built hotel attract land buyer flank west side highland avenue structure fro', 'unpaved road regularly grade gravel hotel become internationally know center civic social lif', 'whitley company developed sell one early residential area ocean view tract whitley much promote th', 'electric light include bring electricity building bank well road cahuenga pas light ran several block', 'whitley land center highland avenue development whitley height name', 'incorporate municipality november vote january', 'banishment liquor city except sell medicinal purpose neithe', 'city vote merger los angeles order secure adequate water supply gain access', 'major company set production near los angeles early', 'motion picture patent company new jersey filmmaker often sue stop production escape film', 'edison patent could enforce also weather ideal quick access various setting los angeles bec', 'director griffith first make motion picture hollywood short film old california film biograph company although hollywood ban movie thea', 'recognize name like abbey road electric lady motown place beatles jimi hendrix diana ross supremes left musical mark', 'since impressive list rock pop country reggae artist aerosmith britney spear keith urban damian marley name follow suit', 'around movie theater begin offer easy inexpensive way public watch movie nickelodeon help movie industry move increase public appeal film generate money filmmaker alongside widespread use theater screen world war', 'moviemakers arrive europe world war director like ernst lubitsch alfred hitchcock fritz lang jean renoir actor like rudolph valentino marlene dietrich ronald colman charles boyer join homegrown supply actor lure west new york city stage introduction sound film form one century remarkable growth industry motion picture height popularity studio crank total movie year see audience million american per week', 'time one could usually guess studio make film largely actor appear mgm example claimed contract star heaven studio style characteristic touch make possible know trait exist today', 'still business however motion picture company make money operating studio system major studio kept thousand people salary actor producer director writer stunt men craftspersons technician', 'supreme court eventually rule major studio ownership theater film distribution violation sherman antitrust act result studio begin release actor technical staff contract studio', 'drive produce spectacle movie screen largely shape american cinema ever since', 'first impression quote assume russo talk future mcu film second part infinity war constrain idea film two hour long audience', 'could year russo brother name appear slew oscar nominee even continue direct popular blockbuster recent memory see agbo experiment pan', 'credit weinstein eventually make good promise follow widely report sexual harassment scandal weinstein company declare bankruptcy', 'six people arrest thursday one arrest friday grand jury indict variety election fraud charge face accusation bribing voter improperly help people fill absentee ballot voting despite convict disqualify felony voting even though live outside city voting district', 'view come surprise already film development perhaps big story journalism past year new york investigation allegation sexual abuse harvey weinstein', 'accord report mumbai mirror lawrence kasanoff producer mortal kombat series make quiet visit mumbai meet tiger discus detail project expect action film lawrence along big studio head emmy writer sean catherine derek batman series come mumbai sanjay grover son veteran actor gulshan grover part project last one year also part meeting report add', 'foremost example late producer read declassify testimony cia exfiltration expert tony mendez cia journal detail help american diplomat escape iran hostage crisis pose group canadian filmmaker', 'seven film superhero movie base character first introduce comic book', 'revenue film tiny fraction major studio release make money strategy keep production cost low rely turn movie sleeper hit', 'accord new report university southern annenberg school communication journalism find anyone like look like', 'chinese film market go large film market short order say charles rivkin former u assistant secretary state january take christopher dodd mpaa chairman building screen', 'movie suppose good plot cardiovascular wife kill man say surgeon sent death row bus crash way prison train crash bus crash richard kimble escape go run five marshal heel literally opening minute fugitive', 'one prominent among asian immigrant community argues movie mark breakthrough asian representation virtue cast hold promise representation', 'ceremony feature presentation performance special achievement award past honoree include diane warren smokey robinson glen campbell', 'matter high quote number representation constantly try jack cache money extra forever best build quote fight thing successful deal become bedrock say one veteran producer also agent skim percent take earn', 'excite aspect fox acquisition allow u greatly accelerate strategy iger say announce deal december believe create relationship vital future medium business high', 'donald open closet door reveal de niro mueller hiding behind door cause eric exclaim robert mueller', 'conduct extreme outrageous say filing yesterday well within statute limitation stymie many type case metoo era', 'completely shock little dialogue film haynes say phone interview', 'male actor command twice screen time female one men hero villain talk last count speak part successful film ratio slightly well late woman still often ornament victim love interest damsel distress useful disrobed attack', 'guy always control actor filmmaker company first time saw control even weinstein seem try', 'sorry bother shock film ever', 'see double beautiful star daughter look like kelly ripa lola consuelos catherine carys douglas resemblance duo uncanny', 'lady gaga surprised small crowd black rabbit rise venue hollywood california thursday performance frank sinatra song ruin party sorry gaga quipped', 'rise ash crash twentieth forge master stroke ruthless producer darryl zanuck merge twentieth century picture william ail boot fox create formidable hollywood player studio turn marilyn monroe star award elizabeth taylor first million payday costly cleopatra million nearly capsize company help solidify blockbuster george space fantasy star war', 'triple frontier good movie action melodrama pen mark boal star ben affleck oscar isaac garrett hedland charlie hunnam pedro pascal new genre classic tower artistic achievement start well end seem somewhat unwilling truly confront morality story especially hero cross line return exactly promise', 'aunt becky desperate housewife become ensnare wild hollywood cheat scam week countless question arose teri hatcher think aunt becky end slammer enough make olsen twin resurface importantly aunt becky get', 'star coronation street currently gaynor faye share shot break hollywood prior take role british soap world english actress open mirror career could take different path head across pond', 'former desperate housewife star felicity huffman actress lori loughlin best know play aunt becky full house face court appearance later month related allegation participate college admission cheat scheme court public opinion however hard say whether fan hollywood uncle jesse say mercy', 'airplane versatile tool arsenal get character place place crash dramatically get fill inexplicable snake also kind nightmare film movie account really design one thing cram many people possible little tube hurl sky without much consideration stuff like camera equipment crew', 'entrepreneur mariam idea use national cuisine provide hope opportunity refugee woman succeed beyond wild dream project mean table laden food drew attention hollywood actress social activist susan sarandon whose documentary also call screen first time week beirut', 'several week ago pomp circumstance oscar sunday stood actress selma blair sheer dress carry cane cover patent leather pose photograph enter vanity fair party first public appearance since reveal diagnose multiple sclerosis', 'formally refer hindi cinema term bollywood often use synecdoche refer', 'however bollywood proper part large indian film industry include production', 'ollywood one large film producer india represent net box office revenue ta', 'rest regional cinema constitutes bollywood also one large center film production wo', 'big film industry world term number people employ number film produce', 'sell across globe comparison ticket hollywood bollywood produce film', 'name bollywood portmanteau derive bombay former name mumbai hollywood center american film industry however u', 'name scheme bollywood inspire tollywood name use refer cinema west bengal date back tollywood early', 'chance juxtaposition two pair rhyme syllable holly tolly lead portmanteau name tollywood coin name tollywood go', 'base junior statesman youth magazine establish precedent film industry use name eventually lead coin bollywood', 'world large film producer credit term claimed several different people include lyricist filmmaker scholar amit khanna', 'film make india industry produce film per annum first indian', 'commercial success clearly huge market talkie musical bollywood regional film industry qu', 'tumultuous time india buffet great depression world war ii indian independence movement violence partition bollywood film unab', 'kanya next year make another colour film version mother india however colour become', 'lavish romantic musical melodrama staple fare cinema', 'follow india independence period late regard film historian golden age hindi cine', 'example include guru dutt film pyaasa kaagaz ke phool raj kapoor film awaara shree', 'india awaara present city nightmare dream pyaasacritiqued unreality city life famous', 'popularise theme reincarnation western popular culture acclaim mainstream hindi filmmaker time include kamal amrohi vijay bhatt successful actor time include', 'commercial hindi cinema thrive also saw emergence new parallel cinema movement though movement mainly lead bengali cinema also begin gain', 'ever since social realist film neecha nagar grand prize first cannes film festival hindi film frequently competition palme cannes film festival throughout early win major prize festival guru dutt overlooke', 'international recognition much later dutt regard one great asian filmmaker time alongside famous indian bengali filmmaker satyaji', 'late early romance movie action film star actor like shammi kapoor jeetendra rajesh khanna dharmendra sanjeev kumar shashi kapoor actress like sharmila tagore mumtaz saira b', 'hindi filmmaker shyam benegal continued produce realistic parallel cinema throughout alongside mani kaul kumar shahani ketan mehta govind nihalani vijaya mehta', 'thus saw rise commercial cinema form endure film sholay consolidated amitabh bachchan position lead actor devotional classic jai santoshi also release another important film', 'late pendulum swung back toward romantic musical success film qayamat se qayamat tak maine pyar kiya', 'film also successful actor like govinda akshay kumar ajay devgan kumar gain popularity perform dangerous stunt action film well know khiladi film series oth', 'film often feature actor like nana patekar manoj bajpai actress like manisha koirala tabu pooja bhattand urmila matondkar whose performance usually critically acclaim', 'sign contract pooja film january respect film padman ask financial help kriarj entertainment pvt ltd also enter contract gothik entertainment pvt ltd november arora company kept production house dark contract', 'shraddha spot rohan last night december allege couple step possibly go dinner date sport chic look white green strip dress denim jacket allege beau don cool look casual black tee blue denim', 'fictional india iconic win london olympics', 'situation could intimacy director someone keep people comfortable set essential filmmaker madhur bhandarkar think', 'trend say conversation entrepreneur india omung kumar man behind two bollywood iconic biopic movie', 'recent report medium consult firm ormax medium say facebook youtube effective medium drive buzz power percent percent respectively television rank number three percent combine thus total buzz power percent', 'dolly ki doli director abhishek dogra feel marriage movie relatable audience', 'always humble fan come speak take time say sometimes miss work change schedule met kauthar abdulalim day press conference could barely see press people line middle', 'india film shot continent wealth beauty australia definitely one perfect location producer', 'december brings second season marvelous maisel series follow housewife discovers talent comedy', 'medium bag honour category be', 'funny cold war dean college asthana boman irani film laughter riot film taught u conventional way may seem like right way go sometimes miracle long due possible relationship becomes mutually understand one well like say teacher never stop learn either', 'losely base life helen keller teacher black film make mark indian cinema story outstanding performance amitabh bachchan rani mukherjee', 'couple week befikre gauri dear zindagi alia bhatt kaira', 'paper entitle transnational cinema cross culture communication medium attempt look transnational innate nature network national international panoptical sense also question related aesthetic idea mostly factor pertain communication especially indian context', 'film fictional action fantasy feature hunky warrior prince throw tree wrestle bull still love mother opening film history', 'debutante behold excitement quote hindustan time', 'film finally get release date slat release', 'character assassinate hardworking driven achiever like priyanka chopra warp petty manner thecut publish piece low low', 'netflix regard india much opportunity first expand january company built large paid online tv network take advantage proliferation internet deliver video service cheaper easy use cable satellite', 'singer release thursday night follow intervention indian embassy abu dhabi navdeep singh suri indian ambassador uae told gulf news', 'mumbai teen reflect upbringing slum accuse cheat indian version want millionaire', 'two friend search long lose companion revisit college day recall memory friend inspire think differently even rest world call idiot', 'boy thought lazy new art teacher patience compassion discover real problem behind struggle school', 'hundred guest attend sparkle event bowden rugby well festive colourful atmosphere guest treat glass bubbly arrival delicious curry buffet mince pie raffle great prize fabulous set dj irf', 'matrix reload science fiction action film first sequel matrix second installment matrix trilogy', 'among guest reach city actor rekha disha patani', 'taimur see bright yellow shirt blue pant sneaker clicked sprint across field baton hand play friend cheer mom kareena even break tear perhaps unhappy cheerlead duty bestow upon', 'trade analyst taran adarsh also share box office figure film', 'like toilet social public awareness similarity two country movie get accepted china', 'veteran actor birthday wife evergreen actress hema malini wish say', 'famous celebrity mehendi artist veena nagda charge deepika mehendi ceremony recently make really cute revelation interaction radio channel', 'box office success one thing outdo singular cultural phenomenon span breadth country take', 'woman dazzle couple rock red carpet bollywood hunk arrive priyanka chopra nick jonas wedding reception mumbai thursday december night', 'politician asif bhamla alka yagnik sudesh bhosale tabassum arun govil', 'beauty update instagram account beautiful picture exotic holiday', 'reception medium wednesday december priyanka nick host big one taj land end hotel', 'bauua coarse charm lose afiya yusufzai bhinder anushka sharma nasa brain affected cerebral palsy', 'get big break hrishikesh namak haraam raj prem rog played main villain make sure look back', 'vikram case money laundering drug deal couple arrest last year kenya', 'smooth talk gangster instant connection aspire actress monica bedi', 'first celebrity gangster bombay get married sona mostly base fact look much like madhubala haji distinct fondness decease madhubala married sona heartbeat', 'anita ayub beauty pageant contestant close underworld terror dawood ibrahim', 'actress famous dance waterfall say affair', 'music label owner gulshan kumar shot death broad daylight', 'hrithik father refuse give dread ali baba budesh share profit kaho na pyaar hai yet learn true meaning extortion', 'mukesh bhatt boney kapoor rakesh roshan admit receive call share profit dire consequence threaten comply', 'sanjay recently charge place jail earlier picked police well', 'srk openly admit speak dread gangster chhota shakeel duress claim listen story decide whether make film apart also gotten threat call abu salem receive death threat movie happy new year', 'start way back okay actually much old underworld basically work headquarter pakistan', 'threaten multiple time since popularity link salman inevitable use underworld', 'let forget connection use connection push career begin switch side become successful', 'fawad khan son sale big pharmaceutical company roped first kjo get owe mumbai blast instead hit first release sonam', 'attack roshan undertaken prove shiv sena could longer protect client', 'rajasthan gangster lawrence bishnoi threaten kill race actor salman khan jodhpur actor go hearing blackbuck poach', 'underworld also ask singer perform two stage show free cost', 'recommendation come base recent audit conduct security cover provide various important personality city report state agency provide security cover srk', 'satya company vaastav upon time mumbaai daddy', 'law amend allow film industry qualify bank credit influence within indian cinema already become deeply entrench pervasive', 'another incident become talk town still remains find ground release film chori chupke chupke', 'mamta kulkarni star couple hit run vikram goswami also case money laundering drug deal live anonymously dubai', 'notice mostly tamil telugu movie hyped time enthusiastic fan', 'know problem majority indian face living condition heavily burden cultural value make movie base poverty corruption police crime drama rich girl poor boy story however make shitty illogical time waste movie', 'south movie content driven', 'baahubali endhiran stun trade pundit start see actor box office demigod', 'point note add malayalam kannada movie especially former produce fine film modern indian cinema slowly attract speak multiplex audience', 'allu arjun also sign tamil biggie director lingusamy jr ntr also say planning soon make debut tamil', 'good example pillai point absence mythological theme genesis malayalam cinema', 'raman make astute observation rise misogynistic cinema', 'contemporary theme public space woman malayalam cinema cruelly restrict', 'zeenat aman know bold appearance star film like satyam shivam sundaram ajanabee hare rama hare krishna great gambler yaadon ki baaraat heera panna']\n"
     ]
    }
   ],
   "source": [
    "# Clean1 (run either Clean1 or Clean2)\n",
    "    # removes stopwords, special chars, nums, extra spaces, to lower case etc. (better code)\n",
    "    # removes unicode characters\n",
    "\n",
    "    \n",
    "def remove_stopwords(sentence):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    return \" \".join([word.lower() for word in word_tokenize(sentence)\n",
    "                    if word.lower() not in stop_words and word.isalpha()])\n",
    "\n",
    "\n",
    "def clean_stopw_more(text1):\n",
    "    # removes stopwords, special chars, nums, extra spaces, to lower case etc. (better code)\n",
    "    # works on list of strings\n",
    "    documents = []\n",
    "    for sen in text1:\n",
    "        documents.append(remove_stopwords(sen))\n",
    "    return documents\n",
    "\n",
    "\n",
    "def clean_unicode(text1):\n",
    "    # removes unicode characters\n",
    "    # works on list of strings\n",
    "    documents = []\n",
    "    for sen in text1:\n",
    "        sen = str(sen.encode('ascii','ignore'))\n",
    "        sen = sen[2:]\n",
    "        sen = sen[:-1]\n",
    "        sen = \" \".join(sen.split())\n",
    "        documents.append(remove_stopwords(sen))\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Lemmatize with POS Tag\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def lemmatize(text1):\n",
    "    # lemmitizes\n",
    "    # works on list of strings\n",
    "    documents = []\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    for sen in text1:      \n",
    "        document = \"\"\n",
    "        for word in sen.split():\n",
    "#             word1 = lemmatizer.lemmatize(word) # only removes 's' at the end of nouns as of now\n",
    "#             word1 = lemmatizer.lemmatize(word, pos =\"a\") # also converts adjective 'better' to 'good'\n",
    "            word1 = lemmatizer.lemmatize(word, get_wordnet_pos(word)) # as per word's POS tag\n",
    "            document = (''.join(document+\" \"+word1)).strip()\n",
    "        documents.append(document)\n",
    "    return documents\n",
    "\n",
    "\n",
    "X = clean_stopw_more(X)\n",
    "X = clean_unicode(X)\n",
    "X = lemmatize(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = FastText(tokenized_corpus, size=feature_size, window=window_context, \n",
    "                    min_count=min_word_count,sample=sample, sg=1, iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "################ Embeddings Generation - FastText\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'los': ['angeles', 'august', 'adequate', 'officially', 'secure'],\n",
       " 'bollywood': ['ollywood', 'tollywood', 'hollywood', 'huge', 'portmanteau'],\n",
       " 'california': ['griffith', 'quipped', 'gaga', 'deed', 'sorry'],\n",
       " 'hollywood': ['ollywood', 'holly', 'tollywood', 'bollywood', 'dawood'],\n",
       " 'voting': ['bribing', 'variety', 'fraud', 'disqualify', 'jury'],\n",
       " 'western': ['mainstream', 'kamal', 'amrohi', 'scottish', 'united'],\n",
       " 'bengali': ['bengal', 'parallel', 'recognition', 'benegal', 'mainly'],\n",
       " 'amitabh': ['bachchan', 'jai', 'mukherjee', 'santoshi', 'helen'],\n",
       " 'bombay': ['decease', 'fondness', 'sona', 'haji', 'married'],\n",
       " 'blackbuck': ['rajasthan', 'poach', 'jodhpur', 'bishnoi', 'hearing'],\n",
       " 'baahubali': ['demigod', 'pundit', 'endhiran', 'trade', 'box']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "tokenized_corpus = [wpt.tokenize(document) for document in X]\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 100    # Word vector dimensionality  \n",
    "window_context = 50          # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# sg decides whether to use the skip-gram model (1) or CBOW (0)\n",
    "ft_model = FastText(tokenized_corpus, size=feature_size, window=window_context, \n",
    "                    min_count=min_word_count,sample=sample, sg=1, iter=50)\n",
    "# tune other params also like min_n, max_n\n",
    "    # min_n : int, optional\n",
    "    #     Minimum length of char n-grams to be used for training word representations.\n",
    "    # max_n : int, optional\n",
    "    #     Max length of char ngrams to be used for training word representations. Set `max_n` to be\n",
    "    #     lesser than `min_n` to avoid char ngrams being used.\n",
    "    # word_ngrams : {1,0}, optional\n",
    "    #     If 1, uses enriches word vectors with subword(n-grams) information.\n",
    "    #     If 0, this is equivalent to :class:`~gensim.models.word2vec.Word2Vec`.\n",
    "                    \n",
    "# view similar words based on gensim's FastText model\n",
    "similar_words = {search_term: [item[0] for item in ft_model.wv.most_similar([search_term], topn=5)]\n",
    "                  for search_term in ['los','bollywood','california','hollywood','voting','western','bengali','amitabh','bombay','blackbuck','baahubali']}\n",
    "similar_words     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antrived/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.901189</td>\n",
       "      <td>-0.429829</td>\n",
       "      <td>0.072537</td>\n",
       "      <td>-0.276152</td>\n",
       "      <td>-0.102324</td>\n",
       "      <td>-0.086483</td>\n",
       "      <td>0.191645</td>\n",
       "      <td>-0.482364</td>\n",
       "      <td>0.018132</td>\n",
       "      <td>-0.299763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649043</td>\n",
       "      <td>-0.253990</td>\n",
       "      <td>0.359053</td>\n",
       "      <td>0.147686</td>\n",
       "      <td>0.055130</td>\n",
       "      <td>-0.029171</td>\n",
       "      <td>-0.174130</td>\n",
       "      <td>-0.064483</td>\n",
       "      <td>0.199549</td>\n",
       "      <td>-0.050065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.695339</td>\n",
       "      <td>-0.254250</td>\n",
       "      <td>-0.171089</td>\n",
       "      <td>0.001380</td>\n",
       "      <td>0.190561</td>\n",
       "      <td>0.006520</td>\n",
       "      <td>0.066503</td>\n",
       "      <td>-0.067713</td>\n",
       "      <td>-0.037900</td>\n",
       "      <td>-0.177279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606408</td>\n",
       "      <td>-0.254672</td>\n",
       "      <td>0.289679</td>\n",
       "      <td>0.203524</td>\n",
       "      <td>0.413498</td>\n",
       "      <td>-0.326171</td>\n",
       "      <td>-0.006671</td>\n",
       "      <td>0.036370</td>\n",
       "      <td>0.295730</td>\n",
       "      <td>0.161813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.431721</td>\n",
       "      <td>-0.501558</td>\n",
       "      <td>-0.125671</td>\n",
       "      <td>-0.229346</td>\n",
       "      <td>0.162066</td>\n",
       "      <td>-0.126459</td>\n",
       "      <td>-0.065437</td>\n",
       "      <td>-0.293841</td>\n",
       "      <td>0.274963</td>\n",
       "      <td>-0.092186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.526628</td>\n",
       "      <td>-0.098478</td>\n",
       "      <td>0.142099</td>\n",
       "      <td>0.217613</td>\n",
       "      <td>0.214516</td>\n",
       "      <td>-0.120521</td>\n",
       "      <td>-0.275076</td>\n",
       "      <td>0.145664</td>\n",
       "      <td>0.234348</td>\n",
       "      <td>0.017835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.809112</td>\n",
       "      <td>-0.515515</td>\n",
       "      <td>0.111718</td>\n",
       "      <td>-0.023580</td>\n",
       "      <td>-0.199963</td>\n",
       "      <td>-0.033900</td>\n",
       "      <td>0.157925</td>\n",
       "      <td>-0.242619</td>\n",
       "      <td>0.170694</td>\n",
       "      <td>-0.153564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787955</td>\n",
       "      <td>-0.254487</td>\n",
       "      <td>0.553937</td>\n",
       "      <td>0.237264</td>\n",
       "      <td>0.081551</td>\n",
       "      <td>-0.417421</td>\n",
       "      <td>-0.228023</td>\n",
       "      <td>0.069544</td>\n",
       "      <td>0.207278</td>\n",
       "      <td>0.045199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.973768</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>-0.182813</td>\n",
       "      <td>0.170368</td>\n",
       "      <td>0.078524</td>\n",
       "      <td>-0.044272</td>\n",
       "      <td>0.086718</td>\n",
       "      <td>0.079002</td>\n",
       "      <td>-0.119036</td>\n",
       "      <td>-0.419411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361189</td>\n",
       "      <td>-0.477632</td>\n",
       "      <td>0.052283</td>\n",
       "      <td>0.579035</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>-0.379824</td>\n",
       "      <td>-0.248732</td>\n",
       "      <td>-0.319573</td>\n",
       "      <td>0.441516</td>\n",
       "      <td>-0.083097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.793502</td>\n",
       "      <td>-0.353642</td>\n",
       "      <td>0.035973</td>\n",
       "      <td>-0.067531</td>\n",
       "      <td>-0.160598</td>\n",
       "      <td>0.252447</td>\n",
       "      <td>-0.247286</td>\n",
       "      <td>-0.457005</td>\n",
       "      <td>0.026140</td>\n",
       "      <td>-0.088253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.437236</td>\n",
       "      <td>-0.106138</td>\n",
       "      <td>-0.214006</td>\n",
       "      <td>0.363571</td>\n",
       "      <td>0.351176</td>\n",
       "      <td>0.055422</td>\n",
       "      <td>0.202854</td>\n",
       "      <td>-0.215443</td>\n",
       "      <td>-0.032757</td>\n",
       "      <td>0.067539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.528605</td>\n",
       "      <td>-0.187904</td>\n",
       "      <td>0.225062</td>\n",
       "      <td>-0.067450</td>\n",
       "      <td>0.102634</td>\n",
       "      <td>0.231602</td>\n",
       "      <td>-0.177473</td>\n",
       "      <td>0.084507</td>\n",
       "      <td>0.291271</td>\n",
       "      <td>0.042524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.602439</td>\n",
       "      <td>-0.220547</td>\n",
       "      <td>0.011694</td>\n",
       "      <td>0.412785</td>\n",
       "      <td>0.234136</td>\n",
       "      <td>-0.351188</td>\n",
       "      <td>-0.042189</td>\n",
       "      <td>-0.111035</td>\n",
       "      <td>0.106946</td>\n",
       "      <td>0.033110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.780326</td>\n",
       "      <td>-0.186257</td>\n",
       "      <td>0.310420</td>\n",
       "      <td>-0.351271</td>\n",
       "      <td>-0.072221</td>\n",
       "      <td>0.469847</td>\n",
       "      <td>0.027111</td>\n",
       "      <td>-0.193692</td>\n",
       "      <td>-0.208649</td>\n",
       "      <td>-0.140602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.582974</td>\n",
       "      <td>-0.420798</td>\n",
       "      <td>-0.017984</td>\n",
       "      <td>0.365377</td>\n",
       "      <td>0.349382</td>\n",
       "      <td>-0.104692</td>\n",
       "      <td>0.229274</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.339272</td>\n",
       "      <td>0.241276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.888432</td>\n",
       "      <td>-0.011046</td>\n",
       "      <td>0.291353</td>\n",
       "      <td>-0.142663</td>\n",
       "      <td>-0.245188</td>\n",
       "      <td>0.061572</td>\n",
       "      <td>0.030417</td>\n",
       "      <td>-0.300714</td>\n",
       "      <td>-0.274550</td>\n",
       "      <td>-0.174237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610636</td>\n",
       "      <td>-0.255742</td>\n",
       "      <td>0.038166</td>\n",
       "      <td>0.412595</td>\n",
       "      <td>0.251110</td>\n",
       "      <td>-0.017999</td>\n",
       "      <td>0.495548</td>\n",
       "      <td>-0.312582</td>\n",
       "      <td>0.361658</td>\n",
       "      <td>0.166411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.666570</td>\n",
       "      <td>-0.122884</td>\n",
       "      <td>0.171197</td>\n",
       "      <td>0.128577</td>\n",
       "      <td>-0.245623</td>\n",
       "      <td>-0.069877</td>\n",
       "      <td>0.207662</td>\n",
       "      <td>0.108106</td>\n",
       "      <td>-0.116315</td>\n",
       "      <td>-0.269313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.653628</td>\n",
       "      <td>-0.486523</td>\n",
       "      <td>0.025241</td>\n",
       "      <td>0.344874</td>\n",
       "      <td>0.283071</td>\n",
       "      <td>-0.502361</td>\n",
       "      <td>0.264869</td>\n",
       "      <td>-0.174671</td>\n",
       "      <td>0.193859</td>\n",
       "      <td>0.176984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.724431</td>\n",
       "      <td>0.084046</td>\n",
       "      <td>0.024213</td>\n",
       "      <td>0.151650</td>\n",
       "      <td>0.105990</td>\n",
       "      <td>-0.073179</td>\n",
       "      <td>-0.238586</td>\n",
       "      <td>-0.173252</td>\n",
       "      <td>-0.398118</td>\n",
       "      <td>0.046092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430278</td>\n",
       "      <td>0.125561</td>\n",
       "      <td>-0.295341</td>\n",
       "      <td>0.407353</td>\n",
       "      <td>0.188891</td>\n",
       "      <td>0.065163</td>\n",
       "      <td>-0.141379</td>\n",
       "      <td>-0.024193</td>\n",
       "      <td>0.485507</td>\n",
       "      <td>-0.169262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.992306</td>\n",
       "      <td>-0.622992</td>\n",
       "      <td>-0.037546</td>\n",
       "      <td>-0.012552</td>\n",
       "      <td>-0.064725</td>\n",
       "      <td>0.159655</td>\n",
       "      <td>0.146614</td>\n",
       "      <td>-0.430713</td>\n",
       "      <td>-0.251893</td>\n",
       "      <td>-0.446166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550828</td>\n",
       "      <td>-0.266724</td>\n",
       "      <td>0.107836</td>\n",
       "      <td>0.469718</td>\n",
       "      <td>0.053655</td>\n",
       "      <td>-0.046802</td>\n",
       "      <td>0.258400</td>\n",
       "      <td>-0.240424</td>\n",
       "      <td>0.380557</td>\n",
       "      <td>0.005610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.735475</td>\n",
       "      <td>-0.683696</td>\n",
       "      <td>0.353858</td>\n",
       "      <td>-0.099091</td>\n",
       "      <td>-0.389984</td>\n",
       "      <td>0.091399</td>\n",
       "      <td>0.076929</td>\n",
       "      <td>-0.439609</td>\n",
       "      <td>0.010741</td>\n",
       "      <td>-0.217991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.547953</td>\n",
       "      <td>-0.406121</td>\n",
       "      <td>0.239892</td>\n",
       "      <td>0.295686</td>\n",
       "      <td>0.099615</td>\n",
       "      <td>0.013574</td>\n",
       "      <td>0.014414</td>\n",
       "      <td>-0.109636</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.159384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.764557</td>\n",
       "      <td>-0.483421</td>\n",
       "      <td>-0.013476</td>\n",
       "      <td>-0.097209</td>\n",
       "      <td>0.058227</td>\n",
       "      <td>0.165232</td>\n",
       "      <td>0.111806</td>\n",
       "      <td>-0.568851</td>\n",
       "      <td>-0.039590</td>\n",
       "      <td>-0.112534</td>\n",
       "      <td>...</td>\n",
       "      <td>0.581963</td>\n",
       "      <td>-0.132045</td>\n",
       "      <td>0.244377</td>\n",
       "      <td>0.394409</td>\n",
       "      <td>0.218964</td>\n",
       "      <td>-0.048158</td>\n",
       "      <td>0.151100</td>\n",
       "      <td>-0.121438</td>\n",
       "      <td>0.042929</td>\n",
       "      <td>0.036125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.694002</td>\n",
       "      <td>-0.645488</td>\n",
       "      <td>0.126540</td>\n",
       "      <td>-0.111351</td>\n",
       "      <td>-0.198325</td>\n",
       "      <td>-0.020982</td>\n",
       "      <td>0.449097</td>\n",
       "      <td>-0.468345</td>\n",
       "      <td>0.068688</td>\n",
       "      <td>-0.476726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.820068</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.148415</td>\n",
       "      <td>0.118659</td>\n",
       "      <td>-0.022891</td>\n",
       "      <td>-0.204031</td>\n",
       "      <td>0.152720</td>\n",
       "      <td>-0.074506</td>\n",
       "      <td>0.052210</td>\n",
       "      <td>0.007829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.710222</td>\n",
       "      <td>-0.645607</td>\n",
       "      <td>0.188873</td>\n",
       "      <td>-0.157526</td>\n",
       "      <td>-0.106837</td>\n",
       "      <td>0.225218</td>\n",
       "      <td>-0.302224</td>\n",
       "      <td>-0.284722</td>\n",
       "      <td>0.121291</td>\n",
       "      <td>-0.200440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.437041</td>\n",
       "      <td>-0.363367</td>\n",
       "      <td>-0.087809</td>\n",
       "      <td>0.277700</td>\n",
       "      <td>0.035143</td>\n",
       "      <td>0.107443</td>\n",
       "      <td>0.005717</td>\n",
       "      <td>0.279443</td>\n",
       "      <td>0.014054</td>\n",
       "      <td>-0.098826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.867346</td>\n",
       "      <td>-0.486642</td>\n",
       "      <td>-0.075895</td>\n",
       "      <td>0.239179</td>\n",
       "      <td>-0.189611</td>\n",
       "      <td>0.158737</td>\n",
       "      <td>0.171785</td>\n",
       "      <td>-0.361284</td>\n",
       "      <td>-0.185968</td>\n",
       "      <td>-0.667794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625038</td>\n",
       "      <td>-0.437999</td>\n",
       "      <td>-0.086152</td>\n",
       "      <td>0.597943</td>\n",
       "      <td>0.233123</td>\n",
       "      <td>-0.496111</td>\n",
       "      <td>0.525102</td>\n",
       "      <td>-0.250782</td>\n",
       "      <td>0.086979</td>\n",
       "      <td>-0.013610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.730183</td>\n",
       "      <td>-0.332017</td>\n",
       "      <td>0.163964</td>\n",
       "      <td>0.023879</td>\n",
       "      <td>-0.173423</td>\n",
       "      <td>-0.015763</td>\n",
       "      <td>0.293534</td>\n",
       "      <td>-0.241411</td>\n",
       "      <td>0.012665</td>\n",
       "      <td>-0.210887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660821</td>\n",
       "      <td>-0.364117</td>\n",
       "      <td>0.392594</td>\n",
       "      <td>0.144690</td>\n",
       "      <td>0.133155</td>\n",
       "      <td>-0.279313</td>\n",
       "      <td>0.330178</td>\n",
       "      <td>-0.075691</td>\n",
       "      <td>-0.036461</td>\n",
       "      <td>0.186498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.582446</td>\n",
       "      <td>-0.378028</td>\n",
       "      <td>0.097328</td>\n",
       "      <td>0.129944</td>\n",
       "      <td>-0.248229</td>\n",
       "      <td>-0.051842</td>\n",
       "      <td>0.243553</td>\n",
       "      <td>-0.013479</td>\n",
       "      <td>0.074761</td>\n",
       "      <td>-0.183352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955924</td>\n",
       "      <td>-0.230237</td>\n",
       "      <td>0.031227</td>\n",
       "      <td>0.050923</td>\n",
       "      <td>0.264776</td>\n",
       "      <td>-0.597880</td>\n",
       "      <td>0.043895</td>\n",
       "      <td>-0.218730</td>\n",
       "      <td>-0.177743</td>\n",
       "      <td>0.151537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.722013</td>\n",
       "      <td>-0.172732</td>\n",
       "      <td>0.052162</td>\n",
       "      <td>-0.225157</td>\n",
       "      <td>0.080901</td>\n",
       "      <td>0.181035</td>\n",
       "      <td>-0.051066</td>\n",
       "      <td>0.040666</td>\n",
       "      <td>0.239261</td>\n",
       "      <td>-0.134733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.911607</td>\n",
       "      <td>-0.141116</td>\n",
       "      <td>-0.154315</td>\n",
       "      <td>0.208176</td>\n",
       "      <td>0.214750</td>\n",
       "      <td>-0.295792</td>\n",
       "      <td>-0.130707</td>\n",
       "      <td>-0.269010</td>\n",
       "      <td>0.038447</td>\n",
       "      <td>-0.035663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.674128</td>\n",
       "      <td>-0.139736</td>\n",
       "      <td>0.022471</td>\n",
       "      <td>-0.215617</td>\n",
       "      <td>-0.053443</td>\n",
       "      <td>0.014383</td>\n",
       "      <td>0.198577</td>\n",
       "      <td>-0.182704</td>\n",
       "      <td>-0.174033</td>\n",
       "      <td>-0.242468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455028</td>\n",
       "      <td>-0.193357</td>\n",
       "      <td>0.136215</td>\n",
       "      <td>0.277116</td>\n",
       "      <td>0.309885</td>\n",
       "      <td>-0.145417</td>\n",
       "      <td>0.117610</td>\n",
       "      <td>-0.139196</td>\n",
       "      <td>-0.187495</td>\n",
       "      <td>0.404533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.365088</td>\n",
       "      <td>-0.390356</td>\n",
       "      <td>-0.094513</td>\n",
       "      <td>-0.144332</td>\n",
       "      <td>0.107592</td>\n",
       "      <td>0.026050</td>\n",
       "      <td>-0.416890</td>\n",
       "      <td>-0.326435</td>\n",
       "      <td>0.114748</td>\n",
       "      <td>-0.178032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.788753</td>\n",
       "      <td>-0.122114</td>\n",
       "      <td>-0.337353</td>\n",
       "      <td>0.238990</td>\n",
       "      <td>0.333730</td>\n",
       "      <td>-0.276520</td>\n",
       "      <td>0.074214</td>\n",
       "      <td>-0.096524</td>\n",
       "      <td>0.311824</td>\n",
       "      <td>-0.120922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.742134</td>\n",
       "      <td>-0.557649</td>\n",
       "      <td>0.174629</td>\n",
       "      <td>-0.054127</td>\n",
       "      <td>-0.234820</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.144151</td>\n",
       "      <td>-0.147045</td>\n",
       "      <td>-0.068389</td>\n",
       "      <td>-0.213106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.873547</td>\n",
       "      <td>-0.340507</td>\n",
       "      <td>0.101957</td>\n",
       "      <td>0.037941</td>\n",
       "      <td>0.332140</td>\n",
       "      <td>-0.395525</td>\n",
       "      <td>0.083798</td>\n",
       "      <td>-0.143875</td>\n",
       "      <td>-0.224044</td>\n",
       "      <td>0.268158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.658875</td>\n",
       "      <td>-0.329375</td>\n",
       "      <td>-0.016084</td>\n",
       "      <td>0.139410</td>\n",
       "      <td>-0.310596</td>\n",
       "      <td>-0.065925</td>\n",
       "      <td>0.237145</td>\n",
       "      <td>-0.330996</td>\n",
       "      <td>-0.007831</td>\n",
       "      <td>-0.184283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.837604</td>\n",
       "      <td>-0.062648</td>\n",
       "      <td>0.260818</td>\n",
       "      <td>0.276173</td>\n",
       "      <td>0.525183</td>\n",
       "      <td>-0.658834</td>\n",
       "      <td>-0.047433</td>\n",
       "      <td>-0.158358</td>\n",
       "      <td>0.135419</td>\n",
       "      <td>0.269249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.549006</td>\n",
       "      <td>-0.234354</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>-0.057032</td>\n",
       "      <td>-0.245063</td>\n",
       "      <td>-0.065521</td>\n",
       "      <td>-0.079639</td>\n",
       "      <td>-0.156548</td>\n",
       "      <td>0.054001</td>\n",
       "      <td>-0.085603</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480299</td>\n",
       "      <td>-0.164290</td>\n",
       "      <td>0.239309</td>\n",
       "      <td>0.117797</td>\n",
       "      <td>0.288546</td>\n",
       "      <td>-0.214383</td>\n",
       "      <td>-0.227600</td>\n",
       "      <td>-0.017484</td>\n",
       "      <td>0.127544</td>\n",
       "      <td>0.071374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.741801</td>\n",
       "      <td>-0.739494</td>\n",
       "      <td>0.308562</td>\n",
       "      <td>-0.149341</td>\n",
       "      <td>-0.460031</td>\n",
       "      <td>-0.079378</td>\n",
       "      <td>0.084274</td>\n",
       "      <td>-0.077967</td>\n",
       "      <td>0.075482</td>\n",
       "      <td>-0.104612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862400</td>\n",
       "      <td>-0.294552</td>\n",
       "      <td>0.358835</td>\n",
       "      <td>0.173644</td>\n",
       "      <td>0.049690</td>\n",
       "      <td>-0.367543</td>\n",
       "      <td>-0.453822</td>\n",
       "      <td>0.125762</td>\n",
       "      <td>0.243909</td>\n",
       "      <td>0.040782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.779332</td>\n",
       "      <td>-0.473609</td>\n",
       "      <td>-0.083932</td>\n",
       "      <td>0.033362</td>\n",
       "      <td>-0.078407</td>\n",
       "      <td>-0.061510</td>\n",
       "      <td>0.265706</td>\n",
       "      <td>-0.393670</td>\n",
       "      <td>0.148718</td>\n",
       "      <td>-0.339413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498039</td>\n",
       "      <td>-0.181604</td>\n",
       "      <td>0.538744</td>\n",
       "      <td>0.398402</td>\n",
       "      <td>0.267943</td>\n",
       "      <td>-0.133994</td>\n",
       "      <td>-0.052655</td>\n",
       "      <td>0.156561</td>\n",
       "      <td>-0.108014</td>\n",
       "      <td>0.168952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.777261</td>\n",
       "      <td>0.050743</td>\n",
       "      <td>-0.190158</td>\n",
       "      <td>-0.013344</td>\n",
       "      <td>0.135204</td>\n",
       "      <td>-0.145747</td>\n",
       "      <td>0.136261</td>\n",
       "      <td>-0.189306</td>\n",
       "      <td>-0.082709</td>\n",
       "      <td>-0.313848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423362</td>\n",
       "      <td>-0.233967</td>\n",
       "      <td>0.330380</td>\n",
       "      <td>0.289080</td>\n",
       "      <td>0.652835</td>\n",
       "      <td>-0.475044</td>\n",
       "      <td>0.110475</td>\n",
       "      <td>-0.264221</td>\n",
       "      <td>0.257415</td>\n",
       "      <td>0.144920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.702403</td>\n",
       "      <td>-0.687170</td>\n",
       "      <td>0.272041</td>\n",
       "      <td>0.180767</td>\n",
       "      <td>-0.476761</td>\n",
       "      <td>0.085457</td>\n",
       "      <td>-0.121718</td>\n",
       "      <td>-0.234537</td>\n",
       "      <td>0.134960</td>\n",
       "      <td>-0.206361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538340</td>\n",
       "      <td>-0.286340</td>\n",
       "      <td>0.260384</td>\n",
       "      <td>0.247529</td>\n",
       "      <td>0.163528</td>\n",
       "      <td>-0.191433</td>\n",
       "      <td>-0.020691</td>\n",
       "      <td>0.020723</td>\n",
       "      <td>0.354150</td>\n",
       "      <td>0.087899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.007299</td>\n",
       "      <td>-0.081757</td>\n",
       "      <td>-0.226707</td>\n",
       "      <td>-0.113309</td>\n",
       "      <td>0.099967</td>\n",
       "      <td>-0.041301</td>\n",
       "      <td>0.282903</td>\n",
       "      <td>-0.455703</td>\n",
       "      <td>-0.161899</td>\n",
       "      <td>-0.346360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.406088</td>\n",
       "      <td>-0.244097</td>\n",
       "      <td>0.528597</td>\n",
       "      <td>0.365624</td>\n",
       "      <td>0.234942</td>\n",
       "      <td>-0.259192</td>\n",
       "      <td>0.090542</td>\n",
       "      <td>-0.206987</td>\n",
       "      <td>0.189673</td>\n",
       "      <td>0.034898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.312795</td>\n",
       "      <td>-0.146187</td>\n",
       "      <td>0.080841</td>\n",
       "      <td>-0.670533</td>\n",
       "      <td>-0.008729</td>\n",
       "      <td>0.393054</td>\n",
       "      <td>-0.114775</td>\n",
       "      <td>-0.448272</td>\n",
       "      <td>-0.140716</td>\n",
       "      <td>0.063235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419906</td>\n",
       "      <td>-0.370290</td>\n",
       "      <td>0.043540</td>\n",
       "      <td>0.829225</td>\n",
       "      <td>-0.064014</td>\n",
       "      <td>-0.483651</td>\n",
       "      <td>-0.211929</td>\n",
       "      <td>-0.476200</td>\n",
       "      <td>0.329590</td>\n",
       "      <td>0.198682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.378983</td>\n",
       "      <td>-0.251552</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.236010</td>\n",
       "      <td>-0.118237</td>\n",
       "      <td>0.238249</td>\n",
       "      <td>0.021712</td>\n",
       "      <td>-0.361023</td>\n",
       "      <td>0.069786</td>\n",
       "      <td>-0.160550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251145</td>\n",
       "      <td>-0.110919</td>\n",
       "      <td>-0.135493</td>\n",
       "      <td>0.366625</td>\n",
       "      <td>-0.005291</td>\n",
       "      <td>-0.294851</td>\n",
       "      <td>0.171291</td>\n",
       "      <td>-0.365220</td>\n",
       "      <td>0.365056</td>\n",
       "      <td>-0.100650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.636972</td>\n",
       "      <td>-0.253072</td>\n",
       "      <td>0.135902</td>\n",
       "      <td>0.064627</td>\n",
       "      <td>-0.064785</td>\n",
       "      <td>0.064222</td>\n",
       "      <td>-0.197405</td>\n",
       "      <td>-0.091370</td>\n",
       "      <td>-0.077241</td>\n",
       "      <td>0.072185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506079</td>\n",
       "      <td>-0.039346</td>\n",
       "      <td>-0.113541</td>\n",
       "      <td>0.668392</td>\n",
       "      <td>0.229442</td>\n",
       "      <td>-0.233961</td>\n",
       "      <td>-0.175184</td>\n",
       "      <td>-0.083833</td>\n",
       "      <td>0.589638</td>\n",
       "      <td>-0.416578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.538338</td>\n",
       "      <td>-0.365483</td>\n",
       "      <td>0.076125</td>\n",
       "      <td>-0.185224</td>\n",
       "      <td>0.100967</td>\n",
       "      <td>0.155169</td>\n",
       "      <td>-0.252380</td>\n",
       "      <td>0.025097</td>\n",
       "      <td>-0.291053</td>\n",
       "      <td>-0.185559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346202</td>\n",
       "      <td>-0.045257</td>\n",
       "      <td>-0.392972</td>\n",
       "      <td>0.310711</td>\n",
       "      <td>-0.071357</td>\n",
       "      <td>0.295142</td>\n",
       "      <td>0.315591</td>\n",
       "      <td>0.425145</td>\n",
       "      <td>0.427571</td>\n",
       "      <td>-0.189762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.597936</td>\n",
       "      <td>-0.358671</td>\n",
       "      <td>0.209767</td>\n",
       "      <td>-0.487063</td>\n",
       "      <td>-0.479696</td>\n",
       "      <td>0.256106</td>\n",
       "      <td>-0.140581</td>\n",
       "      <td>-0.136710</td>\n",
       "      <td>-0.016897</td>\n",
       "      <td>-0.130325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253609</td>\n",
       "      <td>-0.404088</td>\n",
       "      <td>-0.410870</td>\n",
       "      <td>0.527672</td>\n",
       "      <td>-0.070358</td>\n",
       "      <td>0.190427</td>\n",
       "      <td>0.433299</td>\n",
       "      <td>-0.184350</td>\n",
       "      <td>0.596232</td>\n",
       "      <td>-0.105290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.467166</td>\n",
       "      <td>-0.392304</td>\n",
       "      <td>0.226427</td>\n",
       "      <td>-0.429604</td>\n",
       "      <td>-0.312853</td>\n",
       "      <td>0.066769</td>\n",
       "      <td>-0.493390</td>\n",
       "      <td>-0.177944</td>\n",
       "      <td>0.302358</td>\n",
       "      <td>0.005486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131360</td>\n",
       "      <td>-0.473760</td>\n",
       "      <td>-0.285024</td>\n",
       "      <td>0.510621</td>\n",
       "      <td>-0.153012</td>\n",
       "      <td>0.167602</td>\n",
       "      <td>0.349074</td>\n",
       "      <td>-0.099915</td>\n",
       "      <td>0.394395</td>\n",
       "      <td>-0.400930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.554186</td>\n",
       "      <td>-0.288413</td>\n",
       "      <td>-0.102581</td>\n",
       "      <td>-0.389155</td>\n",
       "      <td>-0.035235</td>\n",
       "      <td>-0.175514</td>\n",
       "      <td>-0.326008</td>\n",
       "      <td>-0.200480</td>\n",
       "      <td>0.107697</td>\n",
       "      <td>-0.261754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028670</td>\n",
       "      <td>-0.268664</td>\n",
       "      <td>-0.067927</td>\n",
       "      <td>0.418846</td>\n",
       "      <td>0.022469</td>\n",
       "      <td>-0.114664</td>\n",
       "      <td>-0.151732</td>\n",
       "      <td>0.406275</td>\n",
       "      <td>0.237037</td>\n",
       "      <td>-0.029078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.677034</td>\n",
       "      <td>-0.047682</td>\n",
       "      <td>0.290770</td>\n",
       "      <td>-0.381469</td>\n",
       "      <td>-0.323120</td>\n",
       "      <td>-0.170329</td>\n",
       "      <td>-0.533560</td>\n",
       "      <td>0.038029</td>\n",
       "      <td>0.184373</td>\n",
       "      <td>-0.150321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003694</td>\n",
       "      <td>-0.484851</td>\n",
       "      <td>-0.216496</td>\n",
       "      <td>0.435315</td>\n",
       "      <td>0.029866</td>\n",
       "      <td>-0.261584</td>\n",
       "      <td>0.330693</td>\n",
       "      <td>-0.084482</td>\n",
       "      <td>0.366167</td>\n",
       "      <td>-0.054771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.403216</td>\n",
       "      <td>-0.072065</td>\n",
       "      <td>-0.024274</td>\n",
       "      <td>-0.068782</td>\n",
       "      <td>-0.150847</td>\n",
       "      <td>0.222412</td>\n",
       "      <td>0.063613</td>\n",
       "      <td>-0.298963</td>\n",
       "      <td>-0.262018</td>\n",
       "      <td>-0.426129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139798</td>\n",
       "      <td>-0.383043</td>\n",
       "      <td>-0.289298</td>\n",
       "      <td>0.818757</td>\n",
       "      <td>-0.079654</td>\n",
       "      <td>-0.389350</td>\n",
       "      <td>0.221705</td>\n",
       "      <td>-0.669612</td>\n",
       "      <td>0.429470</td>\n",
       "      <td>0.100483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.512254</td>\n",
       "      <td>-0.029711</td>\n",
       "      <td>0.131772</td>\n",
       "      <td>0.020333</td>\n",
       "      <td>-0.069355</td>\n",
       "      <td>0.292300</td>\n",
       "      <td>-0.382474</td>\n",
       "      <td>-0.086799</td>\n",
       "      <td>-0.036760</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267580</td>\n",
       "      <td>-0.063321</td>\n",
       "      <td>-0.274465</td>\n",
       "      <td>0.516002</td>\n",
       "      <td>0.345808</td>\n",
       "      <td>-0.161888</td>\n",
       "      <td>0.089980</td>\n",
       "      <td>-0.416275</td>\n",
       "      <td>0.260263</td>\n",
       "      <td>-0.274070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.717059</td>\n",
       "      <td>-0.160369</td>\n",
       "      <td>-0.011505</td>\n",
       "      <td>0.257515</td>\n",
       "      <td>0.128134</td>\n",
       "      <td>0.156203</td>\n",
       "      <td>-0.283288</td>\n",
       "      <td>0.407371</td>\n",
       "      <td>0.052193</td>\n",
       "      <td>-0.255562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.678573</td>\n",
       "      <td>-0.324516</td>\n",
       "      <td>-0.245674</td>\n",
       "      <td>0.542462</td>\n",
       "      <td>0.044638</td>\n",
       "      <td>-0.501661</td>\n",
       "      <td>-0.045473</td>\n",
       "      <td>-0.263542</td>\n",
       "      <td>0.324539</td>\n",
       "      <td>-0.032947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.561271</td>\n",
       "      <td>-0.060034</td>\n",
       "      <td>-0.120531</td>\n",
       "      <td>-0.296043</td>\n",
       "      <td>0.255579</td>\n",
       "      <td>0.353040</td>\n",
       "      <td>-0.215581</td>\n",
       "      <td>-0.583652</td>\n",
       "      <td>-0.314661</td>\n",
       "      <td>-0.094519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280568</td>\n",
       "      <td>-0.273177</td>\n",
       "      <td>0.252356</td>\n",
       "      <td>0.595628</td>\n",
       "      <td>0.181204</td>\n",
       "      <td>0.042897</td>\n",
       "      <td>0.100027</td>\n",
       "      <td>0.046675</td>\n",
       "      <td>0.162332</td>\n",
       "      <td>0.353731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.378120</td>\n",
       "      <td>-0.602076</td>\n",
       "      <td>0.130394</td>\n",
       "      <td>0.023939</td>\n",
       "      <td>-0.312931</td>\n",
       "      <td>0.118540</td>\n",
       "      <td>-0.308574</td>\n",
       "      <td>-0.162332</td>\n",
       "      <td>0.148283</td>\n",
       "      <td>-0.213539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050467</td>\n",
       "      <td>-0.516672</td>\n",
       "      <td>-0.188446</td>\n",
       "      <td>0.573062</td>\n",
       "      <td>0.011072</td>\n",
       "      <td>-0.130532</td>\n",
       "      <td>0.446660</td>\n",
       "      <td>0.048519</td>\n",
       "      <td>0.228400</td>\n",
       "      <td>-0.115873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.387770</td>\n",
       "      <td>-0.221687</td>\n",
       "      <td>0.209432</td>\n",
       "      <td>-0.335965</td>\n",
       "      <td>0.012790</td>\n",
       "      <td>0.436755</td>\n",
       "      <td>-0.407993</td>\n",
       "      <td>-0.305490</td>\n",
       "      <td>-0.464991</td>\n",
       "      <td>-0.044555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201287</td>\n",
       "      <td>-0.130632</td>\n",
       "      <td>-0.178453</td>\n",
       "      <td>0.416633</td>\n",
       "      <td>0.102306</td>\n",
       "      <td>0.086474</td>\n",
       "      <td>0.145243</td>\n",
       "      <td>0.121591</td>\n",
       "      <td>0.402643</td>\n",
       "      <td>-0.005412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.489335</td>\n",
       "      <td>-0.252099</td>\n",
       "      <td>-0.039114</td>\n",
       "      <td>0.045788</td>\n",
       "      <td>-0.039855</td>\n",
       "      <td>0.032530</td>\n",
       "      <td>-0.133718</td>\n",
       "      <td>-0.300392</td>\n",
       "      <td>0.065605</td>\n",
       "      <td>-0.427318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322686</td>\n",
       "      <td>-0.065189</td>\n",
       "      <td>0.018688</td>\n",
       "      <td>0.279902</td>\n",
       "      <td>0.006204</td>\n",
       "      <td>-0.170216</td>\n",
       "      <td>0.195324</td>\n",
       "      <td>-0.049234</td>\n",
       "      <td>0.220200</td>\n",
       "      <td>-0.158425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.693385</td>\n",
       "      <td>-0.402935</td>\n",
       "      <td>0.272139</td>\n",
       "      <td>-0.044233</td>\n",
       "      <td>-0.503395</td>\n",
       "      <td>0.222822</td>\n",
       "      <td>-0.039136</td>\n",
       "      <td>0.109511</td>\n",
       "      <td>0.092287</td>\n",
       "      <td>-0.136980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.399139</td>\n",
       "      <td>-0.611369</td>\n",
       "      <td>0.189200</td>\n",
       "      <td>0.414591</td>\n",
       "      <td>0.498813</td>\n",
       "      <td>-0.544231</td>\n",
       "      <td>-0.415466</td>\n",
       "      <td>0.079370</td>\n",
       "      <td>0.416062</td>\n",
       "      <td>0.320245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.594991</td>\n",
       "      <td>-0.061909</td>\n",
       "      <td>-0.186384</td>\n",
       "      <td>-0.013950</td>\n",
       "      <td>0.140468</td>\n",
       "      <td>0.126185</td>\n",
       "      <td>-0.125579</td>\n",
       "      <td>-0.049546</td>\n",
       "      <td>0.085307</td>\n",
       "      <td>-0.125551</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008847</td>\n",
       "      <td>-0.421442</td>\n",
       "      <td>-0.084133</td>\n",
       "      <td>0.426551</td>\n",
       "      <td>0.144577</td>\n",
       "      <td>-0.329679</td>\n",
       "      <td>0.219220</td>\n",
       "      <td>-0.106623</td>\n",
       "      <td>-0.044766</td>\n",
       "      <td>0.002710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.753932</td>\n",
       "      <td>-0.055446</td>\n",
       "      <td>-0.139433</td>\n",
       "      <td>-0.015887</td>\n",
       "      <td>0.115220</td>\n",
       "      <td>-0.131483</td>\n",
       "      <td>0.042237</td>\n",
       "      <td>0.085071</td>\n",
       "      <td>0.064462</td>\n",
       "      <td>-0.339391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.514164</td>\n",
       "      <td>-0.416938</td>\n",
       "      <td>-0.245596</td>\n",
       "      <td>0.246573</td>\n",
       "      <td>0.192295</td>\n",
       "      <td>-0.432222</td>\n",
       "      <td>-0.185253</td>\n",
       "      <td>-0.045742</td>\n",
       "      <td>0.307918</td>\n",
       "      <td>-0.147205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.684723</td>\n",
       "      <td>-0.017150</td>\n",
       "      <td>0.152844</td>\n",
       "      <td>-0.006975</td>\n",
       "      <td>0.290961</td>\n",
       "      <td>-0.055462</td>\n",
       "      <td>-0.177096</td>\n",
       "      <td>0.228441</td>\n",
       "      <td>-0.062594</td>\n",
       "      <td>-0.267527</td>\n",
       "      <td>...</td>\n",
       "      <td>0.584251</td>\n",
       "      <td>-0.522290</td>\n",
       "      <td>0.224084</td>\n",
       "      <td>0.599083</td>\n",
       "      <td>0.178253</td>\n",
       "      <td>-0.701042</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>-0.058474</td>\n",
       "      <td>0.225050</td>\n",
       "      <td>-0.028056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.341456</td>\n",
       "      <td>-0.219856</td>\n",
       "      <td>-0.233426</td>\n",
       "      <td>0.016449</td>\n",
       "      <td>0.233355</td>\n",
       "      <td>0.051864</td>\n",
       "      <td>-0.591106</td>\n",
       "      <td>-0.419599</td>\n",
       "      <td>0.126808</td>\n",
       "      <td>-0.391157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210530</td>\n",
       "      <td>-0.081044</td>\n",
       "      <td>-0.093414</td>\n",
       "      <td>0.513230</td>\n",
       "      <td>-0.062110</td>\n",
       "      <td>0.057366</td>\n",
       "      <td>-0.373013</td>\n",
       "      <td>0.217856</td>\n",
       "      <td>0.375380</td>\n",
       "      <td>0.192681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.618267</td>\n",
       "      <td>0.041395</td>\n",
       "      <td>-0.043230</td>\n",
       "      <td>0.043153</td>\n",
       "      <td>0.254764</td>\n",
       "      <td>-0.019954</td>\n",
       "      <td>-0.153476</td>\n",
       "      <td>0.141446</td>\n",
       "      <td>0.101391</td>\n",
       "      <td>-0.019044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133045</td>\n",
       "      <td>-0.272452</td>\n",
       "      <td>0.059609</td>\n",
       "      <td>0.624487</td>\n",
       "      <td>-0.000883</td>\n",
       "      <td>-0.445110</td>\n",
       "      <td>-0.054985</td>\n",
       "      <td>-0.186467</td>\n",
       "      <td>0.341305</td>\n",
       "      <td>-0.137729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.842152</td>\n",
       "      <td>0.204103</td>\n",
       "      <td>-0.099227</td>\n",
       "      <td>-0.347619</td>\n",
       "      <td>0.201602</td>\n",
       "      <td>0.053263</td>\n",
       "      <td>-0.115662</td>\n",
       "      <td>-0.030162</td>\n",
       "      <td>0.716764</td>\n",
       "      <td>-0.111887</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059789</td>\n",
       "      <td>-0.411974</td>\n",
       "      <td>-0.047497</td>\n",
       "      <td>0.737508</td>\n",
       "      <td>0.136690</td>\n",
       "      <td>-0.458246</td>\n",
       "      <td>-0.291156</td>\n",
       "      <td>-0.007507</td>\n",
       "      <td>0.225339</td>\n",
       "      <td>0.029083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.520451</td>\n",
       "      <td>0.015380</td>\n",
       "      <td>0.017007</td>\n",
       "      <td>-0.016956</td>\n",
       "      <td>0.004985</td>\n",
       "      <td>-0.196502</td>\n",
       "      <td>0.032638</td>\n",
       "      <td>-0.181564</td>\n",
       "      <td>-0.044991</td>\n",
       "      <td>-0.069870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119356</td>\n",
       "      <td>-0.134862</td>\n",
       "      <td>0.024154</td>\n",
       "      <td>0.569440</td>\n",
       "      <td>0.191793</td>\n",
       "      <td>-0.136277</td>\n",
       "      <td>-0.306528</td>\n",
       "      <td>-0.050154</td>\n",
       "      <td>0.376022</td>\n",
       "      <td>-0.087935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.255144</td>\n",
       "      <td>-0.403764</td>\n",
       "      <td>0.006387</td>\n",
       "      <td>-0.089042</td>\n",
       "      <td>-0.168545</td>\n",
       "      <td>0.086765</td>\n",
       "      <td>0.100165</td>\n",
       "      <td>0.086927</td>\n",
       "      <td>-0.074420</td>\n",
       "      <td>-0.270114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226970</td>\n",
       "      <td>-0.360764</td>\n",
       "      <td>0.168831</td>\n",
       "      <td>0.328308</td>\n",
       "      <td>-0.128092</td>\n",
       "      <td>-0.209181</td>\n",
       "      <td>0.050150</td>\n",
       "      <td>0.119662</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>-0.014817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.542557</td>\n",
       "      <td>-0.036758</td>\n",
       "      <td>0.078795</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>0.163540</td>\n",
       "      <td>-0.242437</td>\n",
       "      <td>0.271162</td>\n",
       "      <td>0.250510</td>\n",
       "      <td>0.081382</td>\n",
       "      <td>0.030441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.453650</td>\n",
       "      <td>-0.185285</td>\n",
       "      <td>-0.063034</td>\n",
       "      <td>0.490718</td>\n",
       "      <td>0.427780</td>\n",
       "      <td>-0.652075</td>\n",
       "      <td>0.105600</td>\n",
       "      <td>-0.086675</td>\n",
       "      <td>0.314870</td>\n",
       "      <td>-0.229467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.883901</td>\n",
       "      <td>-0.077509</td>\n",
       "      <td>-0.093506</td>\n",
       "      <td>0.183096</td>\n",
       "      <td>-0.109528</td>\n",
       "      <td>-0.156878</td>\n",
       "      <td>-0.200630</td>\n",
       "      <td>0.003755</td>\n",
       "      <td>-0.169277</td>\n",
       "      <td>-0.348693</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147407</td>\n",
       "      <td>-0.093767</td>\n",
       "      <td>0.279849</td>\n",
       "      <td>0.087729</td>\n",
       "      <td>0.077830</td>\n",
       "      <td>0.022955</td>\n",
       "      <td>0.024352</td>\n",
       "      <td>-0.034973</td>\n",
       "      <td>0.508312</td>\n",
       "      <td>-0.013266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.281227</td>\n",
       "      <td>0.183585</td>\n",
       "      <td>-0.053763</td>\n",
       "      <td>0.301274</td>\n",
       "      <td>0.397751</td>\n",
       "      <td>-0.193960</td>\n",
       "      <td>-0.051147</td>\n",
       "      <td>0.162756</td>\n",
       "      <td>0.199739</td>\n",
       "      <td>0.201938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206297</td>\n",
       "      <td>0.150314</td>\n",
       "      <td>-0.109234</td>\n",
       "      <td>0.511333</td>\n",
       "      <td>0.411646</td>\n",
       "      <td>-0.268795</td>\n",
       "      <td>0.014902</td>\n",
       "      <td>-0.104649</td>\n",
       "      <td>0.406082</td>\n",
       "      <td>-0.205307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0.466288</td>\n",
       "      <td>-0.073255</td>\n",
       "      <td>-0.147557</td>\n",
       "      <td>0.080012</td>\n",
       "      <td>0.317018</td>\n",
       "      <td>-0.003519</td>\n",
       "      <td>0.032687</td>\n",
       "      <td>-0.106634</td>\n",
       "      <td>0.071585</td>\n",
       "      <td>-0.046482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137958</td>\n",
       "      <td>0.099979</td>\n",
       "      <td>0.224067</td>\n",
       "      <td>0.275010</td>\n",
       "      <td>-0.043218</td>\n",
       "      <td>-0.322438</td>\n",
       "      <td>-0.168142</td>\n",
       "      <td>-0.005384</td>\n",
       "      <td>0.351995</td>\n",
       "      <td>0.006588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.407889</td>\n",
       "      <td>-0.053988</td>\n",
       "      <td>-0.131330</td>\n",
       "      <td>0.384929</td>\n",
       "      <td>0.509277</td>\n",
       "      <td>-0.173198</td>\n",
       "      <td>-0.359702</td>\n",
       "      <td>0.025165</td>\n",
       "      <td>0.246271</td>\n",
       "      <td>0.226698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539160</td>\n",
       "      <td>0.145821</td>\n",
       "      <td>-0.090664</td>\n",
       "      <td>0.579544</td>\n",
       "      <td>0.277601</td>\n",
       "      <td>-0.446192</td>\n",
       "      <td>-0.060376</td>\n",
       "      <td>-0.295085</td>\n",
       "      <td>0.504000</td>\n",
       "      <td>-0.260198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.592694</td>\n",
       "      <td>-0.020620</td>\n",
       "      <td>-0.023941</td>\n",
       "      <td>-0.358053</td>\n",
       "      <td>-0.085843</td>\n",
       "      <td>0.065114</td>\n",
       "      <td>-0.264121</td>\n",
       "      <td>-0.199889</td>\n",
       "      <td>0.164822</td>\n",
       "      <td>0.150782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.307056</td>\n",
       "      <td>-0.204739</td>\n",
       "      <td>-0.037277</td>\n",
       "      <td>1.075273</td>\n",
       "      <td>-0.082444</td>\n",
       "      <td>-0.240190</td>\n",
       "      <td>-0.161382</td>\n",
       "      <td>-0.433297</td>\n",
       "      <td>0.342292</td>\n",
       "      <td>-0.493173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0    0.901189 -0.429829  0.072537 -0.276152 -0.102324 -0.086483  0.191645   \n",
       "1    0.695339 -0.254250 -0.171089  0.001380  0.190561  0.006520  0.066503   \n",
       "2    0.431721 -0.501558 -0.125671 -0.229346  0.162066 -0.126459 -0.065437   \n",
       "3    0.809112 -0.515515  0.111718 -0.023580 -0.199963 -0.033900  0.157925   \n",
       "4    0.973768  0.072727 -0.182813  0.170368  0.078524 -0.044272  0.086718   \n",
       "5    0.793502 -0.353642  0.035973 -0.067531 -0.160598  0.252447 -0.247286   \n",
       "6    0.528605 -0.187904  0.225062 -0.067450  0.102634  0.231602 -0.177473   \n",
       "7    0.780326 -0.186257  0.310420 -0.351271 -0.072221  0.469847  0.027111   \n",
       "8    0.888432 -0.011046  0.291353 -0.142663 -0.245188  0.061572  0.030417   \n",
       "9    0.666570 -0.122884  0.171197  0.128577 -0.245623 -0.069877  0.207662   \n",
       "10   0.724431  0.084046  0.024213  0.151650  0.105990 -0.073179 -0.238586   \n",
       "11   0.992306 -0.622992 -0.037546 -0.012552 -0.064725  0.159655  0.146614   \n",
       "12   0.735475 -0.683696  0.353858 -0.099091 -0.389984  0.091399  0.076929   \n",
       "13   0.764557 -0.483421 -0.013476 -0.097209  0.058227  0.165232  0.111806   \n",
       "14   0.694002 -0.645488  0.126540 -0.111351 -0.198325 -0.020982  0.449097   \n",
       "15   0.710222 -0.645607  0.188873 -0.157526 -0.106837  0.225218 -0.302224   \n",
       "16   0.867346 -0.486642 -0.075895  0.239179 -0.189611  0.158737  0.171785   \n",
       "17   0.730183 -0.332017  0.163964  0.023879 -0.173423 -0.015763  0.293534   \n",
       "18   0.582446 -0.378028  0.097328  0.129944 -0.248229 -0.051842  0.243553   \n",
       "19   0.722013 -0.172732  0.052162 -0.225157  0.080901  0.181035 -0.051066   \n",
       "20   0.674128 -0.139736  0.022471 -0.215617 -0.053443  0.014383  0.198577   \n",
       "21   0.365088 -0.390356 -0.094513 -0.144332  0.107592  0.026050 -0.416890   \n",
       "22   0.742134 -0.557649  0.174629 -0.054127 -0.234820  0.035294  0.144151   \n",
       "23   0.658875 -0.329375 -0.016084  0.139410 -0.310596 -0.065925  0.237145   \n",
       "24   0.549006 -0.234354  0.012300 -0.057032 -0.245063 -0.065521 -0.079639   \n",
       "25   0.741801 -0.739494  0.308562 -0.149341 -0.460031 -0.079378  0.084274   \n",
       "26   0.779332 -0.473609 -0.083932  0.033362 -0.078407 -0.061510  0.265706   \n",
       "27   0.777261  0.050743 -0.190158 -0.013344  0.135204 -0.145747  0.136261   \n",
       "28   0.702403 -0.687170  0.272041  0.180767 -0.476761  0.085457 -0.121718   \n",
       "29   1.007299 -0.081757 -0.226707 -0.113309  0.099967 -0.041301  0.282903   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "140  0.312795 -0.146187  0.080841 -0.670533 -0.008729  0.393054 -0.114775   \n",
       "141  0.378983 -0.251552  0.000814  0.236010 -0.118237  0.238249  0.021712   \n",
       "142  0.636972 -0.253072  0.135902  0.064627 -0.064785  0.064222 -0.197405   \n",
       "143  0.538338 -0.365483  0.076125 -0.185224  0.100967  0.155169 -0.252380   \n",
       "144  0.597936 -0.358671  0.209767 -0.487063 -0.479696  0.256106 -0.140581   \n",
       "145  0.467166 -0.392304  0.226427 -0.429604 -0.312853  0.066769 -0.493390   \n",
       "146  0.554186 -0.288413 -0.102581 -0.389155 -0.035235 -0.175514 -0.326008   \n",
       "147  0.677034 -0.047682  0.290770 -0.381469 -0.323120 -0.170329 -0.533560   \n",
       "148  0.403216 -0.072065 -0.024274 -0.068782 -0.150847  0.222412  0.063613   \n",
       "149  0.512254 -0.029711  0.131772  0.020333 -0.069355  0.292300 -0.382474   \n",
       "150  0.717059 -0.160369 -0.011505  0.257515  0.128134  0.156203 -0.283288   \n",
       "151  0.561271 -0.060034 -0.120531 -0.296043  0.255579  0.353040 -0.215581   \n",
       "152  0.378120 -0.602076  0.130394  0.023939 -0.312931  0.118540 -0.308574   \n",
       "153  0.387770 -0.221687  0.209432 -0.335965  0.012790  0.436755 -0.407993   \n",
       "154  0.489335 -0.252099 -0.039114  0.045788 -0.039855  0.032530 -0.133718   \n",
       "155  0.693385 -0.402935  0.272139 -0.044233 -0.503395  0.222822 -0.039136   \n",
       "156  0.594991 -0.061909 -0.186384 -0.013950  0.140468  0.126185 -0.125579   \n",
       "157  0.753932 -0.055446 -0.139433 -0.015887  0.115220 -0.131483  0.042237   \n",
       "158  0.684723 -0.017150  0.152844 -0.006975  0.290961 -0.055462 -0.177096   \n",
       "159  0.341456 -0.219856 -0.233426  0.016449  0.233355  0.051864 -0.591106   \n",
       "160  0.618267  0.041395 -0.043230  0.043153  0.254764 -0.019954 -0.153476   \n",
       "161  0.842152  0.204103 -0.099227 -0.347619  0.201602  0.053263 -0.115662   \n",
       "162  0.520451  0.015380  0.017007 -0.016956  0.004985 -0.196502  0.032638   \n",
       "163  0.255144 -0.403764  0.006387 -0.089042 -0.168545  0.086765  0.100165   \n",
       "164  0.542557 -0.036758  0.078795  0.007072  0.163540 -0.242437  0.271162   \n",
       "165  0.883901 -0.077509 -0.093506  0.183096 -0.109528 -0.156878 -0.200630   \n",
       "166  0.281227  0.183585 -0.053763  0.301274  0.397751 -0.193960 -0.051147   \n",
       "167  0.466288 -0.073255 -0.147557  0.080012  0.317018 -0.003519  0.032687   \n",
       "168  0.407889 -0.053988 -0.131330  0.384929  0.509277 -0.173198 -0.359702   \n",
       "169  0.592694 -0.020620 -0.023941 -0.358053 -0.085843  0.065114 -0.264121   \n",
       "\n",
       "           7         8         9     ...           90        91        92  \\\n",
       "0   -0.482364  0.018132 -0.299763    ...     0.649043 -0.253990  0.359053   \n",
       "1   -0.067713 -0.037900 -0.177279    ...     0.606408 -0.254672  0.289679   \n",
       "2   -0.293841  0.274963 -0.092186    ...     0.526628 -0.098478  0.142099   \n",
       "3   -0.242619  0.170694 -0.153564    ...     0.787955 -0.254487  0.553937   \n",
       "4    0.079002 -0.119036 -0.419411    ...     0.361189 -0.477632  0.052283   \n",
       "5   -0.457005  0.026140 -0.088253    ...     0.437236 -0.106138 -0.214006   \n",
       "6    0.084507  0.291271  0.042524    ...     0.602439 -0.220547  0.011694   \n",
       "7   -0.193692 -0.208649 -0.140602    ...     0.582974 -0.420798 -0.017984   \n",
       "8   -0.300714 -0.274550 -0.174237    ...     0.610636 -0.255742  0.038166   \n",
       "9    0.108106 -0.116315 -0.269313    ...     0.653628 -0.486523  0.025241   \n",
       "10  -0.173252 -0.398118  0.046092    ...     0.430278  0.125561 -0.295341   \n",
       "11  -0.430713 -0.251893 -0.446166    ...     0.550828 -0.266724  0.107836   \n",
       "12  -0.439609  0.010741 -0.217991    ...     0.547953 -0.406121  0.239892   \n",
       "13  -0.568851 -0.039590 -0.112534    ...     0.581963 -0.132045  0.244377   \n",
       "14  -0.468345  0.068688 -0.476726    ...     0.820068 -0.270533  0.148415   \n",
       "15  -0.284722  0.121291 -0.200440    ...     0.437041 -0.363367 -0.087809   \n",
       "16  -0.361284 -0.185968 -0.667794    ...     0.625038 -0.437999 -0.086152   \n",
       "17  -0.241411  0.012665 -0.210887    ...     0.660821 -0.364117  0.392594   \n",
       "18  -0.013479  0.074761 -0.183352    ...     0.955924 -0.230237  0.031227   \n",
       "19   0.040666  0.239261 -0.134733    ...     0.911607 -0.141116 -0.154315   \n",
       "20  -0.182704 -0.174033 -0.242468    ...     0.455028 -0.193357  0.136215   \n",
       "21  -0.326435  0.114748 -0.178032    ...     0.788753 -0.122114 -0.337353   \n",
       "22  -0.147045 -0.068389 -0.213106    ...     0.873547 -0.340507  0.101957   \n",
       "23  -0.330996 -0.007831 -0.184283    ...     0.837604 -0.062648  0.260818   \n",
       "24  -0.156548  0.054001 -0.085603    ...     0.480299 -0.164290  0.239309   \n",
       "25  -0.077967  0.075482 -0.104612    ...     0.862400 -0.294552  0.358835   \n",
       "26  -0.393670  0.148718 -0.339413    ...     0.498039 -0.181604  0.538744   \n",
       "27  -0.189306 -0.082709 -0.313848    ...     0.423362 -0.233967  0.330380   \n",
       "28  -0.234537  0.134960 -0.206361    ...     0.538340 -0.286340  0.260384   \n",
       "29  -0.455703 -0.161899 -0.346360    ...     0.406088 -0.244097  0.528597   \n",
       "..        ...       ...       ...    ...          ...       ...       ...   \n",
       "140 -0.448272 -0.140716  0.063235    ...     0.419906 -0.370290  0.043540   \n",
       "141 -0.361023  0.069786 -0.160550    ...     0.251145 -0.110919 -0.135493   \n",
       "142 -0.091370 -0.077241  0.072185    ...     0.506079 -0.039346 -0.113541   \n",
       "143  0.025097 -0.291053 -0.185559    ...     0.346202 -0.045257 -0.392972   \n",
       "144 -0.136710 -0.016897 -0.130325    ...     0.253609 -0.404088 -0.410870   \n",
       "145 -0.177944  0.302358  0.005486    ...     0.131360 -0.473760 -0.285024   \n",
       "146 -0.200480  0.107697 -0.261754    ...     0.028670 -0.268664 -0.067927   \n",
       "147  0.038029  0.184373 -0.150321    ...     0.003694 -0.484851 -0.216496   \n",
       "148 -0.298963 -0.262018 -0.426129    ...     0.139798 -0.383043 -0.289298   \n",
       "149 -0.086799 -0.036760 -0.000039    ...     0.267580 -0.063321 -0.274465   \n",
       "150  0.407371  0.052193 -0.255562    ...     0.678573 -0.324516 -0.245674   \n",
       "151 -0.583652 -0.314661 -0.094519    ...     0.280568 -0.273177  0.252356   \n",
       "152 -0.162332  0.148283 -0.213539    ...     0.050467 -0.516672 -0.188446   \n",
       "153 -0.305490 -0.464991 -0.044555    ...     0.201287 -0.130632 -0.178453   \n",
       "154 -0.300392  0.065605 -0.427318    ...     0.322686 -0.065189  0.018688   \n",
       "155  0.109511  0.092287 -0.136980    ...     0.399139 -0.611369  0.189200   \n",
       "156 -0.049546  0.085307 -0.125551    ...    -0.008847 -0.421442 -0.084133   \n",
       "157  0.085071  0.064462 -0.339391    ...     0.514164 -0.416938 -0.245596   \n",
       "158  0.228441 -0.062594 -0.267527    ...     0.584251 -0.522290  0.224084   \n",
       "159 -0.419599  0.126808 -0.391157    ...     0.210530 -0.081044 -0.093414   \n",
       "160  0.141446  0.101391 -0.019044    ...     0.133045 -0.272452  0.059609   \n",
       "161 -0.030162  0.716764 -0.111887    ...    -0.059789 -0.411974 -0.047497   \n",
       "162 -0.181564 -0.044991 -0.069870    ...     0.119356 -0.134862  0.024154   \n",
       "163  0.086927 -0.074420 -0.270114    ...     0.226970 -0.360764  0.168831   \n",
       "164  0.250510  0.081382  0.030441    ...     0.453650 -0.185285 -0.063034   \n",
       "165  0.003755 -0.169277 -0.348693    ...     0.147407 -0.093767  0.279849   \n",
       "166  0.162756  0.199739  0.201938    ...     0.206297  0.150314 -0.109234   \n",
       "167 -0.106634  0.071585 -0.046482    ...     0.137958  0.099979  0.224067   \n",
       "168  0.025165  0.246271  0.226698    ...     0.539160  0.145821 -0.090664   \n",
       "169 -0.199889  0.164822  0.150782    ...     0.307056 -0.204739 -0.037277   \n",
       "\n",
       "           93        94        95        96        97        98        99  \n",
       "0    0.147686  0.055130 -0.029171 -0.174130 -0.064483  0.199549 -0.050065  \n",
       "1    0.203524  0.413498 -0.326171 -0.006671  0.036370  0.295730  0.161813  \n",
       "2    0.217613  0.214516 -0.120521 -0.275076  0.145664  0.234348  0.017835  \n",
       "3    0.237264  0.081551 -0.417421 -0.228023  0.069544  0.207278  0.045199  \n",
       "4    0.579035  0.175000 -0.379824 -0.248732 -0.319573  0.441516 -0.083097  \n",
       "5    0.363571  0.351176  0.055422  0.202854 -0.215443 -0.032757  0.067539  \n",
       "6    0.412785  0.234136 -0.351188 -0.042189 -0.111035  0.106946  0.033110  \n",
       "7    0.365377  0.349382 -0.104692  0.229274  0.009290  0.339272  0.241276  \n",
       "8    0.412595  0.251110 -0.017999  0.495548 -0.312582  0.361658  0.166411  \n",
       "9    0.344874  0.283071 -0.502361  0.264869 -0.174671  0.193859  0.176984  \n",
       "10   0.407353  0.188891  0.065163 -0.141379 -0.024193  0.485507 -0.169262  \n",
       "11   0.469718  0.053655 -0.046802  0.258400 -0.240424  0.380557  0.005610  \n",
       "12   0.295686  0.099615  0.013574  0.014414 -0.109636  0.000639  0.159384  \n",
       "13   0.394409  0.218964 -0.048158  0.151100 -0.121438  0.042929  0.036125  \n",
       "14   0.118659 -0.022891 -0.204031  0.152720 -0.074506  0.052210  0.007829  \n",
       "15   0.277700  0.035143  0.107443  0.005717  0.279443  0.014054 -0.098826  \n",
       "16   0.597943  0.233123 -0.496111  0.525102 -0.250782  0.086979 -0.013610  \n",
       "17   0.144690  0.133155 -0.279313  0.330178 -0.075691 -0.036461  0.186498  \n",
       "18   0.050923  0.264776 -0.597880  0.043895 -0.218730 -0.177743  0.151537  \n",
       "19   0.208176  0.214750 -0.295792 -0.130707 -0.269010  0.038447 -0.035663  \n",
       "20   0.277116  0.309885 -0.145417  0.117610 -0.139196 -0.187495  0.404533  \n",
       "21   0.238990  0.333730 -0.276520  0.074214 -0.096524  0.311824 -0.120922  \n",
       "22   0.037941  0.332140 -0.395525  0.083798 -0.143875 -0.224044  0.268158  \n",
       "23   0.276173  0.525183 -0.658834 -0.047433 -0.158358  0.135419  0.269249  \n",
       "24   0.117797  0.288546 -0.214383 -0.227600 -0.017484  0.127544  0.071374  \n",
       "25   0.173644  0.049690 -0.367543 -0.453822  0.125762  0.243909  0.040782  \n",
       "26   0.398402  0.267943 -0.133994 -0.052655  0.156561 -0.108014  0.168952  \n",
       "27   0.289080  0.652835 -0.475044  0.110475 -0.264221  0.257415  0.144920  \n",
       "28   0.247529  0.163528 -0.191433 -0.020691  0.020723  0.354150  0.087899  \n",
       "29   0.365624  0.234942 -0.259192  0.090542 -0.206987  0.189673  0.034898  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "140  0.829225 -0.064014 -0.483651 -0.211929 -0.476200  0.329590  0.198682  \n",
       "141  0.366625 -0.005291 -0.294851  0.171291 -0.365220  0.365056 -0.100650  \n",
       "142  0.668392  0.229442 -0.233961 -0.175184 -0.083833  0.589638 -0.416578  \n",
       "143  0.310711 -0.071357  0.295142  0.315591  0.425145  0.427571 -0.189762  \n",
       "144  0.527672 -0.070358  0.190427  0.433299 -0.184350  0.596232 -0.105290  \n",
       "145  0.510621 -0.153012  0.167602  0.349074 -0.099915  0.394395 -0.400930  \n",
       "146  0.418846  0.022469 -0.114664 -0.151732  0.406275  0.237037 -0.029078  \n",
       "147  0.435315  0.029866 -0.261584  0.330693 -0.084482  0.366167 -0.054771  \n",
       "148  0.818757 -0.079654 -0.389350  0.221705 -0.669612  0.429470  0.100483  \n",
       "149  0.516002  0.345808 -0.161888  0.089980 -0.416275  0.260263 -0.274070  \n",
       "150  0.542462  0.044638 -0.501661 -0.045473 -0.263542  0.324539 -0.032947  \n",
       "151  0.595628  0.181204  0.042897  0.100027  0.046675  0.162332  0.353731  \n",
       "152  0.573062  0.011072 -0.130532  0.446660  0.048519  0.228400 -0.115873  \n",
       "153  0.416633  0.102306  0.086474  0.145243  0.121591  0.402643 -0.005412  \n",
       "154  0.279902  0.006204 -0.170216  0.195324 -0.049234  0.220200 -0.158425  \n",
       "155  0.414591  0.498813 -0.544231 -0.415466  0.079370  0.416062  0.320245  \n",
       "156  0.426551  0.144577 -0.329679  0.219220 -0.106623 -0.044766  0.002710  \n",
       "157  0.246573  0.192295 -0.432222 -0.185253 -0.045742  0.307918 -0.147205  \n",
       "158  0.599083  0.178253 -0.701042  0.112100 -0.058474  0.225050 -0.028056  \n",
       "159  0.513230 -0.062110  0.057366 -0.373013  0.217856  0.375380  0.192681  \n",
       "160  0.624487 -0.000883 -0.445110 -0.054985 -0.186467  0.341305 -0.137729  \n",
       "161  0.737508  0.136690 -0.458246 -0.291156 -0.007507  0.225339  0.029083  \n",
       "162  0.569440  0.191793 -0.136277 -0.306528 -0.050154  0.376022 -0.087935  \n",
       "163  0.328308 -0.128092 -0.209181  0.050150  0.119662  0.001180 -0.014817  \n",
       "164  0.490718  0.427780 -0.652075  0.105600 -0.086675  0.314870 -0.229467  \n",
       "165  0.087729  0.077830  0.022955  0.024352 -0.034973  0.508312 -0.013266  \n",
       "166  0.511333  0.411646 -0.268795  0.014902 -0.104649  0.406082 -0.205307  \n",
       "167  0.275010 -0.043218 -0.322438 -0.168142 -0.005384  0.351995  0.006588  \n",
       "168  0.579544  0.277601 -0.446192 -0.060376 -0.295085  0.504000 -0.260198  \n",
       "169  1.075273 -0.082444 -0.240190 -0.161382 -0.433297  0.342292 -0.493173  \n",
       "\n",
       "[170 rows x 100 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "   \n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "# get document level embeddings\n",
    "ft_feature_array = averaged_word_vectorizer(corpus=tokenized_corpus, model=ft_model,\n",
    "                                             num_features=feature_size)\n",
    "pd.DataFrame(ft_feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ft_feature_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "################ Text Classification Accuracy - Glove-Word_Vectors\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=12)\n",
    "Y_train = Y_train.astype(int)\n",
    "Y_test = Y_test.astype(int)\n",
    "# print(len(X_train)) # 119\n",
    "# print(len(X_test)) # 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.38776971 -0.2216868   0.20943223 ...  0.12159078  0.40264293\n",
      "  -0.00541164]\n",
      " [ 0.85644223  0.08876789 -0.29862993 ... -0.15438376  0.18732723\n",
      "  -0.62196681]\n",
      " [ 0.78082934 -0.53663024  0.09740495 ...  0.0108951   0.38896559\n",
      "  -0.12313614]\n",
      " ...\n",
      " [ 0.52820183 -0.3316175  -0.14240122 ...  0.03900575  0.26242312\n",
      "  -0.02690729]\n",
      " [ 0.6933851  -0.40293498  0.27213865 ...  0.07936968  0.41606206\n",
      "   0.32024485]\n",
      " [ 0.71700441 -0.26136195 -0.09686725 ...  0.0133314   0.22036277\n",
      "  -0.18819032]]\n",
      "[[ 0.85865485 -0.55311524  0.20650863 ... -0.3772923   0.29789165\n",
      "  -0.18515526]\n",
      " [ 0.65887522 -0.32937474 -0.01608407 ... -0.15835804  0.13541921\n",
      "   0.26924939]\n",
      " [ 0.53833791 -0.3654828   0.07612492 ...  0.42514542  0.42757055\n",
      "  -0.18976219]\n",
      " ...\n",
      " [ 0.86270611  0.22517526 -0.20812471 ...  0.1236121   0.16589263\n",
      "  -0.34491893]\n",
      " [ 0.54900582 -0.2343535   0.01229956 ... -0.01748444  0.12754433\n",
      "   0.07137448]\n",
      " [ 0.72201277 -0.17273171  0.05216164 ... -0.26900962  0.03844724\n",
      "  -0.0356634 ]]\n"
     ]
    }
   ],
   "source": [
    "X_train = np.asmatrix(X_train)\n",
    "print(X_train)\n",
    "X_test = np.asmatrix(X_test)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import classification related packages\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Random Forest ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def rf_classifier(X_train, Y_train, n_estimators, max_depth, max_features, random_state):\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features, random_state=random_state)   \n",
    "    Y_train = np.asarray(Y_train, dtype=\"|S6\")\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model\n",
    "\n",
    "def rf_predictions(model, X_test, Y_test):\n",
    "    y_pred = model.predict_proba(X_test) # default 'predict' gives 'categorical' predictions for RF\n",
    "    # classifier.classes_ # to check order of classes in 'predict_proba' (most prob. it's 0,1...)\n",
    "    y_pred = y_pred[:,1] # accessing 2nd column or '1' class probabilities\n",
    "    # converting prob to 1/0 and array type to int\n",
    "    y_pred[y_pred > 0.5] = 1\n",
    "    y_pred[y_pred <= 0.5] = 0\n",
    "    return y_pred\n",
    "\n",
    "## create binary classification report (multi-class not supported)\n",
    "def create_reports(y_pred, Y_test):\n",
    "    # creating CF matrix\n",
    "    Y_test = Y_test.astype(int)\n",
    "    y_pred = y_pred.astype(int)\n",
    "    cm1 = confusion_matrix(y_pred, Y_test)\n",
    "    print(\"\\n--Confusion Matrix--\\n\")\n",
    "    print(cm1)\n",
    "    # calculating auc\n",
    "    fp_rate, tp_rate, thresholds = roc_curve(Y_test, y_pred)\n",
    "    print(\"\\n-------AUC-------\\n\")\n",
    "    print(auc(fp_rate, tp_rate))\n",
    "    print(\"\\n----------------Classification_Report------------------\\n\")\n",
    "    print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--Confusion Matrix--\n",
      "\n",
      "[[27 15]\n",
      " [ 0  9]]\n",
      "\n",
      "-------AUC-------\n",
      "\n",
      "0.6875\n",
      "\n",
      "----------------Classification_Report------------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      1.00      0.78        27\n",
      "           1       1.00      0.38      0.55        24\n",
      "\n",
      "   micro avg       0.71      0.71      0.71        51\n",
      "   macro avg       0.82      0.69      0.66        51\n",
      "weighted avg       0.81      0.71      0.67        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = rf_classifier(X_train, Y_train, n_estimators=300, max_depth=8, max_features=2, random_state=0)\n",
    "y_pred = rf_predictions(model, X_test, Y_test)\n",
    "create_reports(y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insight: Performance improved drastically over RF using regular text classification\n",
    "    # refer: '9.1 Text Classification-One txt column' notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Linear/Non-Linear SVM ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def svm_classifier(X_train, Y_train, kernel, C, gamma):\n",
    "    model = SVC(kernel=kernel, C=C, gamma=gamma) # kernel = rbf/linear \n",
    "    model.fit(X_train, Y_train) \n",
    "    return model\n",
    "\n",
    "def common_predictions(model, X_test, Y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = y_pred.astype(int)\n",
    "    y_pred[y_pred > 0.5] = 1\n",
    "    y_pred[y_pred <= 0.5] = 0\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--Confusion Matrix--\n",
      "\n",
      "[[25  7]\n",
      " [ 2 17]]\n",
      "\n",
      "-------AUC-------\n",
      "\n",
      "0.8171296296296298\n",
      "\n",
      "----------------Classification_Report------------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.93      0.85        27\n",
      "           1       0.89      0.71      0.79        24\n",
      "\n",
      "   micro avg       0.82      0.82      0.82        51\n",
      "   macro avg       0.84      0.82      0.82        51\n",
      "weighted avg       0.83      0.82      0.82        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = svm_classifier(X_train, Y_train, kernel='rbf', C=20, gamma=0.2)\n",
    "# model = svm_classifier(X_train, Y_train, kernel='linear', C=15, gamma=0.2)\n",
    "y_pred = common_predictions(model, X_test, Y_test)\n",
    "create_reports(y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insight: performance of SVM dipped by using word2vec\n",
    "    # because number of vars have reduced from 1500 to 100 and N performs well with more vars\n",
    "    # refer: '9.1 Text Classification-One txt column' notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Naive Bayes ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def nb_classifier(X_train, Y_train):\n",
    "    model = GaussianNB()\n",
    "    temp = X_train.copy()\n",
    "    model.fit(temp, Y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--Confusion Matrix--\n",
      "\n",
      "[[22 12]\n",
      " [ 5 12]]\n",
      "\n",
      "-------AUC-------\n",
      "\n",
      "0.6574074074074074\n",
      "\n",
      "----------------Classification_Report------------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.81      0.72        27\n",
      "           1       0.71      0.50      0.59        24\n",
      "\n",
      "   micro avg       0.67      0.67      0.67        51\n",
      "   macro avg       0.68      0.66      0.65        51\n",
      "weighted avg       0.67      0.67      0.66        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = nb_classifier(X_train, Y_train)\n",
    "y_pred = common_predictions(model, X_test, Y_test)\n",
    "create_reports(y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insight: performance of NB dipped by using word2vec\n",
    "    # because number of vars have reduced from 1500 to 100 and N performs well with more vars\n",
    "    # refer: '9.1 Text Classification-One txt column' notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ LR ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def lr_classifier(X_train, Y_train, C):\n",
    "    model = LogisticRegression(C=C) # Inverse of regularization strength\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--Confusion Matrix--\n",
      "\n",
      "[[25  7]\n",
      " [ 2 17]]\n",
      "\n",
      "-------AUC-------\n",
      "\n",
      "0.8171296296296298\n",
      "\n",
      "----------------Classification_Report------------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.93      0.85        27\n",
      "           1       0.89      0.71      0.79        24\n",
      "\n",
      "   micro avg       0.82      0.82      0.82        51\n",
      "   macro avg       0.84      0.82      0.82        51\n",
      "weighted avg       0.83      0.82      0.82        51\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antrived/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model = lr_classifier(X_train, Y_train, C=50)\n",
    "y_pred = common_predictions(model, X_test, Y_test)\n",
    "create_reports(y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR performance improved significantly by using word2vec\n",
    "    # refer: '9.1 Text Classification-One txt column' notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ XGBoost #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge xgboost # some issues\n",
    "# sudo pip install xgboost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def xgb_classifier(X_train, Y_train, max_depth, learning_rate, n_estimators, objective):\n",
    "    model = XGBClassifier(max_depth=max_depth, learning_rate=learning_rate, n_estimators=n_estimators, objective=objective)\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--Confusion Matrix--\n",
      "\n",
      "[[26 10]\n",
      " [ 1 14]]\n",
      "\n",
      "-------AUC-------\n",
      "\n",
      "0.7731481481481484\n",
      "\n",
      "----------------Classification_Report------------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.96      0.83        27\n",
      "           1       0.93      0.58      0.72        24\n",
      "\n",
      "   micro avg       0.78      0.78      0.78        51\n",
      "   macro avg       0.83      0.77      0.77        51\n",
      "weighted avg       0.82      0.78      0.77        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = xgb_classifier(X_train, Y_train, max_depth=2, learning_rate=0.2, n_estimators=300, \n",
    "                       objective='binary:logistic')\n",
    "y_pred = common_predictions(model, X_test, Y_test)\n",
    "create_reports(y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost performance improved significantly by using word2vec\n",
    "    # refer: '9.1 Text Classification-One txt column' notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ ANN #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing keras libraries and packages:\n",
    "import keras\n",
    "from keras.models import Sequential # to initialize the ANN\n",
    "from keras.layers import Dense # to create layers of ANN\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/antrived/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/40\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 0.6919 - acc: 0.5630\n",
      "Epoch 2/40\n",
      "119/119 [==============================] - 0s 717us/step - loss: 0.6842 - acc: 0.6134\n",
      "Epoch 3/40\n",
      "119/119 [==============================] - 0s 573us/step - loss: 0.6255 - acc: 0.6134\n",
      "Epoch 4/40\n",
      "119/119 [==============================] - 0s 623us/step - loss: 0.5867 - acc: 0.6134\n",
      "Epoch 5/40\n",
      "119/119 [==============================] - 0s 848us/step - loss: 0.6868 - acc: 0.6134\n",
      "Epoch 6/40\n",
      "119/119 [==============================] - 0s 672us/step - loss: 0.6812 - acc: 0.6134\n",
      "Epoch 7/40\n",
      "119/119 [==============================] - 0s 675us/step - loss: 0.6161 - acc: 0.6387\n",
      "Epoch 8/40\n",
      "119/119 [==============================] - 0s 668us/step - loss: 0.4909 - acc: 0.7899\n",
      "Epoch 9/40\n",
      "119/119 [==============================] - 0s 693us/step - loss: 0.4792 - acc: 0.8235\n",
      "Epoch 10/40\n",
      "119/119 [==============================] - 0s 801us/step - loss: 0.4262 - acc: 0.8151\n",
      "Epoch 11/40\n",
      "119/119 [==============================] - 0s 675us/step - loss: 0.3440 - acc: 0.8655\n",
      "Epoch 12/40\n",
      "119/119 [==============================] - 0s 680us/step - loss: 0.3481 - acc: 0.8403\n",
      "Epoch 13/40\n",
      "119/119 [==============================] - 0s 729us/step - loss: 0.2341 - acc: 0.8992\n",
      "Epoch 14/40\n",
      "119/119 [==============================] - 0s 789us/step - loss: 0.2796 - acc: 0.8571\n",
      "Epoch 15/40\n",
      "119/119 [==============================] - 0s 657us/step - loss: 0.1995 - acc: 0.9244\n",
      "Epoch 16/40\n",
      "119/119 [==============================] - 0s 735us/step - loss: 0.2775 - acc: 0.9160\n",
      "Epoch 17/40\n",
      "119/119 [==============================] - 0s 923us/step - loss: 0.1705 - acc: 0.9328\n",
      "Epoch 18/40\n",
      "119/119 [==============================] - 0s 778us/step - loss: 0.1191 - acc: 0.9664\n",
      "Epoch 19/40\n",
      "119/119 [==============================] - 0s 654us/step - loss: 0.0804 - acc: 0.9748\n",
      "Epoch 20/40\n",
      "119/119 [==============================] - 0s 697us/step - loss: 0.0656 - acc: 0.9748\n",
      "Epoch 21/40\n",
      "119/119 [==============================] - 0s 695us/step - loss: 0.0418 - acc: 0.9832\n",
      "Epoch 22/40\n",
      "119/119 [==============================] - 0s 672us/step - loss: 0.0315 - acc: 0.9832\n",
      "Epoch 23/40\n",
      "119/119 [==============================] - 0s 665us/step - loss: 0.0262 - acc: 0.9832\n",
      "Epoch 24/40\n",
      "119/119 [==============================] - 0s 611us/step - loss: 0.0122 - acc: 1.0000\n",
      "Epoch 25/40\n",
      "119/119 [==============================] - 0s 548us/step - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 26/40\n",
      "119/119 [==============================] - 0s 620us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 27/40\n",
      "119/119 [==============================] - 0s 558us/step - loss: 0.1409 - acc: 0.9832\n",
      "Epoch 28/40\n",
      "119/119 [==============================] - 0s 639us/step - loss: 9.1099 - acc: 0.4286\n",
      "Epoch 29/40\n",
      "119/119 [==============================] - 0s 685us/step - loss: 9.7798 - acc: 0.3866\n",
      "Epoch 30/40\n",
      "119/119 [==============================] - 0s 677us/step - loss: 9.7798 - acc: 0.3866\n",
      "Epoch 31/40\n",
      "119/119 [==============================] - 0s 820us/step - loss: 9.7798 - acc: 0.3866\n",
      "Epoch 32/40\n",
      "119/119 [==============================] - 0s 643us/step - loss: 9.7798 - acc: 0.3866\n",
      "Epoch 33/40\n",
      "119/119 [==============================] - 0s 654us/step - loss: 9.7798 - acc: 0.3866\n",
      "Epoch 34/40\n",
      "119/119 [==============================] - 0s 665us/step - loss: 9.7798 - acc: 0.3866\n",
      "Epoch 35/40\n",
      "119/119 [==============================] - 0s 624us/step - loss: 9.7798 - acc: 0.3866\n",
      "Epoch 36/40\n",
      "119/119 [==============================] - 0s 585us/step - loss: 9.7798 - acc: 0.3866\n",
      "Epoch 37/40\n",
      "119/119 [==============================] - 0s 583us/step - loss: 9.7798 - acc: 0.3866\n",
      "Epoch 38/40\n",
      "119/119 [==============================] - 0s 583us/step - loss: 9.7798 - acc: 0.3866\n",
      "Epoch 39/40\n",
      "119/119 [==============================] - 0s 713us/step - loss: 9.7798 - acc: 0.3866\n",
      "Epoch 40/40\n",
      "119/119 [==============================] - 0s 721us/step - loss: 9.7798 - acc: 0.3866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f81cd808cf8>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Basic ANN\n",
    "\n",
    "np.random.seed(100) # works. gives close results on repeating but not exactly same.\n",
    "\n",
    "# Initializing ANN\n",
    "model = Sequential()\n",
    "\n",
    "i = 0.20\n",
    "j = 125\n",
    "\n",
    "# Adding input layer and 1st hidden layer\n",
    "model.add(Dense(activation=\"relu\", input_dim=100, units=j, kernel_initializer=\"uniform\"))\n",
    "# model.add(Dropout(rate=i))\n",
    "# Adding 2nd hidden layer\n",
    "model.add(Dense(activation=\"relu\", units=j, kernel_initializer=\"uniform\"))\n",
    "# model.add(Dropout(rate=i))\n",
    "model.add(Dense(activation=\"relu\", units=j, kernel_initializer=\"uniform\"))\n",
    "# model.add(Dropout(rate=i))\n",
    "model.add(Dense(activation=\"relu\", units=j, kernel_initializer=\"uniform\"))\n",
    "# model.add(Dropout(rate=i))\n",
    "model.add(Dense(activation=\"relu\", units=j, kernel_initializer=\"uniform\"))\n",
    "# model.add(Dropout(rate=i))\n",
    "model.add(Dense(activation=\"relu\", units=j, kernel_initializer=\"uniform\"))\n",
    "# model.add(Dropout(rate=i))\n",
    "model.add(Dense(activation=\"relu\", units=j, kernel_initializer=\"uniform\"))\n",
    "# model.add(Dropout(rate=i))\n",
    "model.add(Dense(activation=\"relu\", units=j, kernel_initializer=\"uniform\"))\n",
    "# model.add(Dropout(rate=i))\n",
    "# model.add(Dense(activation=\"relu\", units=j, kernel_initializer=\"uniform\"))\n",
    "# model.add(Dense(activation=\"relu\", units=j, kernel_initializer=\"uniform\"))\n",
    "# model.add(Dense(activation=\"relu\", units=j, kernel_initializer=\"uniform\"))\n",
    "# model.add(Dense(activation=\"relu\", units=j, kernel_initializer=\"uniform\"))\n",
    "# model.add(Dense(activation=\"relu\", units=j, kernel_initializer=\"uniform\"))\n",
    "# model.add(Dense(activation=\"relu\", units=j, kernel_initializer=\"uniform\"))\n",
    "# model.add(Dense(activation=\"relu\", units=j, kernel_initializer=\"uniform\"))\n",
    "# model.add(Dense(activation=\"relu\", units=j, kernel_initializer=\"uniform\"))\n",
    "# model.add(Dense(activation=\"relu\", units=j, kernel_initializer=\"uniform\"))\n",
    "# # Adding hidden layer\n",
    "# classifier.add(Dense(activation=\"relu\", units=70, kernel_initializer=\"uniform\"))\n",
    "# # Adding hidden layer\n",
    "# classifier.add(Dense(activation=\"relu\", units=7, kernel_initializer=\"uniform\"))\n",
    "# # Adding hidden layer\n",
    "\n",
    "# Adding output layer\n",
    "model.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Compiling the ANN (means run SGD on ANN)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fitting ANN to training set\n",
    "model.fit(X_train, Y_train, batch_size=5, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--Confusion Matrix--\n",
      "\n",
      "[[ 0  0]\n",
      " [27 24]]\n",
      "\n",
      "-------AUC-------\n",
      "\n",
      "0.5\n",
      "\n",
      "----------------Classification_Report------------------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        27\n",
      "           1       0.47      1.00      0.64        24\n",
      "\n",
      "   micro avg       0.47      0.47      0.47        51\n",
      "   macro avg       0.24      0.50      0.32        51\n",
      "weighted avg       0.22      0.47      0.30        51\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antrived/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = common_predictions(model, X_test, Y_test)\n",
    "create_reports(y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText gave better results than tf-idf, gensin w2v, glove\n",
    "    # refer: '9.1 Text Classification-One txt column' notebook\n",
    "    # FastText best acc is from svm(rbf) and log_reg - 0.8171"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "################ Text Classification Scoring\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use 'averaged_word_vectorizer' for new incoming data (check above)\n",
    " # the way it is used for current 'tokenized_corpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "################ Text Classification Accuracy - GloVe\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "################ Embeddings Generation - FastText\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "################ Text Classification Accuracy - FastText\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
