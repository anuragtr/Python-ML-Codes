{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anaconda & Python Version:\n",
    "\n",
    "The version of the notebook server is 5.0.0 and is running on:\n",
    "Python 3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 10:22:32) [MSC v.1900 64 bit (AMD64)]\n",
    "\n",
    "Current Kernel Information:\n",
    "Python 3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 10:22:32) [MSC v.1900 64 bit (AMD64)]\n",
    "Type 'copyright', 'credits' or 'license' for more information\n",
    "IPython 6.1.0 -- An enhanced Interactive Python. Type '?' for help.\n",
    "\n",
    "#### Anaconda 5.0.1 (not sure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "#### 1. Train and Test Manual Data PREP - Images to kept in separate folders: Cats (Names as Cat1, Cat2...) & Dogs\n",
    "#### 2.\n",
    "#### 3.\n",
    "#### 4.\n",
    "#### 5.\n",
    "#### 6.\n",
    "#### 7.\n",
    "#### 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# 1. Train and Validation Manual Data PREP - Images to kept in separate folders: Cats (Names as Cat1, Cat2...) & Dogs\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn # same as scikit-learn\n",
    "\n",
    "# Installing Theano, Tensorflow, Keras, Opencv:\n",
    "# check ANN code for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Keras libraries and packages:\n",
    "import keras\n",
    "from keras.models import Sequential # to initialize the NN\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense # to create layers of NN\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "# importing required package for Image Pre-Processing/Augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anurag.trivedi\\2. Data\\2. CNN Data\\2.1 Data CNN\\Convolutional_Neural_Networks\\dataset\n"
     ]
    }
   ],
   "source": [
    "# Windows - change wd(binary):\n",
    "# Set Udemy folder containing Training and Validation as Working Directory(WD):\n",
    "# 'pwd' command to check current WD: 'C:\\\\Users\\\\anurag.trivedi' (type 'pwd' in new para)\n",
    "# Changing WD:\n",
    "%cd \"C:\\Users\\anurag.trivedi\\2. Data\\2. CNN Data\\2.1 Data CNN\\Convolutional_Neural_Networks\\dataset\"\n",
    "# Reason: Setting dataset folder WD and having images in special folder structure(as per Sec 10, Lecture 45 of Udemy DL AtoZ Course) and is necessary to run Keras code '.flow_from_directory(directory)' and maybe more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linux - change wd(binary):\n",
    "import os\n",
    "os.chdir(\"/home/antrived/Dropbox/Job-25-Feb-2019/prep/x2-more new projects & new techniques/1. Techniques & Algos/0. Deep Learning/2. CNN/2.1 Data CNN/Convolutional_Neural_Networks/dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linux - change wd(multiclass)\n",
    "# change wd:\n",
    "import os\n",
    "os.chdir(\"/home/antrived/Dropbox/Job-25-Feb-2019/prep/x2-more new projects & new techniques/1. Techniques & Algos/0. Deep Learning/2. CNN/2.1 Data CNN/Convolutional_Neural_Networks/dataset/multiclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Building CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/antrived/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antrived/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
      "  \n",
      "/home/antrived/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "/home/antrived/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From /home/antrived/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antrived/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:83: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/antrived/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:83: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., epochs=5, validation_data=<keras_pre..., steps_per_epoch=250, validation_steps=2000)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 49/250 [====>.........................] - ETA: 38s - loss: 0.7636 - acc: 0.5204"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-66fc48087bd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# default was 25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         nb_val_samples=2000) # number of images in validation_set\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Basic Model - Binary Classification (1 Convolutional layer and 1 Fully Connected layer)\n",
    "# Not tuning performed\n",
    "\n",
    "# Initializing CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1: Convolution - Adding Convolution Layer:\n",
    "classifier.add(Convolution2D(  32,3,3,  input_shape=(64,64,3),  activation='relu'  ))\n",
    "# parameter info: https://www.udemy.com/deeplearning/learn/v4/t/lecture/6743906?start=0\n",
    "# 32,3,3 - 32 different 'feature detectors' are used of size, 3 rows and 3 columns\n",
    "# 64,64,3 - all images are converted to 64*64 pixels. 3 is for colored(R.G,B) IMAGES. \n",
    "    # 1 is for B&W images.\n",
    "# 'relu' - all 'feature maps' go through this to remove -ve numbered pixels and separate out feature in the images \n",
    "    # (i.e. remove linearity as explained in Udemy video)\n",
    "\n",
    "# Step 2: Max Pooling - Adding Max Pooling Layer:\n",
    "classifier.add(MaxPooling2D(  pool_size = (2,2)  ))\n",
    "# parameter info: https://www.udemy.com/deeplearning/learn/v4/t/lecture/6743910?start=15\n",
    "# 2,2 - size of 2 will reduce the convoluted images in half without losing on features. Similarly, 3,3 will reduce to 1/3.\n",
    "\n",
    "# Step 3: Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4: Full Connection\n",
    "classifier.add(Dense( output_dim=128, activation='relu' )) # 1st hidden layer; 'output_dim'='units'\n",
    "classifier.add(Dense( output_dim=1, activation='sigmoid' )) # output layer\n",
    "# parameter definition same as ANN\n",
    "\n",
    "# Step 5: Compiling the ANN (means run SGD on ANN)\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# parameter definition same as ANN\n",
    "\n",
    "# i). Image Pre-Processing - Image Augmentation: done to avoid overfitting\n",
    "# Refer keras.io for pre-processing code\n",
    "# Image Augmentation: Augments the number of images in training data by flipping, \n",
    "    # rotating and more on images to increase training data and prevent overfitting\n",
    " \n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255, # rescales all pixel values between 0-1\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Parameter Info: https://www.udemy.com/deeplearning/learn/v4/t/lecture/6743978?start=990\n",
    "# rescale: feature scaled images of the current images\n",
    "# shear range: geometrical transformation also called convection-random images selected, \n",
    "    # pixels are moved in a fixed direction(say right), top pixels move 2x but middle pixels move x, bottom pixels don't move at all\n",
    "# example link of image shearing: \n",
    "    # https://www.google.co.in/search?rlz=1C1GGRV_enIN751IN751&biw=1536&bih=735&tbm=isch&sa=1&ei=2pcbW5C-Fcz4rQG6jqbgBw&q=image+shearing+deep+learning&oq=image+shearing+deep+learning&gs_l=img.3...52060.54569.0.54652.14.14.0.0.0.0.222.1403.2j8j1.11.0....0...1c.1.64.img..3.1.221...0i30k1j0i8i30k1j0i24k1.0.TPEqQTgklRA#imgrc=8cUAVuJx1BfG9M:\n",
    "# zoom range: transformation-range for random zooming\n",
    "# horizontal flip: transformation-randomly flips inputs horizontally \n",
    "    # (vertical also there, not used here)\n",
    "# there are more image transformations in Keras documentation(not used in Udemy video), \n",
    "    # you may try them as well (Link: https://keras.io/preprocessing/image/)\n",
    "\n",
    "\n",
    "# ii). Training and Validation Set Generation:\n",
    "# This section creates training set and validation set by taking all rescaled and transformed images from i) section, ImageDataGenerator\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'training_set', # path after WD to be mentioned; whole path also works\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32, # batch size after which NN weights would be updated\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_set = validation_datagen.flow_from_directory(\n",
    "        'validation_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "# Output Message:\n",
    "# Found 8000 images belonging to 2 classes.\n",
    "# Found 2000 images belonging to 2 classes.\n",
    "\n",
    "\n",
    "# iii). Fitting Model on Training and Evaluating on Validation:\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# np.random.seed(4)\n",
    "classifier.fit_generator(\n",
    "        training_set,\n",
    "        samples_per_epoch=8000, # since we have 8000 images in training_set\n",
    "        epochs=5, # default was 25\n",
    "        validation_data=validation_set,\n",
    "        nb_val_samples=2000) # number of images in validation_set\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "elapsed_time\n",
    "\n",
    "# UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. \n",
    "# `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size.\n",
    "# Note: steps_per_epoch = 250. Since Batche_size = 32. So, 250 steps complete 1 epoch.\n",
    "\n",
    "# Run Time: 2.28 hours (Single Layer)\n",
    "# Model Selection: Select model(at epoch #) where Validation accuracy is high and Training accuracy is close to Validation\n",
    "# accuracy after 25 epochs: acc: 0.8564 - val_acc: 0.7780\n",
    "# accuracy after 16 epochs: acc: 0.8284 - val_acc: 0.8026 (set seed and reproduce this model with epoch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antrived/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
      "  \n",
      "/home/antrived/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "/home/antrived/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=3)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8003 images belonging to 3 classes.\n",
      "Found 2002 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antrived/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:83: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/antrived/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:83: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., epochs=5, validation_data=<keras_pre..., steps_per_epoch=250, validation_steps=2002)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "250/250 [==============================] - 180s 721ms/step - loss: 0.7072 - acc: 0.5777 - val_loss: 0.6186 - val_acc: 0.6894\n",
      "Epoch 2/5\n",
      "250/250 [==============================] - 185s 740ms/step - loss: 0.6111 - acc: 0.6744 - val_loss: 0.7012 - val_acc: 0.6140\n",
      "Epoch 3/5\n",
      "250/250 [==============================] - 179s 715ms/step - loss: 0.5709 - acc: 0.7075 - val_loss: 0.5772 - val_acc: 0.7108\n",
      "Epoch 4/5\n",
      "250/250 [==============================] - 187s 746ms/step - loss: 0.5568 - acc: 0.7147 - val_loss: 0.5963 - val_acc: 0.7042\n",
      "Epoch 5/5\n",
      "250/250 [==============================] - 181s 722ms/step - loss: 0.5373 - acc: 0.7244 - val_loss: 0.5989 - val_acc: 0.6808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "911.5712442398071"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Basic Model - Multiclass Classification (1 Convolutional layer and 1 Fully Connected layer)\n",
    "# Not tuning performed\n",
    "\n",
    "# Initializing CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1: Convolution - Adding Convolution Layer:\n",
    "classifier.add(Convolution2D(  32,3,3,  input_shape=(64,64,3),  activation='relu'  ))\n",
    "# parameter info: https://www.udemy.com/deeplearning/learn/v4/t/lecture/6743906?start=0\n",
    "# 32,3,3 - 32 different 'feature detectors' are used of size, 3 rows and 3 columns\n",
    "# 64,64,3 - all images are converted to 64*64 pixels. 3 is for colored(R.G,B) IMAGES. 1 is for B&W images.\n",
    "# 'relu' - all 'feature maps' go through this to remove -ve numbered pixels and separate out feature in the images \n",
    "#   (i.e. remove linearity as explained in Udemy video)\n",
    "\n",
    "# Step 2: Max Pooling - Adding Max Pooling Layer:\n",
    "classifier.add(MaxPooling2D(  pool_size = (2,2)  ))\n",
    "# parameter info: https://www.udemy.com/deeplearning/learn/v4/t/lecture/6743910?start=15\n",
    "# 2,2 - size of 2 will reduce the convoluted images in half without losing on features. Similarly, 3,3 will reduce to 1/3.\n",
    "\n",
    "# Step 3: Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4: Full Connection\n",
    "classifier.add(Dense( output_dim=128, activation='relu' )) # 1st hidden layer; 'output_dim'='units'\n",
    "classifier.add(Dense( output_dim=3, activation='softmax' )) # output layer\n",
    "# parameter definition same as ANN\n",
    "\n",
    "# Step 5: Compiling the ANN (means run SGD on ANN)\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# parameter definition same as ANN\n",
    "\n",
    "# i). Image Pre-Processing - Image Augmentation: done to avoid overfitting\n",
    "# Refer keras.io for pre-processing code\n",
    "# Image Augmentation: Augments the number of images in training data by flipping, rotating and more on images to increase training data and prevent overfitting\n",
    " \n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255, # rescales all pixel values between 0-1\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Parameter Info: https://www.udemy.com/deeplearning/learn/v4/t/lecture/6743978?start=990\n",
    "# rescale: feature scaled images of the current images\n",
    "# shear range: geometrical transformation also called convection-random images selected, pixels are moved in a fixed direction(say right), top pixels move 2x but middle pixels move x, bottom pixels don't move at all\n",
    " # example link of image shearing: https://www.google.co.in/search?rlz=1C1GGRV_enIN751IN751&biw=1536&bih=735&tbm=isch&sa=1&ei=2pcbW5C-Fcz4rQG6jqbgBw&q=image+shearing+deep+learning&oq=image+shearing+deep+learning&gs_l=img.3...52060.54569.0.54652.14.14.0.0.0.0.222.1403.2j8j1.11.0....0...1c.1.64.img..3.1.221...0i30k1j0i8i30k1j0i24k1.0.TPEqQTgklRA#imgrc=8cUAVuJx1BfG9M:\n",
    "# zoom range: transformation-range for random zooming\n",
    "# horizontal flip: transformation-randomly flips inputs horizontally (vertical also there, not used here)\n",
    "# there are more image transformations in Keras documentation(not used in Udemy video), you may try them as well (Link: https://keras.io/preprocessing/image/)\n",
    "\n",
    "\n",
    "# ii). Training and Validation Set Generation:\n",
    "# This section creates training set and validation set by taking all rescaled and transformed images from i) section, ImageDataGenerator\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'training_set', # path after WD to be mentioned; refer upper sections for current WD\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32, # batch size after which NN weights would be updated\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_set = validation_datagen.flow_from_directory(\n",
    "        'validation_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# Output Message:\n",
    "# Found 8000 images belonging to 2 classes.\n",
    "# Found 2000 images belonging to 2 classes.\n",
    "\n",
    "\n",
    "# iii). Fitting Model on Training and Evaluating on Validation:\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# np.random.seed(4)\n",
    "classifier.fit_generator(\n",
    "        training_set,\n",
    "        samples_per_epoch=8003, # since we have 8000 images in training_set\n",
    "        epochs=5, # default was 25\n",
    "        validation_data=validation_set,\n",
    "        nb_val_samples=2002) # number of images in validation_set\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "elapsed_time\n",
    "\n",
    "# UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. \n",
    "# `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size.\n",
    "# Note: steps_per_epoch = 250. Since Batche_size = 32. So, 250 steps complete 1 epoch.\n",
    "\n",
    "# Run Time: 2.28 hours (Single Layer)\n",
    "# Model Selection: Select model(at epoch #) where Validation accuracy is high and Training accuracy is close to Validation\n",
    "# accuracy after 25 epochs: acc: 0.8564 - val_acc: 0.7780\n",
    "# accuracy after 16 epochs: acc: 0.8284 - val_acc: 0.8026 (set seed and reproduce this model with epoch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(128, 128,..., activation=\"relu\")`\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:71: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:71: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., epochs=75, validation_data=<keras.pre..., steps_per_epoch=250, validation_steps=2000)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      " 71/250 [=======>......................] - ETA: 5:19 - loss: 0.6969 - acc: 0.5018"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-d1f6ace4732d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         nb_val_samples=2000) # number of images in validation_set\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1225\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1227\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1229\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2145\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2146\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2147\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2149\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1837\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1839\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1840\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2357\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Advanced Model(binary) (n Convolutional layer, n Fully Connected layer, and more)\n",
    "## Manual Tuning performed (Automated tuning to be done after this phase)\n",
    "# ( Tip - Try different combinations at small data; then expand for bigger data )\n",
    "\n",
    "# Don't Run\n",
    "\n",
    "# CNN Architecture\n",
    "#########################################################################################\n",
    "\n",
    "classifier = Sequential()\n",
    "\n",
    "# 1st Convolutional layer added\n",
    "classifier.add(Convolution2D(  32,3,3,  input_shape=(64,64,3),  activation='relu'  ))\n",
    "classifier.add(MaxPooling2D(  pool_size = (2,2)  ))\n",
    "\n",
    "# 2nd Convolutional layer added\n",
    "classifier.add(Convolution2D(  64,3,3,  activation='relu'  ))\n",
    "classifier.add(MaxPooling2D(  pool_size = (2,2)  ))\n",
    "\n",
    "# 3rd Convolutional layer added\n",
    "classifier.add(Convolution2D(  128,3,3,  activation='relu'  ))\n",
    "classifier.add(MaxPooling2D(  pool_size = (2,2)  ))\n",
    "\n",
    "# 4th Convolutional layer added\n",
    "classifier.add(Convolution2D(  256,3,3,  activation='relu'  ))\n",
    "classifier.add(MaxPooling2D(  pool_size = (2,2)  ))\n",
    "\n",
    "\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dense( output_dim=128, activation='relu' )) # 1st hidden layer; 'output_dim'='units'\n",
    "classifier.add(Dense( output_dim=1, activation='sigmoid' )) # output layer\n",
    "\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Image Pre-Processing and Fitting\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255, # rescales all pixel values between 0-1\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'training_set', # path after WD to be mentioned; refer upper sections for current WD\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32, # batch size after which NN weights would be updated\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_set = validation_datagen.flow_from_directory(\n",
    "        'validation_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "np.random.seed(4)\n",
    "classifier.fit_generator(\n",
    "        training_set,\n",
    "        samples_per_epoch=8000, # since we have 8000 images in training_set\n",
    "        epochs=50,\n",
    "        validation_data=validation_set,\n",
    "        nb_val_samples=2000) # number of images in validation_set\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "elapsed_time\n",
    "\n",
    "# Don't Run\n",
    "#########################################################################################\n",
    "\n",
    "## Iter1: (over default- 0.8026 - 2nd Convolution layer added): \n",
    "# 2 Convolutional layers(32,32)*,pool_size = (2,2),input shape(64,64) & target_size(64,64),batch_size(32),epochs(25), \n",
    "# ImageDataGenerator(\n",
    "#         rescale=1./255, # rescales all pixel values between 0-1\n",
    "#         shear_range=0.2,\n",
    "#         zoom_range=0.2,\n",
    "#         horizontal_flip=True)\n",
    "# classifier.add(Dense( output_dim=128, activation='relu' )) # 1st hidden layer; 'output_dim'='units'\n",
    "# classifier.add(Dense( output_dim=1, activation='sigmoid' )) # output layer\n",
    "# classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "## Time: 2.32 hours (hardly any increase in time by adding additional Convolutional layer)\n",
    "# accuracy after all 25 epochs: acc: 0.8862 - val_acc: 0.8001\n",
    "# best accuracy after all 20 epochs: acc: 0.8615 - val_acc: 0.8155 (diff also lower)\n",
    "# Note: accuracy increased substantially without increasing time, by adding a convolutional layer (5.75min/epoch)\n",
    "\n",
    "## Iter2(****): (over Iter1-0.8155)\n",
    "# (over Iter 2: 4 Convolutional layers(32,64,128,256)) - 0.8569(best at 25 epochs) - accuracy improved(7.2min/epoch)\n",
    "# Note: Adding 5th C layer gives error: \"Negative dimension size caused by subtracting 3 from 2\"\n",
    "#       But increasing input shape & target size(128), lets you add 5th layer(512) \n",
    "\n",
    "## Iter3(****): (over Iter2-0.8569)\n",
    "# (over Iter 2:epochs=50;target_size(128,128)) - 0.9051(best at 30 epochs) - accuracy improved(21min/epoch)\n",
    "\n",
    "\n",
    "## Iter4: (over Iter2-0.8569) - discontinued as taking excessive time (60+min/epoch)\n",
    "# (over Iter 2: 4 Convolutional layers(256,256,256,256)) - ----(best at -- epochs)\n",
    "\n",
    "## Iter5(*): (over Iter2-0.8569)\n",
    "# (over Iter 2: 2nd dense layer(128)) - 0.8718(best at -- epochs) - accuracy inc(7min/epoch)\n",
    "\n",
    "## Iter6(**): (over Iter2-0.8569)\n",
    "# (over Iter 2: epochs=50) - best at 0.8751(at 40 epochs) - accuracy inc(~7.5min/epoch)\n",
    "\n",
    "## Iter7: (over Iter6-0.8751)\n",
    "# (over Iter 6: Dropout=0.5) - 0.8806(best at 33 epochs) - accuracy improved(~7.5min/epoch)\n",
    "## Iter9*: (over Iter6-0.8751)\n",
    "# (over Iter 6: Dropout=0.6) - 0.8934(best at 48 epochs) - accuracy improved(~7.5min/epoch)\n",
    "## Iter10**: (over Iter6-0.8751)\n",
    "# (over Iter 6:Pre(0.6),dense(128),Post(0.3)dropout) - 0.8989(best at 49 epochs) - accuracy improved(~7.5min/epoch)\n",
    "## Iter12*: (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: layers of dropout and dense (0.6,128,0.5,128,0.4,128,0.3))-0.8857(Best at 43 epochs)\n",
    "# accuracy inc(7min/epoch)\n",
    "# Code:\n",
    "# classifier.add(Dropout(rate=0.6))\n",
    "# classifier.add(Dense( output_dim=128, activation='relu' )) # 1st hidden layer; 'output_dim'='units'\n",
    "# classifier.add(Dropout(rate=0.5))\n",
    "# classifier.add(Dense( output_dim=128, activation='relu' ))\n",
    "# classifier.add(Dropout(rate=0.4))\n",
    "# classifier.add(Dense( output_dim=128, activation='relu' ))\n",
    "# classifier.add(Dropout(rate=0.3))\n",
    "\n",
    "## Iter8(*): (over Iter6-0.8751)\n",
    "# (over Iter 6: output_dim=256) - 0.8911(best at 46 epochs) - accuracy improved(~7.5min/epoch)\n",
    "\n",
    "## Iter11(*****): (over Iter6-0.8751)\n",
    "# (over Iter 6: input shape and target size(150)) - 0.9220(best at 48 epochs) - accuracy improved drastically(45min/epoch)\n",
    "\n",
    "# ## Iter13: (over Iter6-0.8751)\n",
    "# # CNN Architecture (over Iter 6: Batch Normalization after Dense)-0.8854(best at 48 epochs-overfitted & unstable across epochs)\n",
    "# # accuracy inc(7min/epoch)\n",
    "# Code:\n",
    "# classifier.add(Dense( output_dim=128) ) # 1st hidden layer; 'output_dim'='units'\n",
    "# classifier.add(BatchNormalization())\n",
    "# classifier.add(Activation('relu'))\n",
    "\n",
    "## Iter14: (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: Batch Normalization after C layers)-0.8759(best at 39 epochs-overfitted & unstable across epochs)\n",
    "# accuracy similar(18min/epoch)\n",
    "# Code:\n",
    "# classifier.add(Convolution2D(  32,3,3,  input_shape=(64,64,3),  activation='relu'  ))\n",
    "# classifier.add(BatchNormalization())\n",
    "# classifier.add(Activation('relu'))\n",
    "# classifier.add(MaxPooling2D(  pool_size = (2,2)  ))\n",
    "\n",
    "## Iter15: (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: Dropout after C layers)-0.7406(best at 23 epochs)-accuracy reduced significantly(7.5min/epoch)\n",
    "# Code:\n",
    "# classifier.add(Convolution2D(  32,3,3,  input_shape=(64,64,3),  activation='relu'  ))\n",
    "# classifier.add(MaxPooling2D(  pool_size = (2,2)  ))\n",
    "# classifier.add(Dropout(0.6))\n",
    "\n",
    "## Iter16: (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: Adam(lr=0.0005)-0.8700(Best at 49 epochs), accuracy reduced(7min/epoch)\n",
    "## Iter17: (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: Adam(lr=0.0001)-0.8444(Best at 48 epochs), accuracy reduced(7min/epoch)\n",
    "## Iter18: (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: Adam(lr=0.00005)-0.8357(Best at 45 epochs), accuracy reduced(7min/epoch)\n",
    "## Iter19: (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: Adam(lr=0.005)-0.5002(Best at -- epochs), accuracy reduced(7min/epoch)\n",
    "## Iter20: (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: Adam(lr=0.003)-0.5002(Best at -- epochs), accuracy reduced(7min/epoch)\n",
    "## Iter21: (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: Adam(lr=0.002)-0.8534(Best at -- epochs), accuracy reduced(7min/epoch)\n",
    "## Iter22: (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: Adam(lr=0.0015)-0.8670(Best at -- epochs), accuracy reduced(7min/epoch)\n",
    "# Code:\n",
    "# opt = keras.optimizers.Adam(lr=0.0005)\n",
    "# classifier.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "## Iter23(0.5*): (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: C layers(64,128,256,512)) 0.8774(Best at -- epochs), # accuracy inc(14.7min/epoch)\n",
    "\n",
    "## Iter24(0.5*): (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: Shear=0.3) 0.8809(Best at -- epochs), # accuracy increased(7min/epoch)\n",
    "## Iter25(third*): (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: Shear=0.4) 0.8780(Best at -- epochs), # accuracy inc(7min/epoch)\n",
    "\n",
    "## Iter26(*): (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: Zoom=0.3) 0.8852(Best at -- epochs), # accuracy inc(7min/epoch)\n",
    "## Iter27*: (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: Zoom=0.4) 0.8845(Best at -- epochs), # accuracy inc(7min/epoch)\n",
    "\n",
    "## Iter28: (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: optimizer='SGD') 0.6513(Best at --epochs), # accuracy inc(7min/epoch)\n",
    "## Iter29: (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: optimizer='RMSprop') 0.5001(Best at -- epochs), # accuracy red(7min/epoch)\n",
    "## Iter30(third*): (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: optimizer='Adamax') 0.8789(Best at -- epochs), # accuracy inc(7min/epoch)\n",
    "\n",
    "## Iter31: (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: vertical_flip=True) 0.8625(Best at -- epochs), # accuracy inc(7.5min/epoch)\n",
    "## Iter32: (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: rotation_range=0.2) 0.8719(Best at -- epochs), # accuracy inc(7min/epoch)\n",
    "## Iter33(1.5*): (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: width_shift_range=0.2) 0.8895(Best at -- epochs), # accuracy inc(7min/epoch)\n",
    "## Iter34(**): (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: height_shift_range=0.2) 0.8954(Best at -- epochs), # accuracy inc(7min/epoch)\n",
    "## Iter35(*): (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: channel_shift_range=0.2) 0.8860(Best at -- epochs), # accuracy inc(8min/epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:50: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:54: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:61: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:62: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:104: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:104: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., epochs=50, validation_data=<keras.pre..., steps_per_epoch=250, validation_steps=2000)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "250/250 [==============================] - 424s 2s/step - loss: 0.6892 - acc: 0.5252 - val_loss: 0.6577 - val_acc: 0.6266\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 423s 2s/step - loss: 0.6489 - acc: 0.6288 - val_loss: 0.6223 - val_acc: 0.6840\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 419s 2s/step - loss: 0.6134 - acc: 0.6616 - val_loss: 0.5995 - val_acc: 0.6735\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 432s 2s/step - loss: 0.5803 - acc: 0.6944 - val_loss: 0.5539 - val_acc: 0.7186\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 416s 2s/step - loss: 0.5527 - acc: 0.7200 - val_loss: 0.5589 - val_acc: 0.7138\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 421s 2s/step - loss: 0.5269 - acc: 0.7342 - val_loss: 0.5106 - val_acc: 0.7581\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 434s 2s/step - loss: 0.5016 - acc: 0.7560 - val_loss: 0.4617 - val_acc: 0.8013\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 428s 2s/step - loss: 0.4840 - acc: 0.7706 - val_loss: 0.4850 - val_acc: 0.7800\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 421s 2s/step - loss: 0.4717 - acc: 0.7741 - val_loss: 0.4439 - val_acc: 0.8082\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 422s 2s/step - loss: 0.4505 - acc: 0.7864 - val_loss: 0.4640 - val_acc: 0.7820\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 424s 2s/step - loss: 0.4445 - acc: 0.7981 - val_loss: 0.4248 - val_acc: 0.8191\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 422s 2s/step - loss: 0.4322 - acc: 0.8021 - val_loss: 0.4070 - val_acc: 0.8210\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 421s 2s/step - loss: 0.4157 - acc: 0.8063 - val_loss: 0.4175 - val_acc: 0.8102\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 422s 2s/step - loss: 0.4107 - acc: 0.8104 - val_loss: 0.3987 - val_acc: 0.8240\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 425s 2s/step - loss: 0.3979 - acc: 0.8199 - val_loss: 0.3787 - val_acc: 0.8300\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 422s 2s/step - loss: 0.3816 - acc: 0.8272 - val_loss: 0.3665 - val_acc: 0.8489\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 422s 2s/step - loss: 0.3611 - acc: 0.8392 - val_loss: 0.3589 - val_acc: 0.8454\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 423s 2s/step - loss: 0.3707 - acc: 0.8319 - val_loss: 0.3432 - val_acc: 0.8548\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 424s 2s/step - loss: 0.3446 - acc: 0.8495 - val_loss: 0.3368 - val_acc: 0.8606\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 427s 2s/step - loss: 0.3516 - acc: 0.8405 - val_loss: 0.3425 - val_acc: 0.8539\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 422s 2s/step - loss: 0.3281 - acc: 0.8576 - val_loss: 0.3372 - val_acc: 0.8562\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 439s 2s/step - loss: 0.3326 - acc: 0.8540 - val_loss: 0.3402 - val_acc: 0.8570\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 439s 2s/step - loss: 0.3253 - acc: 0.8555 - val_loss: 0.3997 - val_acc: 0.8315\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 425s 2s/step - loss: 0.3142 - acc: 0.8609 - val_loss: 0.3157 - val_acc: 0.8735\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 427s 2s/step - loss: 0.3184 - acc: 0.8599 - val_loss: 0.3424 - val_acc: 0.8545\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 424s 2s/step - loss: 0.3069 - acc: 0.8663 - val_loss: 0.3266 - val_acc: 0.8659\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 422s 2s/step - loss: 0.2870 - acc: 0.8766 - val_loss: 0.3389 - val_acc: 0.8569\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 427s 2s/step - loss: 0.2938 - acc: 0.8707 - val_loss: 0.3633 - val_acc: 0.8501\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 420s 2s/step - loss: 0.2916 - acc: 0.8749 - val_loss: 0.3214 - val_acc: 0.8701\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 421s 2s/step - loss: 0.2777 - acc: 0.8825 - val_loss: 0.3349 - val_acc: 0.8655\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 422s 2s/step - loss: 0.2786 - acc: 0.8775 - val_loss: 0.2946 - val_acc: 0.8819\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 424s 2s/step - loss: 0.2724 - acc: 0.8815 - val_loss: 0.2835 - val_acc: 0.8806\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 423s 2s/step - loss: 0.2663 - acc: 0.8809 - val_loss: 0.4442 - val_acc: 0.8307\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 424s 2s/step - loss: 0.2738 - acc: 0.8805 - val_loss: 0.2868 - val_acc: 0.8750\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 424s 2s/step - loss: 0.2635 - acc: 0.8846 - val_loss: 0.2977 - val_acc: 0.8671\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 421s 2s/step - loss: 0.2431 - acc: 0.8984 - val_loss: 0.2952 - val_acc: 0.8744\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 422s 2s/step - loss: 0.2506 - acc: 0.8911 - val_loss: 0.3113 - val_acc: 0.8649\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 421s 2s/step - loss: 0.2535 - acc: 0.8930 - val_loss: 0.3177 - val_acc: 0.8687\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 421s 2s/step - loss: 0.2401 - acc: 0.8982 - val_loss: 0.3668 - val_acc: 0.8526\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 422s 2s/step - loss: 0.2380 - acc: 0.8964 - val_loss: 0.3005 - val_acc: 0.8834\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 423s 2s/step - loss: 0.2395 - acc: 0.8973 - val_loss: 0.2870 - val_acc: 0.8839\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 434s 2s/step - loss: 0.2423 - acc: 0.8967 - val_loss: 0.2939 - val_acc: 0.8815\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 457s 2s/step - loss: 0.2363 - acc: 0.8970 - val_loss: 0.2957 - val_acc: 0.8819\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 442s 2s/step - loss: 0.2421 - acc: 0.8932 - val_loss: 0.3341 - val_acc: 0.8620\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 425s 2s/step - loss: 0.2160 - acc: 0.9098 - val_loss: 0.3890 - val_acc: 0.8561\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 423s 2s/step - loss: 0.2284 - acc: 0.9036 - val_loss: 0.3072 - val_acc: 0.8800\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 422s 2s/step - loss: 0.2118 - acc: 0.9135 - val_loss: 0.2936 - val_acc: 0.8895\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 423s 2s/step - loss: 0.2226 - acc: 0.9040 - val_loss: 0.5418 - val_acc: 0.7989\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 420s 2s/step - loss: 0.2278 - acc: 0.9009 - val_loss: 0.2765 - val_acc: 0.8860\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 427s 2s/step - loss: 0.2202 - acc: 0.9056 - val_loss: 0.2893 - val_acc: 0.8856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:127: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:131: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:135: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:139: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:146: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:147: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:189: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:189: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., epochs=50, validation_data=<keras.pre..., steps_per_epoch=250, validation_steps=2000)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "250/250 [==============================] - 428s 2s/step - loss: 0.6907 - acc: 0.5330 - val_loss: 0.6750 - val_acc: 0.6276\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 419s 2s/step - loss: 0.6447 - acc: 0.6217 - val_loss: 0.6074 - val_acc: 0.6658\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 423s 2s/step - loss: 0.6171 - acc: 0.6554 - val_loss: 0.5806 - val_acc: 0.7060\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 434s 2s/step - loss: 0.5957 - acc: 0.6796 - val_loss: 0.5775 - val_acc: 0.7220\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 425s 2s/step - loss: 0.5610 - acc: 0.7137 - val_loss: 0.5125 - val_acc: 0.7436\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 422s 2s/step - loss: 0.5278 - acc: 0.7385 - val_loss: 0.4607 - val_acc: 0.7905\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 423s 2s/step - loss: 0.5009 - acc: 0.7589 - val_loss: 0.4334 - val_acc: 0.8065\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 422s 2s/step - loss: 0.4976 - acc: 0.7526 - val_loss: 0.4258 - val_acc: 0.8147\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 424s 2s/step - loss: 0.4570 - acc: 0.7873 - val_loss: 0.3993 - val_acc: 0.8252\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 423s 2s/step - loss: 0.4402 - acc: 0.7939 - val_loss: 0.3807 - val_acc: 0.8327\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 421s 2s/step - loss: 0.4287 - acc: 0.7986 - val_loss: 0.3805 - val_acc: 0.8306\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 424s 2s/step - loss: 0.4221 - acc: 0.8057 - val_loss: 0.3593 - val_acc: 0.8429\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 427s 2s/step - loss: 0.3887 - acc: 0.8196 - val_loss: 0.3730 - val_acc: 0.8336\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 420s 2s/step - loss: 0.3867 - acc: 0.8219 - val_loss: 0.3371 - val_acc: 0.8596\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 423s 2s/step - loss: 0.3788 - acc: 0.8288 - val_loss: 0.3406 - val_acc: 0.8509\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 420s 2s/step - loss: 0.3663 - acc: 0.8375 - val_loss: 0.3319 - val_acc: 0.8629\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 425s 2s/step - loss: 0.3601 - acc: 0.8400 - val_loss: 0.3177 - val_acc: 0.8620\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 423s 2s/step - loss: 0.3468 - acc: 0.8438 - val_loss: 0.3609 - val_acc: 0.8404\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 423s 2s/step - loss: 0.3448 - acc: 0.8456 - val_loss: 0.3370 - val_acc: 0.8546\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 424s 2s/step - loss: 0.3357 - acc: 0.8466 - val_loss: 0.3288 - val_acc: 0.8609\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 426s 2s/step - loss: 0.3308 - acc: 0.8501 - val_loss: 0.3100 - val_acc: 0.8667\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 423s 2s/step - loss: 0.3238 - acc: 0.8564 - val_loss: 0.3675 - val_acc: 0.8394\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 423s 2s/step - loss: 0.3191 - acc: 0.8535 - val_loss: 0.3045 - val_acc: 0.8670\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 420s 2s/step - loss: 0.3136 - acc: 0.8644 - val_loss: 0.3024 - val_acc: 0.8689\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 423s 2s/step - loss: 0.3039 - acc: 0.8635 - val_loss: 0.3090 - val_acc: 0.8700\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 433s 2s/step - loss: 0.2922 - acc: 0.8731 - val_loss: 0.2998 - val_acc: 0.8712\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 425s 2s/step - loss: 0.2881 - acc: 0.8759 - val_loss: 0.3023 - val_acc: 0.8674\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 421s 2s/step - loss: 0.2861 - acc: 0.8730 - val_loss: 0.3046 - val_acc: 0.8691\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 423s 2s/step - loss: 0.2753 - acc: 0.8841 - val_loss: 0.2830 - val_acc: 0.8886\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 426s 2s/step - loss: 0.2746 - acc: 0.8829 - val_loss: 0.3013 - val_acc: 0.8821\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 432s 2s/step - loss: 0.2691 - acc: 0.8801 - val_loss: 0.3009 - val_acc: 0.8768\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 421s 2s/step - loss: 0.2699 - acc: 0.8836 - val_loss: 0.3266 - val_acc: 0.8736\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 432s 2s/step - loss: 0.2590 - acc: 0.8896 - val_loss: 0.2995 - val_acc: 0.8775\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 465s 2s/step - loss: 0.2602 - acc: 0.8862 - val_loss: 0.2850 - val_acc: 0.8756\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 468s 2s/step - loss: 0.2481 - acc: 0.8946 - val_loss: 0.2827 - val_acc: 0.8788\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 429s 2s/step - loss: 0.2382 - acc: 0.8991 - val_loss: 0.2915 - val_acc: 0.8845\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 428s 2s/step - loss: 0.2479 - acc: 0.8966 - val_loss: 0.3223 - val_acc: 0.8815\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 430s 2s/step - loss: 0.2390 - acc: 0.9033 - val_loss: 0.2867 - val_acc: 0.8847\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 430s 2s/step - loss: 0.2398 - acc: 0.8990 - val_loss: 0.3041 - val_acc: 0.8769\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 435s 2s/step - loss: 0.2284 - acc: 0.9004 - val_loss: 0.2844 - val_acc: 0.8836\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 427s 2s/step - loss: 0.2272 - acc: 0.9025 - val_loss: 0.2782 - val_acc: 0.8944\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 428s 2s/step - loss: 0.2258 - acc: 0.9041 - val_loss: 0.3682 - val_acc: 0.8555\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 437s 2s/step - loss: 0.2153 - acc: 0.9109 - val_loss: 0.2841 - val_acc: 0.8878\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 424s 2s/step - loss: 0.2180 - acc: 0.9066 - val_loss: 0.3014 - val_acc: 0.8846\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 427s 2s/step - loss: 0.2256 - acc: 0.9048 - val_loss: 0.2755 - val_acc: 0.8850\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 444s 2s/step - loss: 0.2163 - acc: 0.9041 - val_loss: 0.2905 - val_acc: 0.8846\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 488s 2s/step - loss: 0.2087 - acc: 0.9095 - val_loss: 0.2880 - val_acc: 0.8954\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 486s 2s/step - loss: 0.2107 - acc: 0.9114 - val_loss: 0.2904 - val_acc: 0.8846\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 512s 2s/step - loss: 0.2156 - acc: 0.9074 - val_loss: 0.2884 - val_acc: 0.8869\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 507s 2s/step - loss: 0.2062 - acc: 0.9137 - val_loss: 0.2833 - val_acc: 0.8849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:212: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:216: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:220: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:224: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:231: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:232: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:274: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "C:\\Users\\anurag.trivedi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:274: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., epochs=50, validation_data=<keras.pre..., steps_per_epoch=250, validation_steps=2000)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "250/250 [==============================] - 492s 2s/step - loss: 0.6776 - acc: 0.5681 - val_loss: 0.6151 - val_acc: 0.6702\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 497s 2s/step - loss: 0.6020 - acc: 0.6747 - val_loss: 0.5612 - val_acc: 0.7279\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 505s 2s/step - loss: 0.5652 - acc: 0.7036 - val_loss: 0.5370 - val_acc: 0.7572\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 493s 2s/step - loss: 0.5232 - acc: 0.7365 - val_loss: 0.4933 - val_acc: 0.7644\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 488s 2s/step - loss: 0.4891 - acc: 0.7655 - val_loss: 0.4714 - val_acc: 0.7785\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 485s 2s/step - loss: 0.4664 - acc: 0.7806 - val_loss: 0.4273 - val_acc: 0.8068\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 493s 2s/step - loss: 0.4386 - acc: 0.7943 - val_loss: 0.3985 - val_acc: 0.8230\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 496s 2s/step - loss: 0.4109 - acc: 0.8117 - val_loss: 0.4285 - val_acc: 0.8058\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 488s 2s/step - loss: 0.3940 - acc: 0.8269 - val_loss: 0.3797 - val_acc: 0.8285\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 499s 2s/step - loss: 0.3742 - acc: 0.8332 - val_loss: 0.3572 - val_acc: 0.8385\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 482s 2s/step - loss: 0.3581 - acc: 0.8394 - val_loss: 0.3601 - val_acc: 0.8389\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 462s 2s/step - loss: 0.3468 - acc: 0.8439 - val_loss: 0.3457 - val_acc: 0.8466\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 454s 2s/step - loss: 0.3291 - acc: 0.8543 - val_loss: 0.3825 - val_acc: 0.8284\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 440s 2s/step - loss: 0.3230 - acc: 0.8574 - val_loss: 0.3464 - val_acc: 0.8563\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 427s 2s/step - loss: 0.3191 - acc: 0.8640 - val_loss: 0.3645 - val_acc: 0.8356\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 448s 2s/step - loss: 0.2995 - acc: 0.8673 - val_loss: 0.3845 - val_acc: 0.8402\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 433s 2s/step - loss: 0.2932 - acc: 0.8726 - val_loss: 0.3269 - val_acc: 0.8710\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 431s 2s/step - loss: 0.2730 - acc: 0.8809 - val_loss: 0.3449 - val_acc: 0.8538\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 430s 2s/step - loss: 0.2714 - acc: 0.8809 - val_loss: 0.3290 - val_acc: 0.8594\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 434s 2s/step - loss: 0.2655 - acc: 0.8861 - val_loss: 0.3297 - val_acc: 0.8659\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 430s 2s/step - loss: 0.2543 - acc: 0.8886 - val_loss: 0.3230 - val_acc: 0.8625\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 437s 2s/step - loss: 0.2452 - acc: 0.8940 - val_loss: 0.3077 - val_acc: 0.8709\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 445s 2s/step - loss: 0.2437 - acc: 0.8935 - val_loss: 0.2940 - val_acc: 0.8800\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 462s 2s/step - loss: 0.2296 - acc: 0.9045 - val_loss: 0.3699 - val_acc: 0.8579\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 428s 2s/step - loss: 0.2318 - acc: 0.9011 - val_loss: 0.3209 - val_acc: 0.8746\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 428s 2s/step - loss: 0.2290 - acc: 0.9029 - val_loss: 0.3559 - val_acc: 0.8524\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 443s 2s/step - loss: 0.2227 - acc: 0.9085 - val_loss: 0.3006 - val_acc: 0.8850\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 491s 2s/step - loss: 0.2057 - acc: 0.9152 - val_loss: 0.2924 - val_acc: 0.8834\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 501s 2s/step - loss: 0.2042 - acc: 0.9161 - val_loss: 0.3072 - val_acc: 0.8739\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 514s 2s/step - loss: 0.1932 - acc: 0.9195 - val_loss: 0.3219 - val_acc: 0.8735\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 478s 2s/step - loss: 0.2051 - acc: 0.9154 - val_loss: 0.3077 - val_acc: 0.8830\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 510s 2s/step - loss: 0.1891 - acc: 0.9203 - val_loss: 0.3210 - val_acc: 0.8689\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 494s 2s/step - loss: 0.1938 - acc: 0.9183 - val_loss: 0.3419 - val_acc: 0.8708\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 483s 2s/step - loss: 0.1699 - acc: 0.9310 - val_loss: 0.3441 - val_acc: 0.8814\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 438s 2s/step - loss: 0.1722 - acc: 0.9280 - val_loss: 0.3245 - val_acc: 0.8840\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 434s 2s/step - loss: 0.1751 - acc: 0.9256 - val_loss: 0.3129 - val_acc: 0.8851\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 431s 2s/step - loss: 0.1722 - acc: 0.9299 - val_loss: 0.3754 - val_acc: 0.8746\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 465s 2s/step - loss: 0.1496 - acc: 0.9373 - val_loss: 0.3171 - val_acc: 0.8840\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 453s 2s/step - loss: 0.1661 - acc: 0.9317 - val_loss: 0.3109 - val_acc: 0.8846\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 455s 2s/step - loss: 0.1524 - acc: 0.9374 - val_loss: 0.4445 - val_acc: 0.8698\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 449s 2s/step - loss: 0.1595 - acc: 0.9331 - val_loss: 0.3580 - val_acc: 0.8810\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 448s 2s/step - loss: 0.1495 - acc: 0.9379 - val_loss: 0.3457 - val_acc: 0.8765\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 448s 2s/step - loss: 0.1434 - acc: 0.9447 - val_loss: 0.3381 - val_acc: 0.8860\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 450s 2s/step - loss: 0.1499 - acc: 0.9409 - val_loss: 0.4009 - val_acc: 0.8606\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 446s 2s/step - loss: 0.1367 - acc: 0.9460 - val_loss: 0.3822 - val_acc: 0.8756\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 446s 2s/step - loss: 0.1402 - acc: 0.9435 - val_loss: 0.3513 - val_acc: 0.8822\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 449s 2s/step - loss: 0.1237 - acc: 0.9510 - val_loss: 0.3976 - val_acc: 0.8805\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 448s 2s/step - loss: 0.1388 - acc: 0.9441 - val_loss: 0.3374 - val_acc: 0.8730\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 449s 2s/step - loss: 0.1239 - acc: 0.9510 - val_loss: 0.3569 - val_acc: 0.8851\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 451s 2s/step - loss: 0.1250 - acc: 0.9510 - val_loss: 0.3647 - val_acc: 0.8796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23082.39693427086"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Iterations - Advanced Model(binary) (n Convolutional layer, n Fully Connected layer, and more)\n",
    "## Manual Tuning performed (Automated tuning to be done after this phase)\n",
    "# ( Tip - Try different combinations at small data; then expand for bigger data )\n",
    "\n",
    "\n",
    "# more iterations:\n",
    "# learning rate of optimizer(0.01, 0.001, 0.005)\n",
    "# try 32,32,64,64 C layers\n",
    "# inc input shape(256) and target size(256)\n",
    "# only 1 C layer (256)\n",
    "# layers of dropout and dense (0.6,128,0.5,128,0.4,128,0.3)\n",
    "# epochs(100/150)\n",
    "# 2 consecutive C layers before pooling\n",
    "# create deeper network: 5 layers; 50 layers\n",
    "# changing image generator parameters\n",
    "# add batch normalization: https://yashk2810.github.io/Applying-Convolutional-Neural-Network-on-the-MNIST-dataset/\n",
    "    # https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c\n",
    "    # https://www.udemy.com/deeplearning/learn/v4/questions/2276518\n",
    "    # https://stackoverflow.com/questions/34716454/where-do-i-call-the-batchnormalization-function-in-keras\n",
    "# batch norm and dropout in C layer: https://www.udemy.com/deeplearning/learn/v4/questions/2276518\n",
    "# try different optimizers and their paramaeters\n",
    "\n",
    "# more ideas: https://www.udemy.com/deeplearning/learn/v4/questions/2276518\n",
    "\n",
    "# really imp ones: epochs, output_dim, target size(most probs input shape too)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Iter24(1.5*): (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: width_shift_range=0.2) 0.8895(Best at -- epochs), # accuracy inc(7min/epoch)\n",
    "## Iter24(**): (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: height_shift_range=0.2) 0.8954(Best at -- epochs), # accuracy inc(7min/epoch)\n",
    "## Iter24(*): (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: channel_shift_range=0.2) 0.8860(Best at -- epochs), # accuracy inc(8min/epoch)\n",
    "\n",
    "# #########################################################################################\n",
    "\n",
    "classifier = Sequential()\n",
    "\n",
    "\n",
    "# 1st Convolutional layer added\n",
    "classifier.add(Convolution2D(  32,3,3,  input_shape=(64,64,3),  activation='relu'  ))\n",
    "classifier.add(MaxPooling2D(  pool_size = (2,2)  ))\n",
    "\n",
    "# 2nd Convolutional layer added\n",
    "classifier.add(Convolution2D(  64,3,3,  activation='relu'  ))\n",
    "classifier.add(MaxPooling2D(  pool_size = (2,2)  ))\n",
    "\n",
    "# 3rd Convolutional layer added\n",
    "classifier.add(Convolution2D(  128,3,3,  activation='relu'  ))\n",
    "classifier.add(MaxPooling2D(  pool_size = (2,2)  ))\n",
    "\n",
    "# 4th Convolutional layer added\n",
    "classifier.add(Convolution2D(  256,3,3,  activation='relu'  ))\n",
    "classifier.add(MaxPooling2D(  pool_size = (2,2)  ))\n",
    "\n",
    "\n",
    "\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dense( output_dim=128, activation='relu' )) # 1st hidden layer; 'output_dim'='units'\n",
    "classifier.add(Dense( output_dim=1, activation='sigmoid' )) # output layer\n",
    "\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Image Pre-Processing and Fitting\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255, # rescales all pixel values between 0-1\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        width_shift_range=0.2)\n",
    "# rotation_range=0.0, width_shift_range=0.0, height_shift_range=0.0, channel_shift_range=0.0\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'training_set', # path after WD to be mentioned; refer upper sections for current WD\n",
    "        target_size=(64,64),\n",
    "        batch_size=32, # batch size after which NN weights would be updated\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_set = validation_datagen.flow_from_directory(\n",
    "        'validation_set',\n",
    "        target_size=(64,64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "np.random.seed(4)\n",
    "classifier.fit_generator(\n",
    "        training_set,\n",
    "        samples_per_epoch=8000, # since we have 8000 images in training_set\n",
    "        epochs=50,\n",
    "        validation_data=validation_set,\n",
    "        nb_val_samples=2000) # number of images in validation_set\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "elapsed_time\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Iter24(**): (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: height_shift_range=0.2) 0.8954(Best at -- epochs), # accuracy inc(7min/epoch)\n",
    "## Iter24(*): (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: channel_shift_range=0.2) 0.8860(Best at -- epochs), # accuracy inc(8min/epoch)\n",
    "\n",
    "# #########################################################################################\n",
    "\n",
    "classifier = Sequential()\n",
    "\n",
    "\n",
    "# 1st Convolutional layer added\n",
    "classifier.add(Convolution2D(  32,3,3,  input_shape=(64,64,3),  activation='relu'  ))\n",
    "classifier.add(MaxPooling2D(  pool_size = (2,2)  ))\n",
    "\n",
    "# 2nd Convolutional layer added\n",
    "classifier.add(Convolution2D(  64,3,3,  activation='relu'  ))\n",
    "classifier.add(MaxPooling2D(  pool_size = (2,2)  ))\n",
    "\n",
    "# 3rd Convolutional layer added\n",
    "classifier.add(Convolution2D(  128,3,3,  activation='relu'  ))\n",
    "classifier.add(MaxPooling2D(  pool_size = (2,2)  ))\n",
    "\n",
    "# 4th Convolutional layer added\n",
    "classifier.add(Convolution2D(  256,3,3,  activation='relu'  ))\n",
    "classifier.add(MaxPooling2D(  pool_size = (2,2)  ))\n",
    "\n",
    "\n",
    "\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dense( output_dim=128, activation='relu' )) # 1st hidden layer; 'output_dim'='units'\n",
    "classifier.add(Dense( output_dim=1, activation='sigmoid' )) # output layer\n",
    "\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Image Pre-Processing and Fitting\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255, # rescales all pixel values between 0-1\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        height_shift_range=0.2)\n",
    "# rotation_range=0.0, width_shift_range=0.0, height_shift_range=0.0, channel_shift_range=0.0\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'training_set', # path after WD to be mentioned; refer upper sections for current WD\n",
    "        target_size=(64,64),\n",
    "        batch_size=32, # batch size after which NN weights would be updated\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_set = validation_datagen.flow_from_directory(\n",
    "        'validation_set',\n",
    "        target_size=(64,64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "np.random.seed(4)\n",
    "classifier.fit_generator(\n",
    "        training_set,\n",
    "        samples_per_epoch=8000, # since we have 8000 images in training_set\n",
    "        epochs=50,\n",
    "        validation_data=validation_set,\n",
    "        nb_val_samples=2000) # number of images in validation_set\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "elapsed_time\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Iter24(*): (over Iter6-0.8751)\n",
    "# CNN Architecture (over Iter 6: channel_shift_range=0.2) 0.8860(Best at -- epochs), # accuracy inc(8min/epoch)\n",
    "\n",
    "# #########################################################################################\n",
    "\n",
    "classifier = Sequential()\n",
    "\n",
    "\n",
    "# 1st Convolutional layer added\n",
    "classifier.add(Convolution2D(  32,3,3,  input_shape=(64,64,3),  activation='relu'  ))\n",
    "classifier.add(MaxPooling2D(  pool_size = (2,2)  ))\n",
    "\n",
    "# 2nd Convolutional layer added\n",
    "classifier.add(Convolution2D(  64,3,3,  activation='relu'  ))\n",
    "classifier.add(MaxPooling2D(  pool_size = (2,2)  ))\n",
    "\n",
    "# 3rd Convolutional layer added\n",
    "classifier.add(Convolution2D(  128,3,3,  activation='relu'  ))\n",
    "classifier.add(MaxPooling2D(  pool_size = (2,2)  ))\n",
    "\n",
    "# 4th Convolutional layer added\n",
    "classifier.add(Convolution2D(  256,3,3,  activation='relu'  ))\n",
    "classifier.add(MaxPooling2D(  pool_size = (2,2)  ))\n",
    "\n",
    "\n",
    "\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dense( output_dim=128, activation='relu' )) # 1st hidden layer; 'output_dim'='units'\n",
    "classifier.add(Dense( output_dim=1, activation='sigmoid' )) # output layer\n",
    "\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Image Pre-Processing and Fitting\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255, # rescales all pixel values between 0-1\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        channel_shift_range=0.2)\n",
    "# rotation_range=0.0, width_shift_range=0.0, height_shift_range=0.0, channel_shift_range=0.0\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'training_set', # path after WD to be mentioned; refer upper sections for current WD\n",
    "        target_size=(64,64),\n",
    "        batch_size=32, # batch size after which NN weights would be updated\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_set = validation_datagen.flow_from_directory(\n",
    "        'validation_set',\n",
    "        target_size=(64,64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "np.random.seed(4)\n",
    "classifier.fit_generator(\n",
    "        training_set,\n",
    "        samples_per_epoch=8000, # since we have 8000 images in training_set\n",
    "        epochs=50,\n",
    "        validation_data=validation_set,\n",
    "        nb_val_samples=2000) # number of images in validation_set\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "elapsed_time\n",
    "\n",
    "#########################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tips to improve accuracy: people claiming upto 99.5% - https://www.udemy.com/deeplearning/learn/v4/questions/2276518\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4657116944260067"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Iterations mini(200/8000 training images and 200/2000 test images)\n",
    "# Note: this approach to identify important features (& combination) does not work as it gives different/opposite results than bigger data for example, it gives lower accuracy on adding extra convolution layers\n",
    "# It would be because NN does not perform well for smaller data size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Automated Parameter Tuning: Code to be written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Most Imp Parameters as per Udemy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tip to improve speed on GPU\n",
    "# https://www.udemy.com/deeplearning/learn/v4/questions/2276518"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.581736306349436"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tricks to improve accuracy:\n",
    "# Udemy: Run on GPU\n",
    "# Udemy: More Convolutional Layers\n",
    "# Udemy: Higher # of pixels in input_size and target_size (above it's 64)\n",
    "\n",
    "# Increase Fully Connected layers\n",
    "# Alter different parameters\n",
    "\n",
    "\n",
    "694.9041783809662/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scoring\n",
    "\n",
    "import cv2 # part of Opencv library\n",
    "\n",
    "img = cv2.imread('scoring/score/image.jpg')\n",
    "img = cv2.resize(img,(64,64))\n",
    "img = np.reshape(img,[1,64,64,3])\n",
    "\n",
    "classes = classifier.predict_classes(img)\n",
    "\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking numerical mapping of classes cat/dog\n",
    "temp = validation_set.class_indices # temp is 'dict'; temp[\"cats\"] to access values from dict\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[\"cats\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[\"dogs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 images belonging to 1 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.49793893],\n",
       "       [ 0.51523113],\n",
       "       [ 0.50625318]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "scoring_set = validation_datagen.flow_from_directory(\n",
    "        'scoring',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary',\n",
    "        shuffle=False) # shuffle is necessary to maintain order of predictions else order of predictions are not same as order in the 'scoring' folder\n",
    "\n",
    "predictions = classifier.predict_generator(scoring_set)\n",
    "predictions\n",
    "\n",
    "\n",
    "# Note: predictions are not in the same order as files in 'score' folder; and predictions order change everytime you run\n",
    "\n",
    "# Potential solution:\n",
    "# https://stackoverflow.com/questions/48937676/keras-predict-generator-output-differs-every-time\n",
    "# https://github.com/keras-team/keras/issues/3296\n",
    "    \n",
    "# CM:\n",
    "# https://datascience.stackexchange.com/questions/13894/how-to-get-predictions-with-predict-generator-on-streaming-test-data-in-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['score\\\\33.jpg', 'score\\\\343.jpg', 'score\\\\55.jpg']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring_set.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.1) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4044: error: (-215) ssize.width > 0 && ssize.height > 0 in function cv::resize\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1a6794ba3c0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'scoring/score/cat_or_dog_1.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(3.4.1) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4044: error: (-215) ssize.width > 0 && ssize.height > 0 in function cv::resize\n"
     ]
    }
   ],
   "source": [
    "import cv2 # part of Opencv library\n",
    "\n",
    "img = cv2.imread('scoring/score/cat_or_dog_1.jpg')\n",
    "img = cv2.resize(img,(64,64))\n",
    "img = np.reshape(img,[1,64,64,3])\n",
    "\n",
    "classes = classifier.predict_classes(img)\n",
    "\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set = train_datagen.flow_from_directory(\n",
    "        'training_set', # path after WD to be mentioned; refer upper sections for current WD\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32, # batch size after which NN weights would be updated\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alternate Udemy code for single image prediction:\n",
    "# https://www.udemy.com/deeplearning/learn/v4/t/lecture/6798970?start=375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' rough '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\"\"\" rough \"\"\"\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linux - change wd(binary):\n",
    "import os\n",
    "os.chdir(\"/home/antrived/Dropbox/Job-25-Feb-2019/prep/x2-more new projects & new techniques/1. Techniques & Algos/0. Deep Learning/2. CNN/2.1 Data CNN/Convolutional_Neural_Networks/dataset/rough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antrived/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
      "  \n",
      "/home/antrived/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "/home/antrived/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antrived/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:83: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/antrived/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:83: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., epochs=5, validation_data=<keras_pre..., steps_per_epoch=250, validation_steps=2000)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 27/250 [==>...........................] - ETA: 55s - loss: 0.8282 - acc: 0.4954"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-41e3ef61b435>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# default was 25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         nb_val_samples=2000) # number of images in validation_set\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Basic Model - Binary Classification (1 Convolutional layer and 1 Fully Connected layer)\n",
    "# Not tuning performed\n",
    "\n",
    "# Initializing CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1: Convolution - Adding Convolution Layer:\n",
    "classifier.add(Convolution2D(  32,3,3,  input_shape=(64,64,3),  activation='relu'  ))\n",
    "# parameter info: https://www.udemy.com/deeplearning/learn/v4/t/lecture/6743906?start=0\n",
    "# 32,3,3 - 32 different 'feature detectors' are used of size, 3 rows and 3 columns\n",
    "# 64,64,3 - all images are converted to 64*64 pixels. 3 is for colored(R.G,B) IMAGES. 1 is for B&W images.\n",
    "# 'relu' - all 'feature maps' go through this to remove -ve numbered pixels and separate out feature in the images \n",
    "#   (i.e. remove linearity as explained in Udemy video)\n",
    "\n",
    "# Step 2: Max Pooling - Adding Max Pooling Layer:\n",
    "classifier.add(MaxPooling2D(  pool_size = (2,2)  ))\n",
    "# parameter info: https://www.udemy.com/deeplearning/learn/v4/t/lecture/6743910?start=15\n",
    "# 2,2 - size of 2 will reduce the convoluted images in half without losing on features. Similarly, 3,3 will reduce to 1/3.\n",
    "\n",
    "# Step 3: Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4: Full Connection\n",
    "classifier.add(Dense( output_dim=128, activation='relu' )) # 1st hidden layer; 'output_dim'='units'\n",
    "classifier.add(Dense( output_dim=1, activation='sigmoid' )) # output layer\n",
    "# parameter definition same as ANN\n",
    "\n",
    "# Step 5: Compiling the ANN (means run SGD on ANN)\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# parameter definition same as ANN\n",
    "\n",
    "# i). Image Pre-Processing - Image Augmentation: done to avoid overfitting\n",
    "# Refer keras.io for pre-processing code\n",
    "# Image Augmentation: Augments the number of images in training data by flipping, rotating and more on images to increase training data and prevent overfitting\n",
    " \n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255, # rescales all pixel values between 0-1\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Parameter Info: https://www.udemy.com/deeplearning/learn/v4/t/lecture/6743978?start=990\n",
    "# rescale: feature scaled images of the current images\n",
    "# shear range: geometrical transformation also called convection-random images selected, pixels are moved in a fixed direction(say right), top pixels move 2x but middle pixels move x, bottom pixels don't move at all\n",
    " # example link of image shearing: https://www.google.co.in/search?rlz=1C1GGRV_enIN751IN751&biw=1536&bih=735&tbm=isch&sa=1&ei=2pcbW5C-Fcz4rQG6jqbgBw&q=image+shearing+deep+learning&oq=image+shearing+deep+learning&gs_l=img.3...52060.54569.0.54652.14.14.0.0.0.0.222.1403.2j8j1.11.0....0...1c.1.64.img..3.1.221...0i30k1j0i8i30k1j0i24k1.0.TPEqQTgklRA#imgrc=8cUAVuJx1BfG9M:\n",
    "# zoom range: transformation-range for random zooming\n",
    "# horizontal flip: transformation-randomly flips inputs horizontally (vertical also there, not used here)\n",
    "# there are more image transformations in Keras documentation(not used in Udemy video), you may try them as well (Link: https://keras.io/preprocessing/image/)\n",
    "\n",
    "\n",
    "# ii). Training and Validation Set Generation:\n",
    "# This section creates training set and validation set by taking all rescaled and transformed images from i) section, ImageDataGenerator\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        '/home/antrived/Dropbox/Job-25-Feb-2019/prep/x2-more new projects & new techniques/1. Techniques & Algos/0. Deep Learning/2. CNN/2.1 Data CNN/Convolutional_Neural_Networks/dataset/backup/training_set', # path after WD to be mentioned; refer upper sections for current WD\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32, # batch size after which NN weights would be updated\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_set = validation_datagen.flow_from_directory(\n",
    "        '/home/antrived/Dropbox/Job-25-Feb-2019/prep/x2-more new projects & new techniques/1. Techniques & Algos/0. Deep Learning/2. CNN/2.1 Data CNN/Convolutional_Neural_Networks/dataset/backup/validation_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "# Output Message:\n",
    "# Found 8000 images belonging to 2 classes.\n",
    "# Found 2000 images belonging to 2 classes.\n",
    "\n",
    "\n",
    "# iii). Fitting Model on Training and Evaluating on Validation:\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# np.random.seed(4)\n",
    "classifier.fit_generator(\n",
    "        training_set,\n",
    "        samples_per_epoch=8000, # since we have 8000 images in training_set\n",
    "        epochs=5, # default was 25\n",
    "        validation_data=validation_set,\n",
    "        nb_val_samples=2000) # number of images in validation_set\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "elapsed_time\n",
    "\n",
    "# UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. \n",
    "# `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size.\n",
    "# Note: steps_per_epoch = 250. Since Batche_size = 32. So, 250 steps complete 1 epoch.\n",
    "\n",
    "# Run Time: 2.28 hours (Single Layer)\n",
    "# Model Selection: Select model(at epoch #) where Validation accuracy is high and Training accuracy is close to Validation\n",
    "# accuracy after 25 epochs: acc: 0.8564 - val_acc: 0.7780\n",
    "# accuracy after 16 epochs: acc: 0.8284 - val_acc: 0.8026 (set seed and reproduce this model with epoch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROUGH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
